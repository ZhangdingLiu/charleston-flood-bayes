#!/usr/bin/env python3
"""
å¢å¼ºè¦†ç›–ç‡è´å¶æ–¯ç½‘ç»œæ´ªæ°´é¢„æµ‹éªŒè¯
- ä½¿ç”¨å…¨å†å²æ•°æ®ä½œä¸ºè®­ç»ƒé›†
- æµ‹è¯•ä¸åŒå‚æ•°ç­–ç•¥ä»¥æœ€å¤§åŒ–è¦†ç›–ç‡
- é’ˆå¯¹2017/09/11è¿›è¡Œè¯¦ç»†è¯„ä¼°
"""

import json
import os
import sys
import random
import time
import math
from datetime import datetime
from collections import defaultdict, Counter
import csv

# Set random seed
RANDOM_SEED = 42
random.seed(RANDOM_SEED)

class EnhancedCoverageValidator:
    """å¢å¼ºè¦†ç›–ç‡è´å¶æ–¯æ´ªæ°´é¢„æµ‹éªŒè¯å™¨"""
    
    def __init__(self):
        self.test_date = '2017/09/11'  # å›ºå®šæµ‹è¯•æ—¥æœŸ
        
        # ä¸‰ç§å‚æ•°ç­–ç•¥
        self.parameter_strategies = {
            'conservative': {
                'name': 'ä¿å®ˆä¼˜åŒ–',
                'occ_thr': 3,
                'edge_thr': 2, 
                'weight_thr': 0.15,
                'evidence_ratio': 0.3
            },
            'balanced': {
                'name': 'å¹³è¡¡ä¼˜åŒ–',
                'occ_thr': 2,
                'edge_thr': 2,
                'weight_thr': 0.1,
                'evidence_ratio': 0.3
            },
            'aggressive': {
                'name': 'æ¿€è¿›ä¼˜åŒ–', 
                'occ_thr': 1,
                'edge_thr': 1,
                'weight_thr': 0.05,
                'evidence_ratio': 0.3
            }
        }
        
        # æµ‹è¯•é˜ˆå€¼
        self.pred_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
        
        # æ¯ä¸ªç­–ç•¥çš„é‡å¤è¯•éªŒæ¬¡æ•°
        self.trials_per_strategy = 5
        
        self.flood_data = []
        self.all_results = []
        
    def load_flood_data(self):
        """åŠ è½½æ´ªæ°´æ•°æ®"""
        print("ğŸ”„ åŠ è½½æ´ªæ°´æ•°æ®...")
        
        with open("Road_Closures_2024.csv", 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['REASON'].upper() == 'FLOOD':
                    # æå–æ—¥æœŸ
                    start_date = row['START'].split(' ')[0].replace('"', '')
                    street = row['STREET'].replace('"', '').upper().replace(' ', '_')
                    
                    # å¤„ç†BOMå­—ç¬¦
                    objectid_key = 'OBJECTID'
                    if objectid_key not in row:
                        objectid_key = 'ï»¿OBJECTID'
                        
                    self.flood_data.append({
                        'date': start_date,
                        'street': street,
                        'objectid': row.get(objectid_key, '')
                    })
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.flood_data)} æ¡æ´ªæ°´è®°å½•")
        
        # æ•°æ®åˆ†å‰²ç»Ÿè®¡
        test_records = [r for r in self.flood_data if r['date'] == self.test_date]
        train_records = [r for r in self.flood_data if r['date'] != self.test_date]
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
        print(f"   æµ‹è¯•é›† ({self.test_date}): {len(test_records)} æ¡è®°å½•")
        print(f"   è®­ç»ƒé›† (æ‰€æœ‰å…¶ä»–æ—¥æœŸ): {len(train_records)} æ¡è®°å½•")
        print(f"   è®­ç»ƒ/æµ‹è¯•æ¯”ä¾‹: {len(train_records)/len(test_records):.1f}:1")
        
        # æµ‹è¯•æ—¥æœŸé“è·¯ç»Ÿè®¡
        test_roads = set(r['street'] for r in test_records)
        print(f"   æµ‹è¯•æ—¥æœŸç‹¬ç‰¹é“è·¯: {len(test_roads)} æ¡")
        
        return self.flood_data
    
    def build_bayesian_network(self, train_data, params):
        """æ„å»ºè´å¶æ–¯ç½‘ç»œ (åŸºäºè®­ç»ƒæ•°æ®å’ŒæŒ‡å®šå‚æ•°)"""
        strategy_name = params['name']
        print(f"ğŸ—ï¸ æ„å»ºè´å¶æ–¯ç½‘ç»œ ({strategy_name})...")
        print(f"   å‚æ•°: occ_thr={params['occ_thr']}, edge_thr={params['edge_thr']}, weight_thr={params['weight_thr']}")
        
        try:
            # åˆ›å»ºåŸºæœ¬çš„è®­ç»ƒæ•°æ®åˆ—è¡¨
            simplified_data = []
            for record in train_data:
                simplified_data.append({
                    'date': record['date'],
                    'street': record['street'],
                    'objectid': record['objectid']
                })
            
            # åº”ç”¨å‡ºç°æ¬¡æ•°é˜ˆå€¼è¿‡æ»¤
            road_freq = Counter(r['street'] for r in simplified_data)
            network_roads = [road for road, freq in road_freq.items() 
                            if freq >= params['occ_thr']]
            
            if len(network_roads) < 3:
                print(f"âŒ ç½‘ç»œé“è·¯ä¸è¶³3æ¡ (ä»…{len(network_roads)}æ¡)")
                return None, False
                
            print(f"âœ… ç½‘ç»œæ„å»ºå®Œæˆ: {len(network_roads)} èŠ‚ç‚¹ (é˜ˆå€¼{params['occ_thr']}è¿‡æ»¤)")
            
            # åˆ›å»ºå¢å¼ºçš„ç½‘ç»œå¯¹è±¡
            class EnhancedNetwork:
                def __init__(self, roads, train_data, params):
                    self.nodes = set(roads)
                    self.train_data = train_data
                    self.road_freq = Counter(r['street'] for r in train_data)
                    self.params = params
                    
                    # è®¡ç®—é“è·¯å…±ç°çŸ©é˜µ
                    self.cooccurrence = self._build_cooccurrence_matrix()
                    
                def _build_cooccurrence_matrix(self):
                    """æ„å»ºé“è·¯å…±ç°çŸ©é˜µ"""
                    cooc = defaultdict(lambda: defaultdict(int))
                    
                    # æŒ‰æ—¥æœŸåˆ†ç»„
                    date_roads = defaultdict(set)
                    for record in self.train_data:
                        if record['street'] in self.nodes:
                            date_roads[record['date']].add(record['street'])
                    
                    # è®¡ç®—å…±ç°æ¬¡æ•°
                    for date, roads in date_roads.items():
                        roads = list(roads)
                        for i, road1 in enumerate(roads):
                            for j, road2 in enumerate(roads):
                                if i != j:
                                    cooc[road1][road2] += 1
                    
                    return cooc
                    
                def number_of_nodes(self):
                    return len(self.nodes)
                    
                def number_of_edges(self):
                    # è®¡ç®—æ»¡è¶³é˜ˆå€¼çš„è¾¹æ•°
                    edge_count = 0
                    for road1 in self.nodes:
                        for road2 in self.nodes:
                            if road1 != road2:
                                cooc_count = self.cooccurrence[road1][road2]
                                if cooc_count >= self.params['edge_thr']:
                                    # è®¡ç®—æ¡ä»¶æ¦‚ç‡
                                    road1_freq = self.road_freq[road1]
                                    if road1_freq > 0:
                                        cond_prob = cooc_count / road1_freq
                                        if cond_prob >= self.params['weight_thr']:
                                            edge_count += 1
                    return edge_count
                    
                def infer_w_evidence(self, road, evidence):
                    """å¢å¼ºçš„è´å¶æ–¯æ¨ç†"""
                    if road not in self.nodes:
                        return {'flooded': 0.0}
                    
                    # åŸºç¡€æ¦‚ç‡ï¼šåŸºäºè®­ç»ƒé¢‘æ¬¡ï¼Œå½’ä¸€åŒ–åˆ°æœ€å¤§é¢‘æ¬¡
                    max_freq = max(self.road_freq.values()) if self.road_freq.values() else 1
                    base_prob = self.road_freq.get(road, 0) / max_freq
                    
                    # è¯æ®å½±å“ï¼šåŸºäºå…±ç°å…³ç³»
                    evidence_boost = 0.0
                    evidence_count = 0
                    
                    for ev_road, ev_value in evidence.items():
                        if ev_value == 1 and ev_road in self.nodes and ev_road != road:
                            # è®¡ç®—æ¡ä»¶æ¦‚ç‡ P(target_road | evidence_road)
                            cooc_count = self.cooccurrence[ev_road][road]
                            ev_freq = self.road_freq.get(ev_road, 0)
                            
                            if ev_freq > 0 and cooc_count >= self.params['edge_thr']:
                                cond_prob = cooc_count / ev_freq
                                if cond_prob >= self.params['weight_thr']:
                                    evidence_boost += cond_prob * 0.5  # å¢å¼ºæƒé‡
                                    evidence_count += 1
                    
                    # ç»¼åˆæ¦‚ç‡è®¡ç®—
                    if evidence_count > 0:
                        # æœ‰æœ‰æ•ˆè¯æ®æ—¶ï¼Œç»“åˆåŸºç¡€æ¦‚ç‡å’Œè¯æ®
                        evidence_avg = evidence_boost / evidence_count
                        final_prob = min(1.0, base_prob * 0.3 + evidence_avg * 0.7)
                    else:
                        # æ— æœ‰æ•ˆè¯æ®æ—¶ï¼Œä½¿ç”¨åŸºç¡€æ¦‚ç‡
                        final_prob = base_prob * 0.5  # é™ä½æ— è¯æ®æ—¶çš„ç½®ä¿¡åº¦
                    
                    return {'flooded': final_prob}
            
            enhanced_network = EnhancedNetwork(network_roads, simplified_data, params)
            
            # åˆ›å»ºåŒ…è£…å¯¹è±¡ä»¥å…¼å®¹åŸæœ‰æ¥å£
            class NetworkWrapper:
                def __init__(self, enhanced_net):
                    self.network = enhanced_net
                    
                def infer_w_evidence(self, road, evidence):
                    return self.network.infer_w_evidence(road, evidence)
            
            flood_net = NetworkWrapper(enhanced_network)
            
            print(f"   ç½‘ç»œç»Ÿè®¡: {enhanced_network.number_of_nodes()} èŠ‚ç‚¹, {enhanced_network.number_of_edges()} æ¡è¾¹")
            
            return flood_net, True
            
        except Exception as e:
            print(f"âŒ ç½‘ç»œæ„å»ºå¤±è´¥: {str(e)}")
            return None, False
    
    def run_single_experiment(self, strategy_key, params, pred_threshold, trial_id):
        """è¿è¡Œå•æ¬¡å®éªŒ"""
        print(f"\nğŸ“… å®éªŒ: {params['name']}, é˜ˆå€¼: {pred_threshold}, è¯•éªŒ: {trial_id+1}/{self.trials_per_strategy}")
        
        # 1. æ•°æ®åˆ†å‰² - ä½¿ç”¨å…¨å†å²æ•°æ®
        test_records = [r for r in self.flood_data if r['date'] == self.test_date]
        train_records = [r for r in self.flood_data if r['date'] != self.test_date]
        
        test_roads = set(r['street'] for r in test_records)
        
        if len(test_roads) < 3:
            print("âŒ æµ‹è¯•é“è·¯æ•°é‡ä¸è¶³ï¼Œè·³è¿‡")
            return None
            
        print(f"   æµ‹è¯•æ—¥æœŸé“è·¯æ€»æ•°: {len(test_roads)}")
        
        # 2. æ„å»ºè´å¶æ–¯ç½‘ç»œ
        flood_net, success = self.build_bayesian_network(train_records, params)
        if not success:
            return None
        
        # è·å–ç½‘ç»œä¸­çš„æ‰€æœ‰é“è·¯
        network_roads = flood_net.network.nodes
        
        # è®¡ç®—è¦†ç›–ç‡
        test_network_roads = list(test_roads & network_roads)
        coverage_rate = len(test_network_roads) / len(test_roads)
        
        print(f"   ğŸ¯ è¦†ç›–ç‡åˆ†æ:")
        print(f"   ç½‘ç»œæ€»èŠ‚ç‚¹: {len(network_roads)}")
        print(f"   æµ‹è¯•é“è·¯åœ¨ç½‘ç»œä¸­: {len(test_network_roads)}/{len(test_roads)} = {coverage_rate:.1%}")
        
        if len(test_network_roads) < 3:
            print(f"âŒ ç½‘ç»œä¸­æµ‹è¯•é“è·¯ä¸è¶³3æ¡ï¼Œè·³è¿‡")
            return None
        
        # 3. é€‰æ‹©è¯æ®å’Œé¢„æµ‹ç›®æ ‡
        evidence_count = max(1, int(len(test_network_roads) * params['evidence_ratio']))
        
        # éšæœºé€‰æ‹©è¯æ®é“è·¯
        random.shuffle(test_network_roads)
        evidence_roads = test_network_roads[:evidence_count]
        
        # é¢„æµ‹ç›®æ ‡ï¼šæ‰€æœ‰ç½‘ç»œä¸­çš„éè¯æ®é“è·¯
        all_predict_roads = list(network_roads - set(evidence_roads))
        positive_predict_count = len([road for road in all_predict_roads if road in test_roads])
        negative_predict_count = len(all_predict_roads) - positive_predict_count
        
        print(f"   è¯æ®é“è·¯: {len(evidence_roads)} æ¡")
        print(f"   é¢„æµ‹ç›®æ ‡: {len(all_predict_roads)} æ¡ (æ­£æ ·æœ¬:{positive_predict_count}, è´Ÿæ ·æœ¬:{negative_predict_count})")
        
        # 4. è´å¶æ–¯æ¨ç†é¢„æµ‹
        evidence = {road: 1 for road in evidence_roads}
        predictions = {}
        true_labels = {}
        detailed_predictions = []
        
        successful_predictions = 0
        failed_predictions = 0
        
        for road in all_predict_roads:
            true_label = 1 if road in test_roads else 0
            
            try:
                result = flood_net.infer_w_evidence(road, evidence)
                prob = result.get('flooded', result.get(1, 0))
                
                predicted_label = 1 if prob >= pred_threshold else 0
                predictions[road] = predicted_label
                true_labels[road] = true_label
                
                detailed_predictions.append({
                    'road_name': road,
                    'predicted_probability': float(prob),
                    'true_label': true_label,
                    'predicted_label': predicted_label,
                    'inference_failed': False
                })
                
                successful_predictions += 1
                
            except Exception as e:
                print(f"   âš ï¸ æ¨ç†å¤±è´¥ {road}: {str(e)}")
                
                detailed_predictions.append({
                    'road_name': road,
                    'predicted_probability': None,
                    'true_label': true_label,
                    'predicted_label': None,
                    'inference_failed': True,
                    'error_message': str(e)
                })
                
                failed_predictions += 1
                continue
        
        if successful_predictions == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„è´å¶æ–¯æ¨ç†ï¼Œè·³è¿‡")
            return None
            
        print(f"   æ¨ç†ç»“æœ: {successful_predictions} æˆåŠŸ, {failed_predictions} å¤±è´¥")
        
        # 5. è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        y_true = [true_labels[road] for road in predictions.keys()]
        y_pred = [predictions[road] for road in predictions.keys()]
        
        tp = sum(1 for i in range(len(y_true)) if y_true[i] == 1 and y_pred[i] == 1)
        fp = sum(1 for i in range(len(y_true)) if y_true[i] == 0 and y_pred[i] == 1)
        tn = sum(1 for i in range(len(y_true)) if y_true[i] == 0 and y_pred[i] == 0)
        fn = sum(1 for i in range(len(y_true)) if y_true[i] == 1 and y_pred[i] == 0)
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        
        print(f"   ğŸ“ˆ æ€§èƒ½: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, Acc={accuracy:.3f}")
        print(f"   ğŸ“Š æ··æ·†çŸ©é˜µ: TP={tp}, FP={fp}, TN={tn}, FN={fn}")
        
        # 6. è¿”å›ç»“æœ
        result = {
            'strategy': strategy_key,
            'strategy_name': params['name'],
            'test_date': self.test_date,
            'pred_threshold': pred_threshold,
            'trial_id': trial_id,
            'parameters': params,
            'coverage_rate': coverage_rate,
            'test_roads_total': len(test_roads),
            'test_roads_in_network': len(test_network_roads),
            'network_nodes_total': len(network_roads),
            'evidence_roads_count': len(evidence_roads),
            'positive_predict_roads_count': positive_predict_count,
            'negative_predict_roads_count': negative_predict_count,
            'total_predict_roads_count': len(all_predict_roads),
            'successful_predictions': successful_predictions,
            'failed_predictions': failed_predictions,
            'evidence_roads': evidence_roads,
            'detailed_predictions': detailed_predictions,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'accuracy': accuracy,
            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
            'network_nodes': flood_net.network.number_of_nodes(),
            'network_edges': flood_net.network.number_of_edges()
        }
        
        return result
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å‚æ•°ç­–ç•¥çš„å¯¹æ¯”å®éªŒ"""
        print("ğŸš€ å¼€å§‹å¢å¼ºè¦†ç›–ç‡è´å¶æ–¯ç½‘ç»œå®éªŒ")
        print("ğŸ¯ ç›®æ ‡: æœ€å¤§åŒ–2017/09/11æµ‹è¯•é›†çš„é¢„æµ‹è¦†ç›–ç‡")
        print(f"ğŸ“Š å®éªŒé…ç½®:")
        print(f"   å‚æ•°ç­–ç•¥: {len(self.parameter_strategies)} ç§")
        print(f"   é¢„æµ‹é˜ˆå€¼: {self.pred_thresholds}")
        print(f"   æ¯ç­–ç•¥è¯•éªŒ: {self.trials_per_strategy} æ¬¡")
        print(f"   æ€»å®éªŒæ•°: {len(self.parameter_strategies) * len(self.pred_thresholds) * self.trials_per_strategy} æ¬¡")
        
        start_time = time.time()
        
        # åŠ è½½æ•°æ®
        self.load_flood_data()
        
        # è¿è¡Œå®éªŒ
        successful_results = []
        experiment_count = 0
        
        for strategy_key, params in self.parameter_strategies.items():
            print(f"\n{'='*80}")
            print(f"ğŸ”¬ å¼€å§‹æµ‹è¯•ç­–ç•¥: {params['name']}")
            print(f"{'='*80}")
            
            for threshold in self.pred_thresholds:
                for trial in range(self.trials_per_strategy):
                    experiment_count += 1
                    total_experiments = len(self.parameter_strategies) * len(self.pred_thresholds) * self.trials_per_strategy
                    print(f"\n{'='*60} å®éªŒ {experiment_count}/{total_experiments} {'='*60}")
                    
                    result = self.run_single_experiment(strategy_key, params, threshold, trial)
                    if result:
                        successful_results.append(result)
                        self.all_results.append(result)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å¢å¼ºè¦†ç›–ç‡å®éªŒå®Œæˆæ±‡æ€»")
        print(f"{'='*80}")
        
        if not successful_results:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒ")
            return None
        
        total_planned = len(self.parameter_strategies) * len(self.pred_thresholds) * self.trials_per_strategy
        print(f"âœ… æˆåŠŸå®éªŒ: {len(successful_results)}/{total_planned}")
        
        # æŒ‰ç­–ç•¥åˆ†æè¦†ç›–ç‡
        self.analyze_coverage_results(successful_results)
        
        execution_time = time.time() - start_time
        print(f"\nâ±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.1f} ç§’")
        
        # ä¿å­˜ç»“æœ
        self.save_results(successful_results, execution_time)
        
        return successful_results
    
    def analyze_coverage_results(self, results):
        """åˆ†æè¦†ç›–ç‡ç»“æœ"""
        print(f"\nğŸ“ˆ è¦†ç›–ç‡å’Œæ€§èƒ½åˆ†æ:")
        
        # æŒ‰ç­–ç•¥åˆ†ç»„
        strategy_results = defaultdict(list)
        for result in results:
            strategy_results[result['strategy']].append(result)
        
        print(f"\n{'ç­–ç•¥':<15} {'è¦†ç›–ç‡':<10} {'èŠ‚ç‚¹æ•°':<8} {'è¾¹æ•°':<8} {'Precision':<12} {'Recall':<10} {'F1':<10}")
        print("-" * 80)
        
        for strategy_key in ['conservative', 'balanced', 'aggressive']:
            if strategy_key in strategy_results:
                strategy_data = strategy_results[strategy_key]
                
                # è®¡ç®—å¹³å‡å€¼
                avg_coverage = sum(r['coverage_rate'] for r in strategy_data) / len(strategy_data)
                avg_nodes = sum(r['network_nodes'] for r in strategy_data) / len(strategy_data)
                avg_edges = sum(r['network_edges'] for r in strategy_data) / len(strategy_data)
                avg_precision = sum(r['precision'] for r in strategy_data) / len(strategy_data)
                avg_recall = sum(r['recall'] for r in strategy_data) / len(strategy_data)
                avg_f1 = sum(r['f1_score'] for r in strategy_data) / len(strategy_data)
                
                strategy_name = strategy_data[0]['strategy_name']
                print(f"{strategy_name:<15} {avg_coverage:.1%}      {avg_nodes:.0f}      {avg_edges:.0f}      "
                      f"{avg_precision:.3f}        {avg_recall:.3f}    {avg_f1:.3f}")
        
        # æ‰¾å‡ºæœ€é«˜è¦†ç›–ç‡çš„ç»“æœ
        max_coverage_result = max(results, key=lambda x: x['coverage_rate'])
        print(f"\nğŸ† æœ€é«˜è¦†ç›–ç‡ç»“æœ:")
        print(f"   ç­–ç•¥: {max_coverage_result['strategy_name']}")
        print(f"   è¦†ç›–ç‡: {max_coverage_result['coverage_rate']:.1%} ({max_coverage_result['test_roads_in_network']}/{max_coverage_result['test_roads_total']})")
        print(f"   ç½‘ç»œè§„æ¨¡: {max_coverage_result['network_nodes']} èŠ‚ç‚¹, {max_coverage_result['network_edges']} è¾¹")
        print(f"   æ€§èƒ½: P={max_coverage_result['precision']:.3f}, R={max_coverage_result['recall']:.3f}, F1={max_coverage_result['f1_score']:.3f}")
    
    def save_results(self, results, execution_time):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è¯¦ç»†JSONç»“æœ
        result_file = f"enhanced_coverage_validation_results_{timestamp}.json"
        
        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        summary_stats = {}
        strategy_results = defaultdict(list)
        for result in results:
            strategy_results[result['strategy']].append(result)
        
        for strategy_key, strategy_data in strategy_results.items():
            summary_stats[strategy_key] = {
                'strategy_name': strategy_data[0]['strategy_name'],
                'avg_coverage_rate': sum(r['coverage_rate'] for r in strategy_data) / len(strategy_data),
                'avg_network_nodes': sum(r['network_nodes'] for r in strategy_data) / len(strategy_data),
                'avg_network_edges': sum(r['network_edges'] for r in strategy_data) / len(strategy_data),
                'avg_precision': sum(r['precision'] for r in strategy_data) / len(strategy_data),
                'avg_recall': sum(r['recall'] for r in strategy_data) / len(strategy_data),
                'avg_f1_score': sum(r['f1_score'] for r in strategy_data) / len(strategy_data),
                'experiment_count': len(strategy_data)
            }
        
        result_data = {
            'experiment_summary': {
                'test_date': self.test_date,
                'total_experiments_planned': len(self.parameter_strategies) * len(self.pred_thresholds) * self.trials_per_strategy,
                'successful_experiments': len(results),
                'parameter_strategies': self.parameter_strategies,
                'pred_thresholds': self.pred_thresholds,
                'trials_per_strategy': self.trials_per_strategy,
                'execution_time': execution_time,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'method': 'enhanced_coverage_bayesian_inference',
                'summary_statistics': summary_stats
            },
            'detailed_results': results
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVæ±‡æ€»
        csv_file = f"enhanced_coverage_validation_summary_{timestamp}.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'strategy', 'strategy_name', 'test_date', 'pred_threshold', 'trial_id',
                'coverage_rate', 'test_roads_total', 'test_roads_in_network', 'network_nodes', 'network_edges',
                'positive_predict_count', 'negative_predict_count', 'total_predict_count',
                'successful_predictions', 'failed_predictions',
                'precision', 'recall', 'f1_score', 'accuracy',
                'tp', 'fp', 'tn', 'fn'
            ])
            
            for result in results:
                writer.writerow([
                    result['strategy'], result['strategy_name'], result['test_date'],
                    result['pred_threshold'], result['trial_id'],
                    result['coverage_rate'], result['test_roads_total'], result['test_roads_in_network'],
                    result['network_nodes'], result['network_edges'],
                    result['positive_predict_roads_count'], result['negative_predict_roads_count'],
                    result['total_predict_roads_count'],
                    result['successful_predictions'], result['failed_predictions'],
                    result['precision'], result['recall'], result['f1_score'], result['accuracy'],
                    result['tp'], result['fp'], result['tn'], result['fn']
                ])
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   - {result_file} (è¯¦ç»†ç»“æœ)")
        print(f"   - {csv_file} (æ€§èƒ½æ±‡æ€»)")
        
        return result_file, csv_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŠ Charlestonæ´ªæ°´é¢„æµ‹ - å¢å¼ºè¦†ç›–ç‡è´å¶æ–¯ç½‘ç»œéªŒè¯")
    print("ğŸ¯ ä½¿ç”¨å…¨å†å²æ•°æ®æœ€å¤§åŒ–é¢„æµ‹è¦†ç›–ç‡")
    
    validator = EnhancedCoverageValidator()
    results = validator.run_all_experiments()
    
    if results:
        print(f"\nğŸ‰ å¢å¼ºè¦†ç›–ç‡å®éªŒå®Œæˆï¼æˆåŠŸå®Œæˆ {len(results)} æ¬¡å®éªŒ")
        print("ğŸ“ˆ å…³é”®æ”¹è¿›: ä½¿ç”¨å…¨å†å²æ•°æ® + å‚æ•°ä¼˜åŒ– + è¦†ç›–ç‡æœ€å¤§åŒ–")
    else:
        print(f"\nğŸ’¥ å®éªŒå¤±è´¥ï¼è¯·æ£€æŸ¥æ•°æ®å’Œå‚æ•°è®¾ç½®")
    
    return validator, results

if __name__ == "__main__":
    validator, results = main()