#!/usr/bin/env python3
"""
å¿«é€Ÿåˆ†ææ¯æ—¥floodé“è·¯æ•°é‡ï¼Œæ‰¾å‡ºTop10æ´ªæ°´æ—¥æœŸ
"""

import sys
import os
sys.path.append('/mnt/d/Data/coda_PycharmProjects/PIN_bayesian')

import pandas as pd

def analyze_daily_flood_events():
    """åˆ†ææ¯æ—¥æ´ªæ°´äº‹ä»¶ï¼Œè¿”å›Top10"""
    
    # 1. åŠ è½½æ•°æ®
    df = pd.read_csv("/mnt/d/Data/coda_PycharmProjects/PIN_bayesian/Road_Closures_2024.csv")
    flood_df = df[df["REASON"].str.upper() == "FLOOD"].copy()
    
    # 2. æ—¶é—´å¤„ç†
    flood_df["time_create"] = pd.to_datetime(flood_df["START"], utc=True)
    flood_df["date"] = flood_df["time_create"].dt.date
    
    # 3. æ¯æ—¥ç»Ÿè®¡
    daily_stats = flood_df.groupby("date").agg({
        'STREET': 'nunique',  # ç‹¬ç‰¹é“è·¯æ•°
        'OBJECTID': 'count'   # æ€»è®°å½•æ•°
    }).rename(columns={'STREET': 'unique_roads', 'OBJECTID': 'total_records'})
    
    # 4. Top 10
    top10_dates = daily_stats.nlargest(10, 'unique_roads')
    
    print("ğŸ“Š Top 10 æ´ªæ°´æ—¥æœŸ (æŒ‰é“è·¯æ•°é‡æ’åº):")
    print("=" * 60)
    
    for i, (date, row) in enumerate(top10_dates.iterrows(), 1):
        # è·å–è¯¥æ—¥æœŸçš„å…·ä½“é“è·¯
        day_roads = flood_df[flood_df["date"] == date]["STREET"].unique()
        print(f"{i:2d}. {date} - {row['unique_roads']:2d}æ¡é“è·¯, {row['total_records']:2d}æ¡è®°å½•")
        if len(day_roads) <= 8:
            print(f"    é“è·¯: {', '.join(day_roads)}")
        else:
            print(f"    é“è·¯: {', '.join(day_roads[:8])}... (å…±{len(day_roads)}æ¡)")
        print()
    
    # 5. ä¿å­˜ç»“æœ
    top10_dates.to_csv("top10_flood_dates.csv")
    
    # 6. åŸºæœ¬ç»Ÿè®¡
    print("ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
    print(f"   æ€»æ´ªæ°´è®°å½•: {len(flood_df)}")
    print(f"   æ´ªæ°´æ—¥æœŸæ•°: {len(daily_stats)}")
    print(f"   å¹³å‡æ¯æ—¥é“è·¯æ•°: {daily_stats['unique_roads'].mean():.1f}")
    print(f"   æœ€å¤§å•æ—¥é“è·¯æ•°: {daily_stats['unique_roads'].max()}")
    
    return top10_dates

if __name__ == "__main__":
    results = analyze_daily_flood_events()