#!/usr/bin/env python3
"""
Comprehensive Parameter Grid Search
å…¨é¢çš„å‚æ•°ç½‘æ ¼æœç´¢æ¨¡å—

æ‰§è¡Œè´å¶æ–¯ç½‘ç»œæ´ªæ°´é¢„æµ‹æ¨¡å‹çš„å…¨å‚æ•°ç©ºé—´æœç´¢ï¼Œè¯„ä¼°æ‰€æœ‰å¯èƒ½çš„å‚æ•°ç»„åˆ
å¹¶ä¿å­˜è¯¦ç»†çš„æ€§èƒ½ç»“æœã€‚

ä½œè€…ï¼šClaude AI
æ—¥æœŸï¼š2025-01-21
"""

import random
import numpy as np
import pandas as pd
import warnings
from datetime import datetime
import itertools
import time
import json
import os
import sys
from collections import defaultdict
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from model import FloodBayesNetwork
except ImportError:
    try:
        from core.model import FloodBayesNetwork
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥FloodBayesNetworkï¼Œè¯·ç¡®ä¿model.pyæˆ–core/model.pyå­˜åœ¨")
        sys.exit(1)

# è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class ParameterGridSearcher:
    """å‚æ•°ç½‘æ ¼æœç´¢å™¨"""
    
    def __init__(self, param_grid=None, use_validation_split=True):
        """
        åˆå§‹åŒ–å‚æ•°ç½‘æ ¼æœç´¢å™¨
        
        Args:
            param_grid (dict): å‚æ•°ç½‘æ ¼å­—å…¸
            use_validation_split (bool): æ˜¯å¦ä½¿ç”¨éªŒè¯é›†åˆ†å‰²
        """
        self.param_grid = param_grid or self._get_default_param_grid()
        self.use_validation_split = use_validation_split
        self.results = []
        self.experiment_config = {}
        
    def _get_default_param_grid(self):
        """è·å–é»˜è®¤å‚æ•°ç½‘æ ¼"""
        return {
            # ç½‘ç»œæ„å»ºå‚æ•°
            'occ_thr': [2, 3, 4, 5],           # é“è·¯æœ€å°å‡ºç°æ¬¡æ•°
            'edge_thr': [1, 2, 3],             # è¾¹çš„æœ€å°å…±ç°æ¬¡æ•°  
            'weight_thr': [0.2, 0.3, 0.4, 0.5], # è¾¹æƒé‡é˜ˆå€¼
            
            # è¯„ä¼°å‚æ•°
            'evidence_count': [1, 2, 3, 4],    # è¯æ®é“è·¯æ•°é‡
            'pred_threshold': [0.1, 0.2, 0.3, 0.4, 0.5], # é¢„æµ‹é˜ˆå€¼
            
            # è´Ÿæ ·æœ¬ç­–ç•¥
            'neg_pos_ratio': [1.0, 1.5, 2.0], # è´Ÿæ­£æ ·æœ¬æ¯”ä¾‹
            'marginal_prob_threshold': [0.03, 0.05, 0.08] # è¾¹é™…æ¦‚ç‡é˜ˆå€¼
        }
    
    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("="*80)
        print("åŠ è½½å’Œé¢„å¤„ç†æ´ªæ°´æ•°æ®")
        print("="*80)
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv("Road_Closures_2024.csv")
        df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        
        # é¢„å¤„ç†
        df["time_create"] = pd.to_datetime(df["START"], utc=True)
        df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
        df["link_id"] = df["link_id"].astype(str)
        df["id"] = df["OBJECTID"].astype(str)
        df["year"] = df["time_create"].dt.year
        df['flood_date'] = df['time_create'].dt.floor('D')
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡æ´ªæ°´è®°å½•")
        print(f"æ—¶é—´èŒƒå›´: {df['time_create'].min().strftime('%Y-%m-%d')} åˆ° {df['time_create'].max().strftime('%Y-%m-%d')}")
        print(f"å”¯ä¸€é“è·¯æ•°: {df['link_id'].nunique()}")
        print(f"æ´ªæ°´å¤©æ•°: {df['flood_date'].nunique()}")
        
        return df
    
    def split_data_by_flood_days(self, df, train_ratio=0.6, valid_ratio=0.2, test_ratio=0.2):
        """æŒ‰æ´ªæ°´å¤©æ•°è¿›è¡Œæ—¶é—´åˆ†å‰²"""
        print(f"\n{'='*60}")
        print("æŒ‰æ´ªæ°´å¤©æ•°è¿›è¡Œæ—¶é—´åˆ†å‰²")
        print(f"{'='*60}")
        
        # æŒ‰æ´ªæ°´å¤©åˆ†ç»„
        flood_days = df.groupby('flood_date').size().sort_index()
        unique_days = flood_days.index.tolist()
        
        print(f"æ€»æ´ªæ°´å¤©æ•°: {len(unique_days)}")
        print(f"æ—¥æœŸèŒƒå›´: {unique_days[0].strftime('%Y-%m-%d')} åˆ° {unique_days[-1].strftime('%Y-%m-%d')}")
        
        # æ—¶é—´åˆ†å‰²
        n_days = len(unique_days)
        train_end = int(n_days * train_ratio)
        valid_end = int(n_days * (train_ratio + valid_ratio))
        
        train_days = unique_days[:train_end]
        valid_days = unique_days[train_end:valid_end] if self.use_validation_split else []
        test_days = unique_days[valid_end:] if self.use_validation_split else unique_days[train_end:]
        
        # åˆ†å‰²æ•°æ®
        train_df = df[df['flood_date'].isin(train_days)].copy()
        valid_df = df[df['flood_date'].isin(valid_days)].copy() if self.use_validation_split else pd.DataFrame()
        test_df = df[df['flood_date'].isin(test_days)].copy()
        
        print(f"è®­ç»ƒé›†: {len(train_days)} å¤©, {len(train_df)} æ¡è®°å½•")
        if self.use_validation_split:
            print(f"éªŒè¯é›†: {len(valid_days)} å¤©, {len(valid_df)} æ¡è®°å½•")
        print(f"æµ‹è¯•é›†: {len(test_days)} å¤©, {len(test_df)} æ¡è®°å½•")
        
        return train_df, valid_df, test_df
    
    def build_bayesian_network(self, train_df, occ_thr, edge_thr, weight_thr):
        """æ„å»ºè´å¶æ–¯ç½‘ç»œ"""
        try:
            # æ„å»ºç½‘ç»œ
            flood_net = FloodBayesNetwork(t_window="D")
            flood_net.fit_marginal(train_df)
            flood_net.build_network_by_co_occurrence(
                train_df, 
                occ_thr=occ_thr, 
                edge_thr=edge_thr, 
                weight_thr=weight_thr, 
                report=False
            )
            flood_net.fit_conditional(train_df, max_parents=2, alpha=1.0)
            flood_net.build_bayes_network()
            
            return flood_net, True, ""
        except Exception as e:
            return None, False, str(e)
    
    def evaluate_network(self, flood_net, test_df, evidence_count, pred_threshold, 
                        neg_pos_ratio, marginal_prob_threshold):
        """è¯„ä¼°ç½‘ç»œæ€§èƒ½"""
        try:
            bn_nodes = set(flood_net.network_bayes.nodes())
            marginals_dict = dict(zip(flood_net.marginals['link_id'], flood_net.marginals['p']))
            
            # è·å–è´Ÿæ ·æœ¬å€™é€‰
            negative_candidates = [
                road for road, prob in marginals_dict.items() 
                if road in bn_nodes and prob <= marginal_prob_threshold
            ]
            
            if len(negative_candidates) < 2:
                return None, "è´Ÿæ ·æœ¬å€™é€‰ä¸è¶³"
            
            # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®
            test_by_date = test_df.groupby(test_df["flood_date"])
            
            predictions = []
            valid_days = 0
            total_days = 0
            
            for date, day_group in test_by_date:
                total_days += 1
                
                # å½“å¤©æ´ªæ°´é“è·¯åˆ—è¡¨
                flooded_roads = list(day_group["link_id"].unique())
                flooded_in_bn = [road for road in flooded_roads if road in bn_nodes]
                
                if len(flooded_in_bn) < evidence_count + 1:
                    continue
                    
                valid_days += 1
                
                # Evidenceé€‰æ‹©
                evidence_roads = flooded_in_bn[:evidence_count]
                target_roads = flooded_in_bn[evidence_count:]
                
                evidence = {road: 1 for road in evidence_roads}
                
                # å¤„ç†æ­£æ ·æœ¬
                for target_road in target_roads:
                    try:
                        result = flood_net.infer_w_evidence(target_road, evidence)
                        prob_flood = result['flooded']
                        
                        predictions.append({
                            'type': 'positive',
                            'road': target_road,
                            'true_label': 1,
                            'prob_flood': prob_flood,
                            'date': date
                        })
                    except:
                        continue
                
                # å¤„ç†è´Ÿæ ·æœ¬
                available_negatives = [road for road in negative_candidates if road not in flooded_roads]
                neg_count = min(len(available_negatives), int(len(target_roads) * neg_pos_ratio))
                selected_negatives = available_negatives[:neg_count]
                
                for neg_road in selected_negatives:
                    try:
                        result = flood_net.infer_w_evidence(neg_road, evidence)
                        prob_flood = result['flooded']
                        
                        predictions.append({
                            'type': 'negative',
                            'road': neg_road,
                            'true_label': 0,
                            'prob_flood': prob_flood,
                            'date': date
                        })
                    except:
                        continue
            
            if len(predictions) < 10:
                return None, "é¢„æµ‹æ ·æœ¬ä¸è¶³"
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            tp = fp = tn = fn = 0
            
            for pred in predictions:
                prob = pred['prob_flood']
                true_label = pred['true_label']
                
                # åº”ç”¨é˜ˆå€¼å†³ç­–
                if prob >= pred_threshold:
                    prediction = 1
                else:
                    prediction = 0
                
                # è®¡ç®—æ··æ·†çŸ©é˜µ
                if prediction == 1 and true_label == 1:
                    tp += 1
                elif prediction == 1 and true_label == 0:
                    fp += 1
                elif prediction == 0 and true_label == 1:
                    fn += 1
                elif prediction == 0 and true_label == 0:
                    tn += 1
            
            # è®¡ç®—æŒ‡æ ‡
            total_samples = tp + fp + tn + fn
            positive_samples = sum(1 for p in predictions if p['true_label'] == 1)
            negative_samples = sum(1 for p in predictions if p['true_label'] == 0)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            accuracy = (tp + tn) / total_samples if total_samples > 0 else 0.0
            
            metrics = {
                'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,
                'total_samples': total_samples,
                'positive_samples': positive_samples,
                'negative_samples': negative_samples,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'accuracy': accuracy,
                'valid_days': valid_days,
                'total_days': total_days,
                'network_nodes': len(bn_nodes),
                'negative_candidates_count': len(negative_candidates)
            }
            
            return metrics, "æˆåŠŸ"
            
        except Exception as e:
            return None, f"è¯„ä¼°å¤±è´¥: {str(e)}"
    
    def run_grid_search(self, save_dir="results"):
        """è¿è¡Œç½‘æ ¼æœç´¢"""
        print("="*80)
        print("å¼€å§‹å‚æ•°ç½‘æ ¼æœç´¢")
        print("="*80)
        
        # åˆ›å»ºç»“æœç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_dir = f"{save_dir}/parameter_optimization_{timestamp}"
        os.makedirs(result_dir, exist_ok=True)
        
        # ä¿å­˜å®éªŒé…ç½®
        self.experiment_config = {
            'timestamp': timestamp,
            'param_grid': self.param_grid,
            'use_validation_split': self.use_validation_split,
            'random_seed': RANDOM_SEED
        }
        
        with open(f"{result_dir}/experiment_config.json", 'w') as f:
            json.dump(self.experiment_config, f, indent=2, ensure_ascii=False)
        
        # åŠ è½½æ•°æ®
        df = self.load_and_preprocess_data()
        train_df, valid_df, test_df = self.split_data_by_flood_days(df)
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_names = list(self.param_grid.keys())
        param_values = list(self.param_grid.values())
        all_combinations = list(itertools.product(*param_values))
        
        total_combinations = len(all_combinations)
        print(f"\næ€»å‚æ•°ç»„åˆæ•°: {total_combinations}")
        
        # ç½‘æ ¼æœç´¢
        results = []
        failed_count = 0
        
        for i, combination in enumerate(all_combinations):
            param_dict = dict(zip(param_names, combination))
            
            print(f"\nè¿›åº¦: {i+1}/{total_combinations} ({(i+1)/total_combinations*100:.1f}%)")
            print(f"å½“å‰å‚æ•°: {param_dict}")
            
            start_time = time.time()
            
            # æ„å»ºç½‘ç»œ
            flood_net, build_success, build_error = self.build_bayesian_network(
                train_df, param_dict['occ_thr'], param_dict['edge_thr'], param_dict['weight_thr']
            )
            
            if not build_success:
                print(f"âŒ ç½‘ç»œæ„å»ºå¤±è´¥: {build_error}")
                failed_count += 1
                continue
            
            # è¯„ä¼°ç½‘ç»œ
            metrics, eval_error = self.evaluate_network(
                flood_net, test_df, 
                param_dict['evidence_count'], param_dict['pred_threshold'],
                param_dict['neg_pos_ratio'], param_dict['marginal_prob_threshold']
            )
            
            runtime = time.time() - start_time
            
            if metrics is None:
                print(f"âŒ è¯„ä¼°å¤±è´¥: {eval_error}")
                failed_count += 1
                continue
            
            # åˆå¹¶ç»“æœ
            result = {**param_dict, **metrics, 'runtime_seconds': runtime}
            results.append(result)
            
            print(f"âœ… æˆåŠŸ - P:{metrics['precision']:.3f}, R:{metrics['recall']:.3f}, F1:{metrics['f1_score']:.3f}")
            
            # æ¯100ä¸ªç»„åˆä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if len(results) % 100 == 0:
                self._save_intermediate_results(results, result_dir)
        
        print(f"\n="*80)
        print("ç½‘æ ¼æœç´¢å®Œæˆ!")
        print(f"æˆåŠŸè¯„ä¼°çš„ç»„åˆ: {len(results)}")
        print(f"å¤±è´¥çš„ç»„åˆ: {failed_count}")
        print(f"æˆåŠŸç‡: {len(results)/(len(results)+failed_count)*100:.1f}%")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(results, result_dir)
        
        self.results = results
        return results, result_dir
    
    def _save_intermediate_results(self, results, result_dir):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        if results:
            df_results = pd.DataFrame(results)
            df_results.to_csv(f"{result_dir}/intermediate_results.csv", index=False)
            print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {len(results)} ä¸ªç»„åˆ")
    
    def _save_final_results(self, results, result_dir):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        if not results:
            print("âš ï¸ æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ä¿å­˜")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df_results = pd.DataFrame(results)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        df_results.to_csv(f"{result_dir}/complete_results.csv", index=False)
        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {result_dir}/complete_results.csv")
        
        # ä¿å­˜æ€§èƒ½æ‘˜è¦
        summary = {
            'total_combinations_tested': len(results),
            'best_f1_score': df_results['f1_score'].max(),
            'best_precision': df_results['precision'].max(),
            'best_recall': df_results['recall'].max(),
            'parameter_ranges': {
                param: {
                    'min': df_results[param].min(),
                    'max': df_results[param].max(),
                    'unique_values': sorted(df_results[param].unique().tolist())
                } for param in self.param_grid.keys()
            },
            'performance_statistics': {
                'f1_score': {
                    'mean': df_results['f1_score'].mean(),
                    'std': df_results['f1_score'].std(),
                    'min': df_results['f1_score'].min(),
                    'max': df_results['f1_score'].max()
                },
                'precision': {
                    'mean': df_results['precision'].mean(),
                    'std': df_results['precision'].std(),
                    'min': df_results['precision'].min(),
                    'max': df_results['precision'].max()
                },
                'recall': {
                    'mean': df_results['recall'].mean(),
                    'std': df_results['recall'].std(),
                    'min': df_results['recall'].min(),
                    'max': df_results['recall'].max()
                }
            }
        }
        
        with open(f"{result_dir}/performance_summary.json", 'w') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ€§èƒ½æ‘˜è¦å·²ä¿å­˜: {result_dir}/performance_summary.json")
        
        # åˆ é™¤ä¸­é—´ç»“æœæ–‡ä»¶
        intermediate_file = f"{result_dir}/intermediate_results.csv"
        if os.path.exists(intermediate_file):
            os.remove(intermediate_file)

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    # åˆ›å»ºç½‘æ ¼æœç´¢å™¨
    searcher = ParameterGridSearcher()
    
    # è¿è¡Œç½‘æ ¼æœç´¢
    results, result_dir = searcher.run_grid_search()
    
    print(f"\nğŸ‰ å‚æ•°ç½‘æ ¼æœç´¢å®Œæˆï¼")
    print(f"ç»“æœä¿å­˜åœ¨: {result_dir}")
    
    return searcher, results, result_dir

if __name__ == "__main__":
    searcher, results, result_dir = main()