#!/usr/bin/env python3
"""
è¯¦ç»†åˆ†æï¼šé€æ˜åŒ–è´å¶æ–¯ç½‘ç»œæ„å»ºå’Œè¯„ä¼°è¿‡ç¨‹

è¾“å‡ºæ‰€æœ‰ä¸­é—´æ­¥éª¤ï¼Œè®©ç”¨æˆ·èƒ½å¤ŸéªŒè¯ç»“æœçš„çœŸå®æ€§å’Œå¯é æ€§ï¼š
1. ç½‘ç»œæ„å»ºè¿‡ç¨‹è¯¦ç»†åˆ†æ
2. è®­ç»ƒæ•°æ®æ·±åº¦ç»Ÿè®¡  
3. è¾¹é™…æ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡è¡¨å±•ç¤º
4. æ•°æ®åˆ†å¸ƒå’Œè´¨é‡éªŒè¯
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class DetailedNetworkAnalyzer:
    """è¯¦ç»†çš„ç½‘ç»œåˆ†æå™¨"""
    
    def __init__(self):
        self.train_df = None
        self.test_df = None
        self.flood_net = None
        
    def load_and_analyze_data(self):
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œè¯¦ç»†åˆ†æ"""
        print("ğŸ” æ•°æ®åŠ è½½å’Œé¢„å¤„ç†è¯¦ç»†åˆ†æ")
        print("=" * 60)
        
        # åŠ è½½åŸå§‹æ•°æ®
        df = pd.read_csv("Road_Closures_2024.csv")
        print(f"åŸå§‹æ•°æ®: {len(df)}æ¡è®°å½•")
        
        # è¿‡æ»¤æ´ªæ°´è®°å½•
        flood_df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        print(f"æ´ªæ°´è®°å½•: {len(flood_df)}æ¡ ({len(flood_df)/len(df)*100:.1f}%)")
        
        # æ•°æ®é¢„å¤„ç†
        flood_df["time_create"] = pd.to_datetime(flood_df["START"], utc=True)
        flood_df["link_id"] = flood_df["STREET"].str.upper().str.replace(" ", "_")
        flood_df["link_id"] = flood_df["link_id"].astype(str)
        flood_df["id"] = flood_df["OBJECTID"].astype(str)
        
        # æ—¶é—´åˆ†æ
        print(f"\nğŸ“… æ—¶é—´èŒƒå›´åˆ†æ:")
        print(f"   å¼€å§‹æ—¶é—´: {flood_df['time_create'].min()}")
        print(f"   ç»“æŸæ—¶é—´: {flood_df['time_create'].max()}")
        print(f"   æ—¶é—´è·¨åº¦: {(flood_df['time_create'].max() - flood_df['time_create'].min()).days}å¤©")
        
        # æŒ‰æœˆç»Ÿè®¡
        monthly_counts = flood_df.groupby(flood_df['time_create'].dt.to_period('M')).size()
        print(f"\nğŸ“Š æœˆåº¦æ´ªæ°´è®°å½•åˆ†å¸ƒ:")\n        for month, count in monthly_counts.items():\n            print(f\"   {month}: {count}æ¡\")\n        \n        # é“è·¯åˆ†æ\n        road_counts = flood_df['link_id'].value_counts()\n        print(f\"\\nğŸ›£ï¸  é“è·¯åˆ†æ:\")\n        print(f\"   ç‹¬ç‰¹é“è·¯æ•°: {len(road_counts)}æ¡\")\n        print(f\"   å¹³å‡æ¯æ¡é“è·¯æ´ªæ°´æ¬¡æ•°: {road_counts.mean():.1f}\")\n        print(f\"   æœ€é«˜æ´ªæ°´æ¬¡æ•°: {road_counts.max()}æ¬¡ (é“è·¯: {road_counts.index[0]})\")\n        \n        print(f\"\\nğŸ” æ´ªæ°´é¢‘æ¬¡TOP-10é“è·¯:\")\n        for i, (road, count) in enumerate(road_counts.head(10).items(), 1):\n            print(f\"   {i:2d}. {road:<25} {count:3d}æ¬¡\")\n        \n        # æ—¶åºåˆ†å‰²è¯¦ç»†åˆ†æ\n        df_sorted = flood_df.sort_values('time_create')\n        split_idx = int(len(df_sorted) * 0.7)\n        self.train_df = df_sorted.iloc[:split_idx].copy()\n        self.test_df = df_sorted.iloc[split_idx:].copy()\n        \n        print(f\"\\nâœ‚ï¸  æ—¶åºåˆ†å‰²è¯¦æƒ…:\")\n        print(f\"   è®­ç»ƒé›†: {len(self.train_df)}æ¡ ({len(self.train_df)/len(flood_df)*100:.1f}%)\")\n        print(f\"   æµ‹è¯•é›†: {len(self.test_df)}æ¡ ({len(self.test_df)/len(flood_df)*100:.1f}%)\")\n        print(f\"   è®­ç»ƒæ—¶é—´æ®µ: {self.train_df['time_create'].min().date()} è‡³ {self.train_df['time_create'].max().date()}\")\n        print(f\"   æµ‹è¯•æ—¶é—´æ®µ: {self.test_df['time_create'].min().date()} è‡³ {self.test_df['time_create'].max().date()}\")\n        \n        # è®­ç»ƒé›†vsæµ‹è¯•é›†é“è·¯é‡å åˆ†æ\n        train_roads = set(self.train_df['link_id'].unique())\n        test_roads = set(self.test_df['link_id'].unique())\n        overlap_roads = train_roads & test_roads\n        \n        print(f\"\\nğŸ”„ è®­ç»ƒé›†vsæµ‹è¯•é›†é“è·¯åˆ†æ:\")\n        print(f\"   è®­ç»ƒé›†ç‹¬ç‰¹é“è·¯: {len(train_roads)}æ¡\")\n        print(f\"   æµ‹è¯•é›†ç‹¬ç‰¹é“è·¯: {len(test_roads)}æ¡\")\n        print(f\"   é‡å é“è·¯: {len(overlap_roads)}æ¡ ({len(overlap_roads)/len(train_roads)*100:.1f}% of è®­ç»ƒé›†)\")\n        print(f\"   æµ‹è¯•é›†æ–°é“è·¯: {len(test_roads - train_roads)}æ¡\")\n        \n        return flood_df\n        \n    def analyze_network_construction(self):\n        \"\"\"è¯¦ç»†åˆ†æç½‘ç»œæ„å»ºè¿‡ç¨‹\"\"\"\n        print(f\"\\n\\nğŸ—ï¸  è´å¶æ–¯ç½‘ç»œæ„å»ºè¯¦ç»†è¿‡ç¨‹\")\n        print(\"=\" * 60)\n        \n        # 1. åˆ›å»ºç½‘ç»œå¹¶è®¡ç®—è¾¹é™…æ¦‚ç‡\n        self.flood_net = FloodBayesNetwork(t_window=\"D\")\n        self.flood_net.fit_marginal(self.train_df)\n        \n        print(f\"\\n1ï¸âƒ£  è¾¹é™…æ¦‚ç‡è®¡ç®—:\")\n        marginals_sorted = self.flood_net.marginals.sort_values('p', ascending=False)\n        print(f\"   åŸºäºè®­ç»ƒé›†çš„{len(self.train_df)}æ¡è®°å½•\")\n        print(f\"   è®¡ç®—äº†{len(marginals_sorted)}æ¡é“è·¯çš„è¾¹é™…æ¦‚ç‡\")\n        \n        print(f\"\\nğŸ“Š è¾¹é™…æ¦‚ç‡åˆ†å¸ƒ:\")\n        prob_ranges = [(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 1.0)]\n        for low, high in prob_ranges:\n            count = len(marginals_sorted[(marginals_sorted['p'] >= low) & (marginals_sorted['p'] < high)])\n            print(f\"   {low:.1f}-{high:.1f}: {count:2d}æ¡é“è·¯\")\n        \n        print(f\"\\nğŸ” è¾¹é™…æ¦‚ç‡TOP-15é“è·¯:\")\n        for i, (_, row) in enumerate(marginals_sorted.head(15).iterrows(), 1):\n            print(f\"   {i:2d}. {row['link_id']:<25} P={row['p']:.3f}\")\n        \n        print(f\"\\nğŸ”» è¾¹é™…æ¦‚ç‡BOTTOM-10é“è·¯:\")\n        for i, (_, row) in enumerate(marginals_sorted.tail(10).iterrows(), 1):\n            print(f\"   {i:2d}. {row['link_id']:<25} P={row['p']:.3f}\")\n        \n        # 2. å…±ç°åˆ†æ\n        print(f\"\\n2ï¸âƒ£  å…±ç°ç½‘ç»œæ„å»º (å‚æ•°: occ_thr=3, edge_thr=2, weight_thr=0.3):\")\n        \n        # æ˜¾ç¤ºæ„å»ºå‰çš„ç»Ÿè®¡\n        time_groups, occurrence, co_occurrence = self.flood_net.process_raw_flood_data(self.train_df.copy())\n        \n        print(f\"   è®­ç»ƒæ•°æ®åŒ…å«{len(time_groups)}ä¸ªæ´ªæ°´æ—¥\")\n        print(f\"   é“è·¯å‡ºç°ç»Ÿè®¡: {len(occurrence)}æ¡é“è·¯\")\n        print(f\"   å…±ç°å¯¹ç»Ÿè®¡: {len(co_occurrence)}ä¸ªé“è·¯å¯¹\")\n        \n        # æ˜¾ç¤ºå‡ºç°é¢‘æ¬¡åˆ†å¸ƒ\n        occ_counts = Counter(occurrence.values())\n        print(f\"\\nğŸ“ˆ é“è·¯å‡ºç°é¢‘æ¬¡åˆ†å¸ƒ:\")\n        for freq in sorted(occ_counts.keys(), reverse=True)[:10]:\n            print(f\"   å‡ºç°{freq}æ¬¡: {occ_counts[freq]}æ¡é“è·¯\")\n        \n        # æ„å»ºç½‘ç»œ\n        self.flood_net.build_network_by_co_occurrence(\n            self.train_df,\n            occ_thr=3,\n            edge_thr=2,\n            weight_thr=0.3,\n            report=False\n        )\n        \n        print(f\"\\nâœ… ç½‘ç»œæ„å»ºå®Œæˆ:\")\n        print(f\"   èŠ‚ç‚¹æ•°: {self.flood_net.network.number_of_nodes()}\")\n        print(f\"   è¾¹æ•°: {self.flood_net.network.number_of_edges()}\")\n        \n        # åˆ†æè¢«è¿‡æ»¤æ‰çš„é“è·¯\n        all_roads = set(occurrence.keys())\n        network_roads = set(self.flood_net.network.nodes())\n        filtered_roads = all_roads - network_roads\n        \n        print(f\"\\nğŸš« è¿‡æ»¤åˆ†æ:\")\n        print(f\"   åŸå§‹é“è·¯æ•°: {len(all_roads)}\")\n        print(f\"   ä¿ç•™é“è·¯æ•°: {len(network_roads)}\")\n        print(f\"   è¿‡æ»¤é“è·¯æ•°: {len(filtered_roads)} ({len(filtered_roads)/len(all_roads)*100:.1f}%)\")\n        \n        if len(filtered_roads) > 0:\n            print(f\"\\nğŸ” éƒ¨åˆ†è¢«è¿‡æ»¤é“è·¯ (å‡ºç°æ¬¡æ•°<3):\")\n            filtered_with_count = [(road, occurrence[road]) for road in filtered_roads]\n            filtered_with_count.sort(key=lambda x: x[1], reverse=True)\n            for road, count in filtered_with_count[:10]:\n                print(f\"   {road:<25} å‡ºç°{count}æ¬¡\")\n        \n        # ç½‘ç»œæ‹“æ‰‘åˆ†æ\n        print(f\"\\n3ï¸âƒ£  ç½‘ç»œæ‹“æ‰‘ç‰¹å¾:\")\n        \n        # åº¦åˆ†å¸ƒ\n        in_degrees = dict(self.flood_net.network.in_degree())\n        out_degrees = dict(self.flood_net.network.out_degree())\n        \n        print(f\"   å¹³å‡å…¥åº¦: {np.mean(list(in_degrees.values())):.2f}\")\n        print(f\"   å¹³å‡å‡ºåº¦: {np.mean(list(out_degrees.values())):.2f}\")\n        print(f\"   æœ€å¤§å…¥åº¦: {max(in_degrees.values())} (èŠ‚ç‚¹: {max(in_degrees, key=in_degrees.get)})\")\n        print(f\"   æœ€å¤§å‡ºåº¦: {max(out_degrees.values())} (èŠ‚ç‚¹: {max(out_degrees, key=out_degrees.get)})\")\n        \n        # æ˜¾ç¤ºé«˜åº¦èŠ‚ç‚¹\n        print(f\"\\nğŸŒŸ é«˜å…¥åº¦èŠ‚ç‚¹ (å®¹æ˜“è¢«å…¶ä»–é“è·¯å½±å“):\")\n        high_in_degree = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:8]\n        for road, degree in high_in_degree:\n            marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == road]['p'].iloc[0]\n            print(f\"   {road:<25} å…¥åº¦={degree}, P(æ´ªæ°´)={marginal_p:.3f}\")\n        \n        print(f\"\\nğŸŒŸ é«˜å‡ºåº¦èŠ‚ç‚¹ (å®¹æ˜“å½±å“å…¶ä»–é“è·¯):\")\n        high_out_degree = sorted(out_degrees.items(), key=lambda x: x[1], reverse=True)[:8]\n        for road, degree in high_out_degree:\n            marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == road]['p'].iloc[0]\n            print(f\"   {road:<25} å‡ºåº¦={degree}, P(æ´ªæ°´)={marginal_p:.3f}\")\n        \n        # è¾¹æƒåˆ†æ\n        edge_weights = [d['weight'] for u, v, d in self.flood_net.network.edges(data=True)]\n        print(f\"\\nğŸ”— è¾¹æƒåˆ†æ:\")\n        print(f\"   è¾¹æƒèŒƒå›´: {min(edge_weights):.3f} - {max(edge_weights):.3f}\")\n        print(f\"   å¹³å‡è¾¹æƒ: {np.mean(edge_weights):.3f}\")\n        print(f\"   è¾¹æƒä¸­ä½æ•°: {np.median(edge_weights):.3f}\")\n        \n        # æ˜¾ç¤ºæœ€å¼ºè¿æ¥\n        edge_list = [(u, v, d['weight']) for u, v, d in self.flood_net.network.edges(data=True)]\n        edge_list.sort(key=lambda x: x[2], reverse=True)\n        \n        print(f\"\\nğŸ’ª æœ€å¼ºä¾èµ–å…³ç³»TOP-10:\")\n        for i, (u, v, weight) in enumerate(edge_list[:10], 1):\n            print(f\"   {i:2d}. {u} â†’ {v} (æƒé‡={weight:.3f})\")\n        \n        return network_roads\n    \n    def analyze_conditional_probabilities(self):\n        \"\"\"åˆ†ææ¡ä»¶æ¦‚ç‡è¡¨\"\"\"\n        print(f\"\\n\\n4ï¸âƒ£  æ¡ä»¶æ¦‚ç‡è¡¨ (CPT) æ„å»ºè¯¦ç»†è¿‡ç¨‹\")\n        print(\"=\" * 60)\n        \n        # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡\n        self.flood_net.fit_conditional(self.train_df, max_parents=2, alpha=1.0)\n        \n        print(f\"å‚æ•°: max_parents=2, alpha=1.0 (æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘)\")\n        \n        # åˆ†æCPTç»Ÿè®¡\n        nodes_with_parents = 0\n        nodes_without_parents = 0\n        total_cpt_entries = 0\n        \n        for node in self.flood_net.network.nodes():\n            parents = list(self.flood_net.network.predecessors(node))\n            if len(parents) > 0 and node in self.flood_net.conditionals:\n                nodes_with_parents += 1\n                cpt_size = 2 ** len(self.flood_net.conditionals[node]['parents'])\n                total_cpt_entries += cpt_size\n            else:\n                nodes_without_parents += 1\n        \n        print(f\"\\nğŸ“Š CPTç»Ÿè®¡:\")\n        print(f\"   æœ‰çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹: {nodes_with_parents}ä¸ª\")\n        print(f\"   æ— çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹: {nodes_without_parents}ä¸ª\")\n        print(f\"   æ€»CPTæ¡ç›®æ•°: {total_cpt_entries}ä¸ª\")\n        \n        # æ˜¾ç¤ºå‡ ä¸ªå…·ä½“çš„CPTä¾‹å­\n        print(f\"\\nğŸ“‹ æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºä¾‹:\")\n        \n        cpt_examples = 0\n        for node in self.flood_net.network.nodes():\n            if node in self.flood_net.conditionals and cpt_examples < 3:\n                cpt_examples += 1\n                cfg = self.flood_net.conditionals[node]\n                parents = cfg['parents']\n                conditionals = cfg['conditionals']\n                \n                marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == node]['p'].iloc[0]\n                print(f\"\\n   èŠ‚ç‚¹: {node} (è¾¹é™…æ¦‚ç‡={marginal_p:.3f})\")\n                print(f\"   çˆ¶èŠ‚ç‚¹: {parents}\")\n                print(f\"   æ¡ä»¶æ¦‚ç‡:\")\n                \n                for state, prob in conditionals.items():\n                    parent_state_str = \", \".join([f\"{p}={s}\" for p, s in zip(parents, state)])\n                    print(f\"     P({node}=1 | {parent_state_str}) = {prob:.3f}\")\n        \n        # æ„å»ºæœ€ç»ˆçš„è´å¶æ–¯ç½‘ç»œ\n        self.flood_net.build_bayes_network()\n        print(f\"\\nâœ… è´å¶æ–¯ç½‘ç»œæ„å»ºå®Œæˆï¼Œå‡†å¤‡è¿›è¡Œæ¨ç†\")\n        \n    def analyze_test_data_structure(self):\n        \"\"\"åˆ†ææµ‹è¯•æ•°æ®ç»“æ„\"\"\"\n        print(f\"\\n\\nğŸ§ª æµ‹è¯•æ•°æ®ç»“æ„è¯¦ç»†åˆ†æ\")\n        print(\"=\" * 60)\n        \n        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®\n        test_by_date = self.test_df.groupby(self.test_df[\"time_create\"].dt.floor(\"D\"))\n        \n        print(f\"æµ‹è¯•æ•°æ®è¦†ç›–{len(test_by_date)}ä¸ªæ´ªæ°´æ—¥\")\n        \n        # ç»Ÿè®¡æ¯æ—¥æ´ªæ°´é“è·¯æ•°é‡\n        daily_road_counts = []\n        daily_network_road_counts = []\n        network_roads = set(self.flood_net.network.nodes())\n        \n        print(f\"\\nğŸ“… æµ‹è¯•æ—¥æœŸè¯¦ç»†åˆ†è§£:\")\n        for i, (date, day_group) in enumerate(test_by_date):\n            flooded_roads = list(day_group[\"link_id\"].unique())\n            flooded_in_network = [road for road in flooded_roads if road in network_roads]\n            \n            daily_road_counts.append(len(flooded_roads))\n            daily_network_road_counts.append(len(flooded_in_network))\n            \n            if i < 10:  # æ˜¾ç¤ºå‰10å¤©çš„è¯¦æƒ…\n                print(f\"   {date.date()}: {len(flooded_roads)}æ¡é“è·¯æ´ªæ°´, {len(flooded_in_network)}æ¡åœ¨ç½‘ç»œä¸­\")\n                if len(flooded_in_network) > 0:\n                    print(f\"      ç½‘ç»œé“è·¯: {', '.join(flooded_in_network[:5])}{'...' if len(flooded_in_network) > 5 else ''}\")\n        \n        if len(test_by_date) > 10:\n            print(f\"   ... (è¿˜æœ‰{len(test_by_date)-10}å¤©)\")\n        \n        print(f\"\\nğŸ“Š æµ‹è¯•æ•°æ®ç»Ÿè®¡:\")\n        print(f\"   å¹³å‡æ¯æ—¥æ´ªæ°´é“è·¯: {np.mean(daily_road_counts):.1f}æ¡\")\n        print(f\"   å¹³å‡æ¯æ—¥ç½‘ç»œé“è·¯: {np.mean(daily_network_road_counts):.1f}æ¡\")\n        print(f\"   å¯è¯„ä¼°æ—¥æ•° (â‰¥2æ¡ç½‘ç»œé“è·¯): {sum(1 for x in daily_network_road_counts if x >= 2)}å¤©\")\n        \n        # æµ‹è¯•é“è·¯é¢‘æ¬¡åˆ†æ\n        test_road_counts = self.test_df['link_id'].value_counts()\n        test_network_roads = test_road_counts[test_road_counts.index.isin(network_roads)]\n        \n        print(f\"\\nğŸ›£ï¸  æµ‹è¯•é›†é“è·¯åˆ†æ:\")\n        print(f\"   æµ‹è¯•é›†ç‹¬ç‰¹é“è·¯: {len(test_road_counts)}æ¡\")\n        print(f\"   æµ‹è¯•é›†ç½‘ç»œé“è·¯: {len(test_network_roads)}æ¡\")\n        print(f\"   è¦†ç›–ç‡: {len(test_network_roads)/len(network_roads)*100:.1f}% (ç½‘ç»œé“è·¯åœ¨æµ‹è¯•é›†ä¸­å‡ºç°)\")\n        \n        print(f\"\\nğŸ” æµ‹è¯•é›†é«˜é¢‘æ´ªæ°´é“è·¯TOP-10:\")\n        for i, (road, count) in enumerate(test_network_roads.head(10).items(), 1):\n            marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == road]['p'].iloc[0]\n            print(f\"   {i:2d}. {road:<25} {count:2d}æ¬¡ (è®­ç»ƒP={marginal_p:.3f})\")\n    \n    def run_complete_analysis(self):\n        \"\"\"è¿è¡Œå®Œæ•´çš„è¯¦ç»†åˆ†æ\"\"\"\n        # 1. æ•°æ®åˆ†æ\n        original_df = self.load_and_analyze_data()\n        \n        # 2. ç½‘ç»œæ„å»ºåˆ†æ\n        network_roads = self.analyze_network_construction()\n        \n        # 3. æ¡ä»¶æ¦‚ç‡åˆ†æ\n        self.analyze_conditional_probabilities()\n        \n        # 4. æµ‹è¯•æ•°æ®åˆ†æ\n        self.analyze_test_data_structure()\n        \n        print(f\"\\n\\nâœ… è¯¦ç»†åˆ†æå®Œæˆï¼\")\n        print(f\"ğŸ“Š å…³é”®ç»Ÿè®¡æ€»ç»“:\")\n        print(f\"   è®­ç»ƒæ•°æ®: {len(self.train_df)}æ¡è®°å½•\")\n        print(f\"   æµ‹è¯•æ•°æ®: {len(self.test_df)}æ¡è®°å½•\")\n        print(f\"   ç½‘ç»œèŠ‚ç‚¹: {len(network_roads)}ä¸ª\")\n        print(f\"   ç½‘ç»œè¾¹æ•°: {self.flood_net.network.number_of_edges()}æ¡\")\n        \n        return {\n            'train_df': self.train_df,\n            'test_df': self.test_df,\n            'flood_net': self.flood_net,\n            'network_roads': network_roads,\n            'original_df': original_df\n        }\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    analyzer = DetailedNetworkAnalyzer()\n    results = analyzer.run_complete_analysis()\n    return analyzer, results\n\nif __name__ == \"__main__\":\n    analyzer, results = main()