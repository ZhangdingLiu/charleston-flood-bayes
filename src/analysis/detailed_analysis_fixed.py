#!/usr/bin/env python3
"""
è¯¦ç»†åˆ†æï¼šé€æ˜åŒ–è´å¶æ–¯ç½‘ç»œæ„å»ºå’Œè¯„ä¼°è¿‡ç¨‹

è¾“å‡ºæ‰€æœ‰ä¸­é—´æ­¥éª¤ï¼Œè®©ç”¨æˆ·èƒ½å¤ŸéªŒè¯ç»“æœçš„çœŸå®æ€§å’Œå¯é æ€§ï¼š
1. ç½‘ç»œæ„å»ºè¿‡ç¨‹è¯¦ç»†åˆ†æ
2. è®­ç»ƒæ•°æ®æ·±åº¦ç»Ÿè®¡  
3. è¾¹é™…æ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡è¡¨å±•ç¤º
4. æ•°æ®åˆ†å¸ƒå’Œè´¨é‡éªŒè¯
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class DetailedNetworkAnalyzer:
    """è¯¦ç»†çš„ç½‘ç»œåˆ†æå™¨"""
    
    def __init__(self):
        self.train_df = None
        self.test_df = None
        self.flood_net = None
        
    def load_and_analyze_data(self):
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œè¯¦ç»†åˆ†æ"""
        print("ğŸ” æ•°æ®åŠ è½½å’Œé¢„å¤„ç†è¯¦ç»†åˆ†æ")
        print("=" * 60)
        
        # åŠ è½½åŸå§‹æ•°æ®
        df = pd.read_csv("Road_Closures_2024.csv")
        print(f"åŸå§‹æ•°æ®: {len(df)}æ¡è®°å½•")
        
        # è¿‡æ»¤æ´ªæ°´è®°å½•
        flood_df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        print(f"æ´ªæ°´è®°å½•: {len(flood_df)}æ¡ ({len(flood_df)/len(df)*100:.1f}%)")
        
        # æ•°æ®é¢„å¤„ç†
        flood_df["time_create"] = pd.to_datetime(flood_df["START"], utc=True)
        flood_df["link_id"] = flood_df["STREET"].str.upper().str.replace(" ", "_")
        flood_df["link_id"] = flood_df["link_id"].astype(str)
        flood_df["id"] = flood_df["OBJECTID"].astype(str)
        
        # æ—¶é—´åˆ†æ
        print(f"\nğŸ“… æ—¶é—´èŒƒå›´åˆ†æ:")
        print(f"   å¼€å§‹æ—¶é—´: {flood_df['time_create'].min()}")
        print(f"   ç»“æŸæ—¶é—´: {flood_df['time_create'].max()}")
        print(f"   æ—¶é—´è·¨åº¦: {(flood_df['time_create'].max() - flood_df['time_create'].min()).days}å¤©")
        
        # æŒ‰æœˆç»Ÿè®¡
        monthly_counts = flood_df.groupby(flood_df['time_create'].dt.to_period('M')).size()
        print(f"\nğŸ“Š æœˆåº¦æ´ªæ°´è®°å½•åˆ†å¸ƒ:")
        for month, count in monthly_counts.items():
            print(f"   {month}: {count}æ¡")
        
        # é“è·¯åˆ†æ
        road_counts = flood_df['link_id'].value_counts()
        print(f"\nğŸ›£ï¸  é“è·¯åˆ†æ:")
        print(f"   ç‹¬ç‰¹é“è·¯æ•°: {len(road_counts)}æ¡")
        print(f"   å¹³å‡æ¯æ¡é“è·¯æ´ªæ°´æ¬¡æ•°: {road_counts.mean():.1f}")
        print(f"   æœ€é«˜æ´ªæ°´æ¬¡æ•°: {road_counts.max()}æ¬¡ (é“è·¯: {road_counts.index[0]})")
        
        print(f"\nğŸ” æ´ªæ°´é¢‘æ¬¡TOP-10é“è·¯:")
        for i, (road, count) in enumerate(road_counts.head(10).items(), 1):
            print(f"   {i:2d}. {road:<25} {count:3d}æ¬¡")
        
        # æ—¶åºåˆ†å‰²è¯¦ç»†åˆ†æ
        df_sorted = flood_df.sort_values('time_create')
        split_idx = int(len(df_sorted) * 0.7)
        self.train_df = df_sorted.iloc[:split_idx].copy()
        self.test_df = df_sorted.iloc[split_idx:].copy()
        
        print(f"\nâœ‚ï¸  æ—¶åºåˆ†å‰²è¯¦æƒ…:")
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡ ({len(self.train_df)/len(flood_df)*100:.1f}%)")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡ ({len(self.test_df)/len(flood_df)*100:.1f}%)")
        print(f"   è®­ç»ƒæ—¶é—´æ®µ: {self.train_df['time_create'].min().date()} è‡³ {self.train_df['time_create'].max().date()}")
        print(f"   æµ‹è¯•æ—¶é—´æ®µ: {self.test_df['time_create'].min().date()} è‡³ {self.test_df['time_create'].max().date()}")
        
        # è®­ç»ƒé›†vsæµ‹è¯•é›†é“è·¯é‡å åˆ†æ
        train_roads = set(self.train_df['link_id'].unique())
        test_roads = set(self.test_df['link_id'].unique())
        overlap_roads = train_roads & test_roads
        
        print(f"\nğŸ”„ è®­ç»ƒé›†vsæµ‹è¯•é›†é“è·¯åˆ†æ:")
        print(f"   è®­ç»ƒé›†ç‹¬ç‰¹é“è·¯: {len(train_roads)}æ¡")
        print(f"   æµ‹è¯•é›†ç‹¬ç‰¹é“è·¯: {len(test_roads)}æ¡")
        print(f"   é‡å é“è·¯: {len(overlap_roads)}æ¡ ({len(overlap_roads)/len(train_roads)*100:.1f}% of è®­ç»ƒé›†)")
        print(f"   æµ‹è¯•é›†æ–°é“è·¯: {len(test_roads - train_roads)}æ¡")
        
        return flood_df
        
    def analyze_network_construction(self):
        """è¯¦ç»†åˆ†æç½‘ç»œæ„å»ºè¿‡ç¨‹"""
        print(f"\n\nğŸ—ï¸  è´å¶æ–¯ç½‘ç»œæ„å»ºè¯¦ç»†è¿‡ç¨‹")
        print("=" * 60)
        
        # 1. åˆ›å»ºç½‘ç»œå¹¶è®¡ç®—è¾¹é™…æ¦‚ç‡
        self.flood_net = FloodBayesNetwork(t_window="D")
        self.flood_net.fit_marginal(self.train_df)
        
        print(f"\n1ï¸âƒ£  è¾¹é™…æ¦‚ç‡è®¡ç®—:")
        marginals_sorted = self.flood_net.marginals.sort_values('p', ascending=False)
        print(f"   åŸºäºè®­ç»ƒé›†çš„{len(self.train_df)}æ¡è®°å½•")
        print(f"   è®¡ç®—äº†{len(marginals_sorted)}æ¡é“è·¯çš„è¾¹é™…æ¦‚ç‡")
        
        print(f"\nğŸ“Š è¾¹é™…æ¦‚ç‡åˆ†å¸ƒ:")
        prob_ranges = [(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 0.4), (0.4, 0.5), (0.5, 1.0)]
        for low, high in prob_ranges:
            count = len(marginals_sorted[(marginals_sorted['p'] >= low) & (marginals_sorted['p'] < high)])
            print(f"   {low:.1f}-{high:.1f}: {count:2d}æ¡é“è·¯")
        
        print(f"\nğŸ” è¾¹é™…æ¦‚ç‡TOP-15é“è·¯:")
        for i, (_, row) in enumerate(marginals_sorted.head(15).iterrows(), 1):
            print(f"   {i:2d}. {row['link_id']:<25} P={row['p']:.3f}")
        
        print(f"\nğŸ”» è¾¹é™…æ¦‚ç‡BOTTOM-10é“è·¯:")
        for i, (_, row) in enumerate(marginals_sorted.tail(10).iterrows(), 1):
            print(f"   {i:2d}. {row['link_id']:<25} P={row['p']:.3f}")
        
        # 2. å…±ç°åˆ†æ
        print(f"\n2ï¸âƒ£  å…±ç°ç½‘ç»œæ„å»º (å‚æ•°: occ_thr=3, edge_thr=2, weight_thr=0.3):")
        
        # æ˜¾ç¤ºæ„å»ºå‰çš„ç»Ÿè®¡
        time_groups, occurrence, co_occurrence = self.flood_net.process_raw_flood_data(self.train_df.copy())
        
        print(f"   è®­ç»ƒæ•°æ®åŒ…å«{len(time_groups)}ä¸ªæ´ªæ°´æ—¥")
        print(f"   é“è·¯å‡ºç°ç»Ÿè®¡: {len(occurrence)}æ¡é“è·¯")
        print(f"   å…±ç°å¯¹ç»Ÿè®¡: {len(co_occurrence)}ä¸ªé“è·¯å¯¹")
        
        # æ˜¾ç¤ºå‡ºç°é¢‘æ¬¡åˆ†å¸ƒ
        occ_counts = Counter(occurrence.values())
        print(f"\nğŸ“ˆ é“è·¯å‡ºç°é¢‘æ¬¡åˆ†å¸ƒ:")
        for freq in sorted(occ_counts.keys(), reverse=True)[:10]:
            print(f"   å‡ºç°{freq}æ¬¡: {occ_counts[freq]}æ¡é“è·¯")
        
        # æ„å»ºç½‘ç»œ
        self.flood_net.build_network_by_co_occurrence(
            self.train_df,
            occ_thr=3,
            edge_thr=2,
            weight_thr=0.3,
            report=False
        )
        
        print(f"\nâœ… ç½‘ç»œæ„å»ºå®Œæˆ:")
        print(f"   èŠ‚ç‚¹æ•°: {self.flood_net.network.number_of_nodes()}")
        print(f"   è¾¹æ•°: {self.flood_net.network.number_of_edges()}")
        
        # åˆ†æè¢«è¿‡æ»¤æ‰çš„é“è·¯
        all_roads = set(occurrence.keys())
        network_roads = set(self.flood_net.network.nodes())
        filtered_roads = all_roads - network_roads
        
        print(f"\nğŸš« è¿‡æ»¤åˆ†æ:")
        print(f"   åŸå§‹é“è·¯æ•°: {len(all_roads)}")
        print(f"   ä¿ç•™é“è·¯æ•°: {len(network_roads)}")
        print(f"   è¿‡æ»¤é“è·¯æ•°: {len(filtered_roads)} ({len(filtered_roads)/len(all_roads)*100:.1f}%)")
        
        if len(filtered_roads) > 0:
            print(f"\nğŸ” éƒ¨åˆ†è¢«è¿‡æ»¤é“è·¯ (å‡ºç°æ¬¡æ•°<3):")
            filtered_with_count = [(road, occurrence[road]) for road in filtered_roads]
            filtered_with_count.sort(key=lambda x: x[1], reverse=True)
            for road, count in filtered_with_count[:10]:
                print(f"   {road:<25} å‡ºç°{count}æ¬¡")
        
        # ç½‘ç»œæ‹“æ‰‘åˆ†æ
        print(f"\n3ï¸âƒ£  ç½‘ç»œæ‹“æ‰‘ç‰¹å¾:")
        
        # åº¦åˆ†å¸ƒ
        in_degrees = dict(self.flood_net.network.in_degree())
        out_degrees = dict(self.flood_net.network.out_degree())
        
        print(f"   å¹³å‡å…¥åº¦: {np.mean(list(in_degrees.values())):.2f}")
        print(f"   å¹³å‡å‡ºåº¦: {np.mean(list(out_degrees.values())):.2f}")
        print(f"   æœ€å¤§å…¥åº¦: {max(in_degrees.values())} (èŠ‚ç‚¹: {max(in_degrees, key=in_degrees.get)})")
        print(f"   æœ€å¤§å‡ºåº¦: {max(out_degrees.values())} (èŠ‚ç‚¹: {max(out_degrees, key=out_degrees.get)})")
        
        # æ˜¾ç¤ºé«˜åº¦èŠ‚ç‚¹
        print(f"\nğŸŒŸ é«˜å…¥åº¦èŠ‚ç‚¹ (å®¹æ˜“è¢«å…¶ä»–é“è·¯å½±å“):")
        high_in_degree = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:8]
        for road, degree in high_in_degree:
            marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == road]['p'].iloc[0]
            print(f"   {road:<25} å…¥åº¦={degree}, P(æ´ªæ°´)={marginal_p:.3f}")
        
        print(f"\nğŸŒŸ é«˜å‡ºåº¦èŠ‚ç‚¹ (å®¹æ˜“å½±å“å…¶ä»–é“è·¯):")
        high_out_degree = sorted(out_degrees.items(), key=lambda x: x[1], reverse=True)[:8]
        for road, degree in high_out_degree:
            marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == road]['p'].iloc[0]
            print(f"   {road:<25} å‡ºåº¦={degree}, P(æ´ªæ°´)={marginal_p:.3f}")
        
        # è¾¹æƒåˆ†æ
        edge_weights = [d['weight'] for u, v, d in self.flood_net.network.edges(data=True)]
        print(f"\nğŸ”— è¾¹æƒåˆ†æ:")
        print(f"   è¾¹æƒèŒƒå›´: {min(edge_weights):.3f} - {max(edge_weights):.3f}")
        print(f"   å¹³å‡è¾¹æƒ: {np.mean(edge_weights):.3f}")
        print(f"   è¾¹æƒä¸­ä½æ•°: {np.median(edge_weights):.3f}")
        
        # æ˜¾ç¤ºæœ€å¼ºè¿æ¥
        edge_list = [(u, v, d['weight']) for u, v, d in self.flood_net.network.edges(data=True)]
        edge_list.sort(key=lambda x: x[2], reverse=True)
        
        print(f"\nğŸ’ª æœ€å¼ºä¾èµ–å…³ç³»TOP-10:")
        for i, (u, v, weight) in enumerate(edge_list[:10], 1):
            print(f"   {i:2d}. {u} â†’ {v} (æƒé‡={weight:.3f})")
        
        return network_roads
    
    def analyze_conditional_probabilities(self):
        """åˆ†ææ¡ä»¶æ¦‚ç‡è¡¨"""
        print(f"\n\n4ï¸âƒ£  æ¡ä»¶æ¦‚ç‡è¡¨ (CPT) æ„å»ºè¯¦ç»†è¿‡ç¨‹")
        print("=" * 60)
        
        # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡
        self.flood_net.fit_conditional(self.train_df, max_parents=2, alpha=1.0)
        
        print(f"å‚æ•°: max_parents=2, alpha=1.0 (æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘)")
        
        # åˆ†æCPTç»Ÿè®¡
        nodes_with_parents = 0
        nodes_without_parents = 0
        total_cpt_entries = 0
        
        for node in self.flood_net.network.nodes():
            parents = list(self.flood_net.network.predecessors(node))
            if len(parents) > 0 and node in self.flood_net.conditionals:
                nodes_with_parents += 1
                cpt_size = 2 ** len(self.flood_net.conditionals[node]['parents'])
                total_cpt_entries += cpt_size
            else:
                nodes_without_parents += 1
        
        print(f"\nğŸ“Š CPTç»Ÿè®¡:")
        print(f"   æœ‰çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹: {nodes_with_parents}ä¸ª")
        print(f"   æ— çˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹: {nodes_without_parents}ä¸ª")
        print(f"   æ€»CPTæ¡ç›®æ•°: {total_cpt_entries}ä¸ª")
        
        # æ˜¾ç¤ºå‡ ä¸ªå…·ä½“çš„CPTä¾‹å­
        print(f"\nğŸ“‹ æ¡ä»¶æ¦‚ç‡è¡¨ç¤ºä¾‹:")
        
        cpt_examples = 0
        for node in self.flood_net.network.nodes():
            if node in self.flood_net.conditionals and cpt_examples < 3:
                cpt_examples += 1
                cfg = self.flood_net.conditionals[node]
                parents = cfg['parents']
                conditionals = cfg['conditionals']
                
                marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == node]['p'].iloc[0]
                print(f"\n   èŠ‚ç‚¹: {node} (è¾¹é™…æ¦‚ç‡={marginal_p:.3f})")
                print(f"   çˆ¶èŠ‚ç‚¹: {parents}")
                print(f"   æ¡ä»¶æ¦‚ç‡:")
                
                for state, prob in conditionals.items():
                    parent_state_str = ", ".join([f"{p}={s}" for p, s in zip(parents, state)])
                    print(f"     P({node}=1 | {parent_state_str}) = {prob:.3f}")
        
        # æ„å»ºæœ€ç»ˆçš„è´å¶æ–¯ç½‘ç»œ
        self.flood_net.build_bayes_network()
        print(f"\nâœ… è´å¶æ–¯ç½‘ç»œæ„å»ºå®Œæˆï¼Œå‡†å¤‡è¿›è¡Œæ¨ç†")
        
    def analyze_test_data_structure(self):
        """åˆ†ææµ‹è¯•æ•°æ®ç»“æ„"""
        print(f"\n\nğŸ§ª æµ‹è¯•æ•°æ®ç»“æ„è¯¦ç»†åˆ†æ")
        print("=" * 60)
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        
        print(f"æµ‹è¯•æ•°æ®è¦†ç›–{len(test_by_date)}ä¸ªæ´ªæ°´æ—¥")
        
        # ç»Ÿè®¡æ¯æ—¥æ´ªæ°´é“è·¯æ•°é‡
        daily_road_counts = []
        daily_network_road_counts = []
        network_roads = set(self.flood_net.network.nodes())
        
        print(f"\nğŸ“… æµ‹è¯•æ—¥æœŸè¯¦ç»†åˆ†è§£:")
        for i, (date, day_group) in enumerate(test_by_date):
            flooded_roads = list(day_group["link_id"].unique())
            flooded_in_network = [road for road in flooded_roads if road in network_roads]
            
            daily_road_counts.append(len(flooded_roads))
            daily_network_road_counts.append(len(flooded_in_network))
            
            if i < 10:  # æ˜¾ç¤ºå‰10å¤©çš„è¯¦æƒ…
                print(f"   {date.date()}: {len(flooded_roads)}æ¡é“è·¯æ´ªæ°´, {len(flooded_in_network)}æ¡åœ¨ç½‘ç»œä¸­")
                if len(flooded_in_network) > 0:
                    print(f"      ç½‘ç»œé“è·¯: {', '.join(flooded_in_network[:5])}{'...' if len(flooded_in_network) > 5 else ''}")
        
        if len(test_by_date) > 10:
            print(f"   ... (è¿˜æœ‰{len(test_by_date)-10}å¤©)")
        
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®ç»Ÿè®¡:")
        print(f"   å¹³å‡æ¯æ—¥æ´ªæ°´é“è·¯: {np.mean(daily_road_counts):.1f}æ¡")
        print(f"   å¹³å‡æ¯æ—¥ç½‘ç»œé“è·¯: {np.mean(daily_network_road_counts):.1f}æ¡")
        print(f"   å¯è¯„ä¼°æ—¥æ•° (â‰¥2æ¡ç½‘ç»œé“è·¯): {sum(1 for x in daily_network_road_counts if x >= 2)}å¤©")
        
        # æµ‹è¯•é“è·¯é¢‘æ¬¡åˆ†æ
        test_road_counts = self.test_df['link_id'].value_counts()
        test_network_roads = test_road_counts[test_road_counts.index.isin(network_roads)]
        
        print(f"\nğŸ›£ï¸  æµ‹è¯•é›†é“è·¯åˆ†æ:")
        print(f"   æµ‹è¯•é›†ç‹¬ç‰¹é“è·¯: {len(test_road_counts)}æ¡")
        print(f"   æµ‹è¯•é›†ç½‘ç»œé“è·¯: {len(test_network_roads)}æ¡")
        print(f"   è¦†ç›–ç‡: {len(test_network_roads)/len(network_roads)*100:.1f}% (ç½‘ç»œé“è·¯åœ¨æµ‹è¯•é›†ä¸­å‡ºç°)")
        
        print(f"\nğŸ” æµ‹è¯•é›†é«˜é¢‘æ´ªæ°´é“è·¯TOP-10:")
        for i, (road, count) in enumerate(test_network_roads.head(10).items(), 1):
            marginal_p = self.flood_net.marginals[self.flood_net.marginals['link_id'] == road]['p'].iloc[0]
            print(f"   {i:2d}. {road:<25} {count:2d}æ¬¡ (è®­ç»ƒP={marginal_p:.3f})")
    
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„è¯¦ç»†åˆ†æ"""
        # 1. æ•°æ®åˆ†æ
        original_df = self.load_and_analyze_data()
        
        # 2. ç½‘ç»œæ„å»ºåˆ†æ
        network_roads = self.analyze_network_construction()
        
        # 3. æ¡ä»¶æ¦‚ç‡åˆ†æ
        self.analyze_conditional_probabilities()
        
        # 4. æµ‹è¯•æ•°æ®åˆ†æ
        self.analyze_test_data_structure()
        
        print(f"\n\nâœ… è¯¦ç»†åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š å…³é”®ç»Ÿè®¡æ€»ç»“:")
        print(f"   è®­ç»ƒæ•°æ®: {len(self.train_df)}æ¡è®°å½•")
        print(f"   æµ‹è¯•æ•°æ®: {len(self.test_df)}æ¡è®°å½•")
        print(f"   ç½‘ç»œèŠ‚ç‚¹: {len(network_roads)}ä¸ª")
        print(f"   ç½‘ç»œè¾¹æ•°: {self.flood_net.network.number_of_edges()}æ¡")
        
        return {
            'train_df': self.train_df,
            'test_df': self.test_df,
            'flood_net': self.flood_net,
            'network_roads': network_roads,
            'original_df': original_df
        }

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DetailedNetworkAnalyzer()
    results = analyzer.run_complete_analysis()
    return analyzer, results

if __name__ == "__main__":
    analyzer, results = main()