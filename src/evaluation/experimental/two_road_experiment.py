#!/usr/bin/env python3
"""
N MARKET ST â†” S MARKET ST ä¸¤æ¡é“è·¯è´å¶æ–¯ç½‘ç»œå®éªŒ

ç›®çš„ï¼š
é€šè¿‡æœ€ç®€å•çš„ä¸¤æ¡é“è·¯æ¡ˆä¾‹ï¼Œæ·±å…¥åˆ†æä¸ºä»€ä¹ˆè´å¶æ–¯ç½‘ç»œåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å·®
é€‰æ‹©è¿™ä¸¤æ¡é“è·¯æ˜¯å› ä¸ºå®ƒä»¬åœ¨å…±ç°åˆ†æä¸­æ˜¾ç¤ºäº†å¼ºå…³è”æ€§

å®éªŒè®¾è®¡ï¼š
1. ä»…ä½¿ç”¨ N MARKET ST å’Œ S MARKET ST ä¸¤æ¡é“è·¯
2. åœ¨è®­ç»ƒé›†ä¸Šæ„å»ºè´å¶æ–¯ç½‘ç»œ 
3. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
4. è¯¦ç»†åˆ†æè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­è¿™ä¸¤æ¡é“è·¯çš„æ´ªæ°´å‡ºç°æ¨¡å¼
5. æ˜¾ç¤ºå…·ä½“çš„æ¡ä»¶æ¦‚ç‡è®¡ç®—å’Œæ¨ç†è¿‡ç¨‹

ç”¨æ³•ï¼š
    python two_road_experiment.py

è¾“å‡ºï¼š
    - è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ä¸¤æ¡é“è·¯çš„è¯¦ç»†ç»Ÿè®¡
    - è´å¶æ–¯ç½‘ç»œç»“æ„å’Œæ¡ä»¶æ¦‚ç‡è¡¨
    - æµ‹è¯•é›†é¢„æµ‹ç»“æœçš„è¯¦ç»†åˆ†æ
    - æ€§èƒ½ä¸ä½³çš„æ ¹æœ¬åŸå› åˆ†æ
"""

import pandas as pd
import numpy as np
import pickle
import os
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    precision_score, recall_score, f1_score, confusion_matrix,
    accuracy_score
)

# è´å¶æ–¯ç½‘ç»œç›¸å…³
try:
    try:
        from pgmpy.models import DiscreteBayesianNetwork as BayesianNetwork
    except ImportError:
        from pgmpy.models import BayesianNetwork
    from pgmpy.factors.discrete import TabularCPD
    from pgmpy.inference import VariableElimination
except ImportError:
    print("è¯·å®‰è£…pgmpy: pip install pgmpy")
    exit(1)

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42

class TwoRoadBayesianExperiment:
    """ä¸¤æ¡é“è·¯è´å¶æ–¯ç½‘ç»œå®éªŒç±»"""
    
    def __init__(self, data_csv_path="Road_Closures_2024.csv"):
        self.data_csv_path = data_csv_path
        self.selected_roads = ['N MARKET ST', 'S MARKET ST']
        
        # æ•°æ®å­˜å‚¨
        self.train_df = None
        self.test_df = None
        self.train_filtered = None
        self.test_filtered = None
        self.bayesian_network = None
        self.inference_engine = None
        
        # ç»Ÿè®¡æ•°æ®
        self.train_stats = {}
        self.test_stats = {}
        
    def load_and_split_data(self):
        """åŠ è½½æ•°æ®å¹¶åˆ†å‰²"""
        print("ğŸš€ å¼€å§‹ä¸¤æ¡é“è·¯è´å¶æ–¯ç½‘ç»œå®éªŒ")
        print("="*60)
        print("1. åŠ è½½å’Œåˆ†å‰²æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_csv_path)
        df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        
        # æ•°æ®é¢„å¤„ç†
        df["time_create"] = pd.to_datetime(df["START"], utc=True)
        df["road"] = df["STREET"].str.upper().str.strip()
        df["date"] = df["time_create"].dt.floor("D")
        df["id"] = df["OBJECTID"].astype(str)
        
        # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­åˆ†å‰²
        self.train_df, self.test_df = train_test_split(
            df, test_size=0.3, random_state=RANDOM_SEED
        )
        
        print(f"   æ€»æ´ªæ°´è®°å½•: {len(df)}æ¡")
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡")
        
        # è¿‡æ»¤å‡ºä¸¤æ¡ç›®æ ‡é“è·¯
        self.train_filtered = self.train_df[self.train_df['road'].isin(self.selected_roads)].copy()
        self.test_filtered = self.test_df[self.test_df['road'].isin(self.selected_roads)].copy()
        
        print(f"   è®­ç»ƒé›†ç›®æ ‡é“è·¯è®°å½•: {len(self.train_filtered)}æ¡")
        print(f"   æµ‹è¯•é›†ç›®æ ‡é“è·¯è®°å½•: {len(self.test_filtered)}æ¡")
        
        return True
        
    def analyze_road_patterns(self):
        """åˆ†æä¸¤æ¡é“è·¯åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„æ¨¡å¼"""
        print("\n2. åˆ†æé“è·¯æ´ªæ°´æ¨¡å¼...")
        
        def analyze_dataset(filtered_df, dataset_name):
            print(f"\n   ğŸ“Š {dataset_name}ç»Ÿè®¡:")
            
            # å„é“è·¯å‡ºç°æ¬¡æ•°
            road_counts = filtered_df['road'].value_counts()
            for road in self.selected_roads:
                count = road_counts.get(road, 0)
                print(f"     {road}: {count}æ¬¡")
            
            # æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œè®¡ç®—å…±ç°
            daily_roads = defaultdict(set)
            for _, row in filtered_df.iterrows():
                date_str = str(row['date'].date())
                daily_roads[date_str].add(row['road'])
            
            # ç»Ÿè®¡å…±ç°æ¨¡å¼
            patterns = {
                'only_n_market': 0,
                'only_s_market': 0, 
                'both_roads': 0,
                'total_days': len(daily_roads)
            }
            
            cooccur_dates = []
            for date, roads in daily_roads.items():
                if 'N MARKET ST' in roads and 'S MARKET ST' in roads:
                    patterns['both_roads'] += 1
                    cooccur_dates.append(date)
                elif 'N MARKET ST' in roads:
                    patterns['only_n_market'] += 1
                elif 'S MARKET ST' in roads:
                    patterns['only_s_market'] += 1
            
            print(f"     æœ‰æ´ªæ°´çš„å¤©æ•°: {patterns['total_days']}å¤©")
            print(f"     ä»…N MARKET ST: {patterns['only_n_market']}å¤©")
            print(f"     ä»…S MARKET ST: {patterns['only_s_market']}å¤©")
            print(f"     ä¸¤æ¡è·¯åŒæ—¶: {patterns['both_roads']}å¤©")
            
            if patterns['both_roads'] > 0:
                print(f"     å…±ç°æ—¥æœŸç¤ºä¾‹: {cooccur_dates[:3]}")
            
            return patterns, daily_roads
        
        # åˆ†æè®­ç»ƒé›†
        self.train_stats, self.train_daily = analyze_dataset(self.train_filtered, "è®­ç»ƒé›†")
        
        # åˆ†ææµ‹è¯•é›†
        self.test_stats, self.test_daily = analyze_dataset(self.test_filtered, "æµ‹è¯•é›†")
        
        # è®¡ç®—æ¡ä»¶æ¦‚ç‡ç»Ÿè®¡
        print(f"\n   ğŸ“ˆ è®­ç»ƒé›†æ¡ä»¶æ¦‚ç‡è®¡ç®—:")
        train_n_count = sum(1 for roads in self.train_daily.values() if 'N MARKET ST' in roads)
        train_s_count = sum(1 for roads in self.train_daily.values() if 'S MARKET ST' in roads)
        train_both_count = self.train_stats['both_roads']
        
        if train_n_count > 0:
            conf_s_given_n = train_both_count / train_n_count
            print(f"     P(S MARKET ST=æ´ªæ°´ | N MARKET ST=æ´ªæ°´) = {train_both_count}/{train_n_count} = {conf_s_given_n:.4f}")
        
        if train_s_count > 0:
            conf_n_given_s = train_both_count / train_s_count
            print(f"     P(N MARKET ST=æ´ªæ°´ | S MARKET ST=æ´ªæ°´) = {train_both_count}/{train_s_count} = {conf_n_given_s:.4f}")
        
        return True
        
    def build_bayesian_network(self):
        """æ„å»ºä¸¤æ¡é“è·¯çš„è´å¶æ–¯ç½‘ç»œ"""
        print("\n3. æ„å»ºè´å¶æ–¯ç½‘ç»œ...")
        
        # åˆ›å»ºæ—¥æœŸ-é“è·¯äºŒå…ƒçŸ©é˜µ
        all_dates = sorted(set(self.train_daily.keys()))
        matrix_data = []
        
        for date in all_dates:
            roads_today = self.train_daily[date]
            row_data = {
                'date': date,
                'N_MARKET_ST': 1 if 'N MARKET ST' in roads_today else 0,
                'S_MARKET_ST': 1 if 'S MARKET ST' in roads_today else 0
            }
            matrix_data.append(row_data)
        
        binary_df = pd.DataFrame(matrix_data)
        
        print(f"   è®­ç»ƒçŸ©é˜µå¤§å°: {len(binary_df)} å¤© Ã— 2 é“è·¯")
        print(f"   N MARKET STæ´ªæ°´é¢‘ç‡: {binary_df['N_MARKET_ST'].mean():.3f}")
        print(f"   S MARKET STæ´ªæ°´é¢‘ç‡: {binary_df['S_MARKET_ST'].mean():.3f}")
        
        # å†³å®šç½‘ç»œç»“æ„ï¼ˆé€‰æ‹©æ¡ä»¶æ¦‚ç‡æ›´é«˜çš„æ–¹å‘ï¼‰
        n_count = binary_df['N_MARKET_ST'].sum()
        s_count = binary_df['S_MARKET_ST'].sum()
        both_count = ((binary_df['N_MARKET_ST'] == 1) & (binary_df['S_MARKET_ST'] == 1)).sum()
        
        if n_count > 0 and s_count > 0:
            conf_s_given_n = both_count / n_count
            conf_n_given_s = both_count / s_count
            
            print(f"   æ¡ä»¶æ¦‚ç‡æ¯”è¾ƒ:")
            print(f"     P(S|N) = {conf_s_given_n:.4f}")
            print(f"     P(N|S) = {conf_n_given_s:.4f}")
            
            # é€‰æ‹©æ¡ä»¶æ¦‚ç‡æ›´é«˜çš„æ–¹å‘
            if conf_s_given_n >= conf_n_given_s:
                edges = [('N_MARKET_ST', 'S_MARKET_ST')]
                print(f"   é€‰æ‹©ç»“æ„: N MARKET ST â†’ S MARKET ST")
            else:
                edges = [('S_MARKET_ST', 'N_MARKET_ST')]
                print(f"   é€‰æ‹©ç»“æ„: S MARKET ST â†’ N MARKET ST")
        else:
            # å¦‚æœä¸€æ¡è·¯ä»æœªå‡ºç°ï¼Œåˆ›å»ºæ— è¾¹ç½‘ç»œ
            edges = []
            print(f"   æ— è¶³å¤Ÿæ•°æ®ï¼Œåˆ›å»ºæ— è¾¹ç½‘ç»œ")
        
        # åˆ›å»ºè´å¶æ–¯ç½‘ç»œ
        self.bayesian_network = BayesianNetwork(edges)
        
        # è®¡ç®—CPDå‚æ•°
        cpds = []
        
        for node in ['N_MARKET_ST', 'S_MARKET_ST']:
            parents = list(self.bayesian_network.predecessors(node))
            
            if len(parents) == 0:
                # æ ¹èŠ‚ç‚¹ - å…ˆéªŒæ¦‚ç‡
                prob_1 = (binary_df[node].sum() + 1) / (len(binary_df) + 2)  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                prob_0 = 1 - prob_1
                
                cpd = TabularCPD(
                    variable=node,
                    variable_card=2,
                    values=[[prob_0], [prob_1]],
                    state_names={node: [0, 1]}
                )
                
                print(f"   {node} (æ ¹èŠ‚ç‚¹):")
                print(f"     P({node}=1) = {prob_1:.4f}")
                
            else:
                # æœ‰çˆ¶èŠ‚ç‚¹
                parent = parents[0]
                
                # è®¡ç®—æ¡ä»¶æ¦‚ç‡è¡¨
                parent_0_child_0 = len(binary_df[(binary_df[parent] == 0) & (binary_df[node] == 0)])
                parent_0_child_1 = len(binary_df[(binary_df[parent] == 0) & (binary_df[node] == 1)])
                parent_1_child_0 = len(binary_df[(binary_df[parent] == 1) & (binary_df[node] == 0)])
                parent_1_child_1 = len(binary_df[(binary_df[parent] == 1) & (binary_df[node] == 1)])
                
                # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
                total_parent_0 = parent_0_child_0 + parent_0_child_1 + 2
                total_parent_1 = parent_1_child_0 + parent_1_child_1 + 2
                
                prob_child_0_given_parent_0 = (parent_0_child_0 + 1) / total_parent_0
                prob_child_1_given_parent_0 = (parent_0_child_1 + 1) / total_parent_0
                prob_child_0_given_parent_1 = (parent_1_child_0 + 1) / total_parent_1
                prob_child_1_given_parent_1 = (parent_1_child_1 + 1) / total_parent_1
                
                cpd = TabularCPD(
                    variable=node,
                    variable_card=2,
                    values=[
                        [prob_child_0_given_parent_0, prob_child_0_given_parent_1],
                        [prob_child_1_given_parent_0, prob_child_1_given_parent_1]
                    ],
                    evidence=[parent],
                    evidence_card=[2],
                    state_names={node: [0, 1], parent: [0, 1]}
                )
                
                print(f"   {node} (å­èŠ‚ç‚¹, çˆ¶èŠ‚ç‚¹: {parent}):")
                print(f"     P({node}=1|{parent}=0) = {prob_child_1_given_parent_0:.4f}")
                print(f"     P({node}=1|{parent}=1) = {prob_child_1_given_parent_1:.4f}")
                
                # è®¡ç®—æå‡æ•ˆæœ
                if prob_child_1_given_parent_0 > 0:
                    lift = prob_child_1_given_parent_1 / prob_child_1_given_parent_0
                    print(f"     æ¡ä»¶æ¦‚ç‡æå‡: {lift:.2f}x")
            
            cpds.append(cpd)
        
        # æ·»åŠ CPDåˆ°ç½‘ç»œ
        self.bayesian_network.add_cpds(*cpds)
        
        # éªŒè¯ç½‘ç»œ
        if self.bayesian_network.check_model():
            print("   âœ… è´å¶æ–¯ç½‘ç»œæ„å»ºæˆåŠŸ")
        else:
            print("   âŒ è´å¶æ–¯ç½‘ç»œéªŒè¯å¤±è´¥")
            return False
        
        # åˆ›å»ºæ¨ç†å¼•æ“
        self.inference_engine = VariableElimination(self.bayesian_network)
        
        return True
        
    def evaluate_on_test_set(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹å’Œè¯„ä¼°"""
        print("\n4. æµ‹è¯•é›†é¢„æµ‹è¯„ä¼°...")
        
        if len(self.test_daily) == 0:
            print("   âŒ æµ‹è¯•é›†ä¸­æ²¡æœ‰ç›®æ ‡é“è·¯çš„æ´ªæ°´è®°å½•")
            return False
        
        predictions = []
        
        print(f"   æµ‹è¯•é›†ä¸­æœ‰æ´ªæ°´çš„å¤©æ•°: {len(self.test_daily)}å¤©")
        print(f"   é€æ—¥é¢„æµ‹åˆ†æ:")
        
        for date, roads_today in self.test_daily.items():
            print(f"\n     ğŸ“… {date}:")
            print(f"       å®é™…æ´ªæ°´é“è·¯: {list(roads_today)}")
            
            # ä¸ºæ¯æ¡é“è·¯è¿›è¡Œé¢„æµ‹
            for target_road in self.selected_roads:
                target_node = target_road.replace(' ', '_').replace('-', '_')
                
                # æ„å»ºevidenceï¼ˆé™¤ç›®æ ‡é“è·¯å¤–çš„å…¶ä»–é“è·¯ï¼‰
                evidence = {}
                for other_road in self.selected_roads:
                    if other_road != target_road:
                        other_node = other_road.replace(' ', '_').replace('-', '_')
                        evidence[other_node] = 1 if other_road in roads_today else 0
                
                try:
                    # è´å¶æ–¯æ¨ç†
                    if len(evidence) > 0:
                        query_result = self.inference_engine.query(
                            variables=[target_node], 
                            evidence=evidence
                        )
                        prob_flood = query_result.values[1]
                    else:
                        # æ²¡æœ‰evidenceï¼Œä½¿ç”¨å…ˆéªŒæ¦‚ç‡
                        query_result = self.inference_engine.query(variables=[target_node])
                        prob_flood = query_result.values[1]
                    
                    # çœŸå®æ ‡ç­¾
                    true_flood = 1 if target_road in roads_today else 0
                    
                    predictions.append({
                        'date': date,
                        'road': target_road,
                        'evidence': evidence,
                        'prob_flood': prob_flood,
                        'true_flood': true_flood
                    })
                    
                    print(f"       {target_road}:")
                    print(f"         Evidence: {evidence}")
                    print(f"         é¢„æµ‹æ¦‚ç‡: {prob_flood:.4f}")
                    print(f"         çœŸå®æ ‡ç­¾: {true_flood}")
                    
                except Exception as e:
                    print(f"       {target_road}: æ¨ç†å¤±è´¥ - {e}")
        
        if len(predictions) == 0:
            print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
            return False
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        print(f"\n   ğŸ“ˆ æ€§èƒ½è¯„ä¼°:")
        
        thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]
        print(f"   {'é˜ˆå€¼':<6} {'å‡†ç¡®ç‡':<8} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1':<8} {'TP':<3} {'TN':<3} {'FP':<3} {'FN':<3}")
        print(f"   {'-'*60}")
        
        for threshold in thresholds:
            y_true = [p['true_flood'] for p in predictions]
            y_pred = [1 if p['prob_flood'] >= threshold else 0 for p in predictions]
            
            acc = accuracy_score(y_true, y_pred)
            prec = precision_score(y_true, y_pred, zero_division=0)
            rec = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            cm = confusion_matrix(y_true, y_pred)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (len(y_true), 0, 0, 0)
            
            print(f"   {threshold:<6.1f} {acc:<8.4f} {prec:<8.4f} {rec:<8.4f} {f1:<8.4f} "
                  f"{tp:<3} {tn:<3} {fp:<3} {fn:<3}")
        
        return predictions
        
    def analyze_performance_issues(self, predictions):
        """åˆ†ææ€§èƒ½é—®é¢˜çš„æ ¹æœ¬åŸå› """
        print(f"\n5. æ€§èƒ½é—®é¢˜æ ¹æœ¬åŸå› åˆ†æ...")
        
        print(f"   ğŸ” å…³é”®å‘ç°:")
        
        # 1. æ ·æœ¬é‡åˆ†æ
        total_predictions = len(predictions)
        positive_samples = sum(p['true_flood'] for p in predictions)
        print(f"     æ€»é¢„æµ‹æ ·æœ¬: {total_predictions}ä¸ª")
        print(f"     æ­£æ ·æœ¬(çœŸå®æ´ªæ°´): {positive_samples}ä¸ª ({positive_samples/total_predictions*100:.1f}%)")
        print(f"     è´Ÿæ ·æœ¬(æ— æ´ªæ°´): {total_predictions-positive_samples}ä¸ª ({(total_predictions-positive_samples)/total_predictions*100:.1f}%)")
        
        # 2. è®­ç»ƒ-æµ‹è¯•åˆ†å¸ƒå·®å¼‚
        print(f"\n     è®­ç»ƒé›† vs æµ‹è¯•é›†å¯¹æ¯”:")
        print(f"     è®­ç»ƒé›†æ´ªæ°´å¤©æ•°: {self.train_stats['total_days']}å¤©")
        print(f"     æµ‹è¯•é›†æ´ªæ°´å¤©æ•°: {self.test_stats['total_days']}å¤©")
        print(f"     è®­ç»ƒé›†å…±ç°å¤©æ•°: {self.train_stats['both_roads']}å¤©")
        print(f"     æµ‹è¯•é›†å…±ç°å¤©æ•°: {self.test_stats['both_roads']}å¤©")
        
        # 3. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        probs = [p['prob_flood'] for p in predictions]
        print(f"\n     é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:")
        print(f"     æœ€å°æ¦‚ç‡: {min(probs):.4f}")
        print(f"     æœ€å¤§æ¦‚ç‡: {max(probs):.4f}")
        print(f"     å¹³å‡æ¦‚ç‡: {np.mean(probs):.4f}")
        print(f"     ä¸­ä½æ•°æ¦‚ç‡: {np.median(probs):.4f}")
        
        # 4. å…·ä½“æ¡ˆä¾‹åˆ†æ
        print(f"\n     å…¸å‹é¢„æµ‹æ¡ˆä¾‹:")
        for i, pred in enumerate(predictions[:5]):
            print(f"     æ¡ˆä¾‹{i+1}: {pred['date']} {pred['road']}")
            print(f"       Evidence: {pred['evidence']}")
            print(f"       é¢„æµ‹: {pred['prob_flood']:.4f}, çœŸå®: {pred['true_flood']}")
        
        # 5. æ ¹æœ¬åŸå› æ€»ç»“
        print(f"\n   ğŸ’¡ æ ¹æœ¬åŸå› æ€»ç»“:")
        
        reasons = []
        if positive_samples == 0:
            reasons.append("æµ‹è¯•é›†ä¸­æ— æ­£æ ·æœ¬ï¼Œæ— æ³•è¯„ä¼°å¬å›ç‡")
        
        if self.test_stats['both_roads'] == 0:
            reasons.append("æµ‹è¯•é›†ä¸­ä¸¤æ¡é“è·¯ä»æœªåŒæ—¶æ´ªæ°´ï¼Œè®­ç»ƒçš„æ¡ä»¶æ¦‚ç‡æ— æ³•éªŒè¯")
        
        if self.train_stats['total_days'] < 10:
            reasons.append("è®­ç»ƒæ•°æ®é‡ä¸è¶³ï¼Œç»Ÿè®¡ä¸å¯é ")
        
        if max(probs) < 0.5:
            reasons.append("é¢„æµ‹æ¦‚ç‡æ™®éåä½ï¼Œé˜ˆå€¼è®¾ç½®å¯èƒ½ä¸å½“")
        
        for i, reason in enumerate(reasons, 1):
            print(f"     {i}. {reason}")
        
        if not reasons:
            print(f"     ç½‘ç»œæ„å»ºæ­£å¸¸ï¼Œå¯èƒ½æ˜¯æ•°æ®æœ¬èº«çš„é¢„æµ‹éš¾åº¦è¾ƒé«˜")
        
        return True
        
    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        try:
            # 1. æ•°æ®åŠ è½½å’Œåˆ†å‰²
            if not self.load_and_split_data():
                return False
            
            # 2. åˆ†æé“è·¯æ¨¡å¼
            if not self.analyze_road_patterns():
                return False
            
            # 3. æ„å»ºè´å¶æ–¯ç½‘ç»œ
            if not self.build_bayesian_network():
                return False
            
            # 4. æµ‹è¯•é›†è¯„ä¼°
            predictions = self.evaluate_on_test_set()
            if not predictions:
                return False
            
            # 5. æ€§èƒ½é—®é¢˜åˆ†æ
            self.analyze_performance_issues(predictions)
            
            print(f"\nâœ… ä¸¤æ¡é“è·¯è´å¶æ–¯ç½‘ç»œå®éªŒå®Œæˆï¼")
            print(f"ğŸ¯ é€šè¿‡è¿™ä¸ªç®€å•æ¡ˆä¾‹ï¼Œæˆ‘ä»¬æ·±å…¥ç†è§£äº†è´å¶æ–¯ç½‘ç»œæ€§èƒ½é—®é¢˜çš„æ ¹æº")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    experiment = TwoRoadBayesianExperiment()
    experiment.run_experiment()

if __name__ == "__main__":
    main()