#!/usr/bin/env python3
"""
é€æ ·æœ¬è¯¦ç»†è¯„ä¼°è¾“å‡º

å±•ç¤ºæµ‹è¯•é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„å®Œæ•´è¯„ä¼°è¿‡ç¨‹ï¼š
1. æ¯ä¸ªæµ‹è¯•æ—¥çš„è¯¦ç»†æ¨ç†è¿‡ç¨‹
2. Evidenceè®¾ç½®å’Œç›®æ ‡é€‰æ‹©
3. æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡å’Œå†³ç­–
4. TP/FP/TN/FNçš„å…·ä½“å½’ç±»
5. æœ€ç»ˆæ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡è®¡ç®—
6. å¯è§†åŒ–ç»“æœ
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
from precision_focused_evaluation import PrecisionFocusedEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class SampleBySeamleEvaluator:
    """é€æ ·æœ¬è¯¦ç»†è¯„ä¼°å™¨"""
    
    def __init__(self, flood_net, test_df):
        self.flood_net = flood_net
        self.test_df = test_df
        self.bn_nodes = set(flood_net.network_bayes.nodes())
        self.marginals_dict = dict(zip(
            flood_net.marginals['link_id'], 
            flood_net.marginals['p']
        ))
        
        # è¯„ä¼°å‚æ•°
        self.positive_threshold = 0.6
        self.negative_threshold = 0.3
        self.min_marginal_for_negative = 0.15
        
        # å­˜å‚¨è¯¦ç»†ç»“æœ
        self.detailed_results = []
        self.confusion_matrix = {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0}
        self.uncertain_predictions = 0
        
    def get_negative_candidates(self):
        """è·å–è´Ÿæ ·æœ¬å€™é€‰"""
        return [
            road for road, prob in self.marginals_dict.items() 
            if road in self.bn_nodes and prob <= self.min_marginal_for_negative
        ]
    
    def make_prediction_with_details(self, target_road, evidence):
        """è¿›è¡Œé¢„æµ‹å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        try:
            result = self.flood_net.infer_w_evidence(target_road, evidence)
            prob_flood = result['flooded']
            
            # è·å–çˆ¶èŠ‚ç‚¹ä¿¡æ¯
            parents = list(self.flood_net.network.predecessors(target_road))
            relevant_evidence = {k: v for k, v in evidence.items() if k in parents}
            
            # é˜ˆå€¼å†³ç­–
            if prob_flood >= self.positive_threshold:
                prediction = 1
                decision = "POSITIVE"
                confidence = prob_flood
            elif prob_flood <= self.negative_threshold:
                prediction = 0
                decision = "NEGATIVE" 
                confidence = 1 - prob_flood
            else:
                prediction = -1
                decision = "UNCERTAIN"
                confidence = 0.5
            
            return {
                'prob_flood': prob_flood,
                'prediction': prediction,
                'decision': decision,
                'confidence': confidence,
                'parents': parents,
                'relevant_evidence': relevant_evidence,
                'marginal_prob': self.marginals_dict.get(target_road, 0),
                'success': True,
                'error': None
            }
            
        except Exception as e:
            return {
                'prob_flood': 0.0,
                'prediction': -1,
                'decision': "ERROR",
                'confidence': 0.0,
                'parents': [],
                'relevant_evidence': {},
                'marginal_prob': self.marginals_dict.get(target_road, 0),
                'success': False,
                'error': str(e)
            }
    
    def evaluate_single_day(self, date, day_group, day_index):
        """è¯„ä¼°å•æ—¥æ•°æ®"""
        print(f"\nğŸ“… ã€ç¬¬{day_index}å¤©ã€‘{date.date()}")
        print("=" * 60)
        
        # è·å–å½“å¤©æ´ªæ°´é“è·¯
        flooded_roads = list(day_group["link_id"].unique())
        flooded_in_bn = [road for road in flooded_roads if road in self.bn_nodes]
        
        print(f"ğŸŒŠ å½“å¤©æ´ªæ°´æƒ…å†µ:")
        print(f"   åŸå§‹æ´ªæ°´é“è·¯: {len(flooded_roads)}æ¡")
        print(f"   ç½‘ç»œä¸­æ´ªæ°´é“è·¯: {len(flooded_in_bn)}æ¡")
        print(f"   æ´ªæ°´é“è·¯åˆ—è¡¨: {flooded_roads}")
        print(f"   ç½‘ç»œé“è·¯åˆ—è¡¨: {flooded_in_bn}")
        
        day_results = {
            'date': date,
            'flooded_roads': flooded_roads,
            'flooded_in_bn': flooded_in_bn,
            'positive_samples': [],
            'negative_samples': [],
            'day_tp': 0,
            'day_fp': 0,
            'day_tn': 0,
            'day_fn': 0,
            'day_uncertain': 0
        }
        
        if len(flooded_in_bn) < 2:
            print("âš ï¸  å¯ç”¨ç½‘ç»œé“è·¯ä¸è¶³2æ¡ï¼Œè·³è¿‡æ­¤æ—¥æœŸ")
            return day_results
        
        # Evidenceé€‰æ‹©ç­–ç•¥
        evidence_count = max(1, int(len(flooded_in_bn) * 0.5))
        evidence_roads = flooded_in_bn[:evidence_count]
        target_roads = flooded_in_bn[evidence_count:]
        
        evidence = {road: 1 for road in evidence_roads}
        
        print(f"\nğŸ¯ Evidenceè®¾ç½®:")
        print(f"   Evidenceé“è·¯ ({len(evidence_roads)}æ¡): {evidence_roads}")
        for road in evidence_roads:
            marginal_p = self.marginals_dict.get(road, 0)
            print(f"     {road}: P(æ´ªæ°´)={marginal_p:.3f}")
        
        print(f"\nğŸ” æ­£æ ·æœ¬æ¨ç† ({len(target_roads)}æ¡):")
        
        # å¤„ç†æ­£æ ·æœ¬ï¼ˆçœŸå®æ´ªæ°´é“è·¯ï¼‰
        for i, target_road in enumerate(target_roads, 1):
            pred_details = self.make_prediction_with_details(target_road, evidence)
            
            sample_result = {
                'type': 'POSITIVE',
                'road': target_road,
                'true_label': 1,
                'evidence': evidence.copy(),
                'prediction_details': pred_details
            }
            
            day_results['positive_samples'].append(sample_result)
            
            # æ˜¾ç¤ºè¯¦ç»†æ¨ç†è¿‡ç¨‹
            print(f"\n   [{i}] ç›®æ ‡é“è·¯: {target_road}")
            print(f"       è¾¹é™…æ¦‚ç‡: P(æ´ªæ°´)={pred_details['marginal_prob']:.6f}")
            print(f"       çˆ¶èŠ‚ç‚¹: {pred_details['parents']}")
            print(f"       ç›¸å…³Evidence: {pred_details['relevant_evidence']}")
            
            if pred_details['success']:
                print(f"       æ¨ç†æ¦‚ç‡: P(æ´ªæ°´|Evidence)={pred_details['prob_flood']:.6f}")
                print(f"       å†³ç­–: {pred_details['decision']} (ç½®ä¿¡åº¦={pred_details['confidence']:.3f})")
                
                # è®¡ç®—æ··æ·†çŸ©é˜µè´¡çŒ®
                if pred_details['prediction'] == 1:
                    day_results['day_tp'] += 1
                    result_type = "TP âœ…"
                elif pred_details['prediction'] == 0:
                    day_results['day_fn'] += 1
                    result_type = "FN âŒ"
                else:
                    day_results['day_uncertain'] += 1
                    result_type = "UNCERTAIN â“"
                
                print(f"       ç»“æœ: é¢„æµ‹={pred_details['prediction']}, çœŸå®=1 â†’ {result_type}")
            else:
                print(f"       âŒ æ¨ç†å¤±è´¥: {pred_details['error']}")
                day_results['day_uncertain'] += 1
        
        # å¤„ç†è´Ÿæ ·æœ¬
        negative_candidates = self.get_negative_candidates()
        available_negatives = [road for road in negative_candidates if road not in flooded_roads]
        selected_negatives = available_negatives[:min(3, len(target_roads))]
        
        if selected_negatives:
            print(f"\nğŸš« è´Ÿæ ·æœ¬æ¨ç† ({len(selected_negatives)}æ¡):")
            
            for i, neg_road in enumerate(selected_negatives, 1):
                pred_details = self.make_prediction_with_details(neg_road, evidence)
                
                sample_result = {
                    'type': 'NEGATIVE',
                    'road': neg_road,
                    'true_label': 0,
                    'evidence': evidence.copy(),
                    'prediction_details': pred_details
                }
                
                day_results['negative_samples'].append(sample_result)
                
                print(f"\n   [{i}] è´Ÿæ ·æœ¬é“è·¯: {neg_road}")
                print(f"       è¾¹é™…æ¦‚ç‡: P(æ´ªæ°´)={pred_details['marginal_prob']:.6f}")
                print(f"       é€‰æ‹©ç†ç”±: ä½æ¦‚ç‡ä¸”å½“å¤©æ— æ´ªæ°´è®°å½•")
                
                if pred_details['success']:
                    print(f"       æ¨ç†æ¦‚ç‡: P(æ´ªæ°´|Evidence)={pred_details['prob_flood']:.6f}")
                    print(f"       å†³ç­–: {pred_details['decision']} (ç½®ä¿¡åº¦={pred_details['confidence']:.3f})")
                    
                    # è®¡ç®—æ··æ·†çŸ©é˜µè´¡çŒ®
                    if pred_details['prediction'] == 0:
                        day_results['day_tn'] += 1
                        result_type = "TN âœ…"
                    elif pred_details['prediction'] == 1:
                        day_results['day_fp'] += 1
                        result_type = "FP âŒ"
                    else:
                        day_results['day_uncertain'] += 1
                        result_type = "UNCERTAIN â“"
                    
                    print(f"       ç»“æœ: é¢„æµ‹={pred_details['prediction']}, çœŸå®=0 â†’ {result_type}")
                else:
                    print(f"       âŒ æ¨ç†å¤±è´¥: {pred_details['error']}")
                    day_results['day_uncertain'] += 1
        
        # å½“æ—¥ç»Ÿè®¡
        total_samples = len(day_results['positive_samples']) + len(day_results['negative_samples'])
        
        print(f"\nğŸ“Š å½“æ—¥ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   TP: {day_results['day_tp']}, FP: {day_results['day_fp']}")
        print(f"   TN: {day_results['day_tn']}, FN: {day_results['day_fn']}")
        print(f"   ä¸ç¡®å®š: {day_results['day_uncertain']}")
        
        if total_samples > 0:
            day_precision = day_results['day_tp'] / (day_results['day_tp'] + day_results['day_fp']) if (day_results['day_tp'] + day_results['day_fp']) > 0 else 0
            day_recall = day_results['day_tp'] / (day_results['day_tp'] + day_results['day_fn']) if (day_results['day_tp'] + day_results['day_fn']) > 0 else 0
            day_accuracy = (day_results['day_tp'] + day_results['day_tn']) / (total_samples - day_results['day_uncertain']) if (total_samples - day_results['day_uncertain']) > 0 else 0
            
            print(f"   å½“æ—¥ç²¾ç¡®åº¦: {day_precision:.3f}")
            print(f"   å½“æ—¥å¬å›ç‡: {day_recall:.3f}")
            print(f"   å½“æ—¥å‡†ç¡®ç‡: {day_accuracy:.3f}")
        
        return day_results
    
    def run_detailed_evaluation(self, max_days=None):
        """è¿è¡Œè¯¦ç»†è¯„ä¼°"""
        print("ğŸ”¬ é€æ ·æœ¬è¯¦ç»†è¯„ä¼°å¼€å§‹")
        print("=" * 80)
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        
        day_groups = list(test_by_date)
        if max_days:
            day_groups = day_groups[:max_days]
        
        print(f"ğŸ“… å°†è¯„ä¼° {len(day_groups)} ä¸ªæµ‹è¯•æ—¥")
        
        # é€æ—¥è¯„ä¼°
        for day_index, (date, day_group) in enumerate(day_groups, 1):
            day_result = self.evaluate_single_day(date, day_group, day_index)
            
            if day_result['positive_samples'] or day_result['negative_samples']:
                self.detailed_results.append(day_result)
                
                # ç´¯è®¡æ··æ·†çŸ©é˜µ
                self.confusion_matrix['TP'] += day_result['day_tp']
                self.confusion_matrix['FP'] += day_result['day_fp']
                self.confusion_matrix['TN'] += day_result['day_tn']
                self.confusion_matrix['FN'] += day_result['day_fn']
                self.uncertain_predictions += day_result['day_uncertain']
        
        return self.detailed_results
    
    def generate_confusion_matrix_analysis(self):
        """ç”Ÿæˆæ··æ·†çŸ©é˜µåˆ†æ"""
        print(f"\n\nğŸ“ˆ æ··æ·†çŸ©é˜µè¯¦ç»†åˆ†æ")
        print("=" * 80)
        
        # åŸºç¡€ç»Ÿè®¡
        tp = self.confusion_matrix['TP']
        fp = self.confusion_matrix['FP']
        tn = self.confusion_matrix['TN']
        fn = self.confusion_matrix['FN']
        uncertain = self.uncertain_predictions
        
        total_predictions = tp + fp + tn + fn
        total_samples = total_predictions + uncertain
        
        print(f"ğŸ”¢ æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   æœ‰æ•ˆé¢„æµ‹: {total_predictions}")
        print(f"   ä¸ç¡®å®šé¢„æµ‹: {uncertain}")
        print(f"   å¼ƒæƒç‡: {uncertain/total_samples*100:.1f}%")
        
        # æ··æ·†çŸ©é˜µ
        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print(f"                  é¢„æµ‹")
        print(f"              æ­£ç±»    è´Ÿç±»")
        print(f"    çœŸå® æ­£ç±»  {tp:4d}   {fn:4d}")
        print(f"         è´Ÿç±»  {fp:4d}   {tn:4d}")
        
        # è¯¦ç»†åˆ†ç±»ç»Ÿè®¡
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»:")
        print(f"   True Positives (TP):  {tp:4d} - æ­£ç¡®é¢„æµ‹ä¸ºæ´ªæ°´")
        print(f"   False Positives (FP): {fp:4d} - é”™è¯¯é¢„æµ‹ä¸ºæ´ªæ°´")
        print(f"   True Negatives (TN):  {tn:4d} - æ­£ç¡®é¢„æµ‹ä¸ºæ— æ´ªæ°´")
        print(f"   False Negatives (FN): {fn:4d} - é”™è¯¯é¢„æµ‹ä¸ºæ— æ´ªæ°´")
        print(f"   Uncertain:            {uncertain:4d} - ä¸ç¡®å®šé¢„æµ‹(å¼ƒæƒ)")
        
        # è®¡ç®—æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        accuracy = (tp + tn) / total_predictions if total_predictions > 0 else 0.0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   ç²¾ç¡®åº¦ (Precision):    {precision:.6f}")
        print(f"   å¬å›ç‡ (Recall):       {recall:.6f}")
        print(f"   ç‰¹å¼‚æ€§ (Specificity):  {specificity:.6f}")
        print(f"   å‡†ç¡®ç‡ (Accuracy):     {accuracy:.6f}")
        print(f"   F1åˆ†æ•° (F1-Score):     {f1_score:.6f}")
        
        # ç›®æ ‡è¾¾æˆæƒ…å†µ
        print(f"\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        precision_target = 0.8
        recall_target = 0.3
        
        precision_status = "âœ… è¾¾æˆ" if precision >= precision_target else "âŒ æœªè¾¾æˆ"
        recall_status = "âœ… è¾¾æˆ" if recall >= recall_target else "âŒ æœªè¾¾æˆ"
        
        print(f"   ç²¾ç¡®åº¦ç›®æ ‡ (â‰¥{precision_target}): {precision:.3f} {precision_status}")
        print(f"   å¬å›ç‡ç›®æ ‡ (â‰¥{recall_target}):  {recall:.3f} {recall_status}")
        
        return {
            'confusion_matrix': self.confusion_matrix,
            'metrics': {
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'accuracy': accuracy,
                'f1_score': f1_score
            },
            'targets': {
                'precision_target': precision_target,
                'recall_target': recall_target,
                'precision_achieved': precision >= precision_target,
                'recall_achieved': recall >= recall_target
            }
        }
    
    def create_confusion_matrix_visualization(self, save_path="confusion_matrix.png"):
        """åˆ›å»ºæ··æ·†çŸ©é˜µå¯è§†åŒ–"""
        print(f"\nğŸ¨ ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–")
        
        # å‡†å¤‡æ•°æ®
        tp = self.confusion_matrix['TP']
        fp = self.confusion_matrix['FP']
        tn = self.confusion_matrix['TN']
        fn = self.confusion_matrix['FN']
        
        # åˆ›å»ºæ··æ·†çŸ©é˜µæ•°ç»„
        cm_array = np.array([[tp, fn], [fp, tn]])
        
        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ··æ·†çŸ©é˜µçƒ­å›¾
        sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['é¢„æµ‹è´Ÿç±»', 'é¢„æµ‹æ­£ç±»'],
                   yticklabels=['çœŸå®æ­£ç±»', 'çœŸå®è´Ÿç±»'],
                   ax=ax1, annot_kws={'size': 14})
        ax1.set_title('æ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
        ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        
        # åœ¨çƒ­å›¾ä¸­æ·»åŠ ç™¾åˆ†æ¯”
        total = cm_array.sum()
        for i in range(2):
            for j in range(2):
                percentage = cm_array[i, j] / total * 100
                ax1.text(j+0.5, i+0.7, f'({percentage:.1f}%)', 
                        ha='center', va='center', fontsize=10, color='gray')
        
        # æŒ‡æ ‡æŸ±çŠ¶å›¾
        metrics = self.generate_confusion_matrix_analysis()['metrics']
        metric_names = ['ç²¾ç¡®åº¦', 'å¬å›ç‡', 'ç‰¹å¼‚æ€§', 'å‡†ç¡®ç‡', 'F1åˆ†æ•°']
        metric_values = [metrics['precision'], metrics['recall'], 
                        metrics['specificity'], metrics['accuracy'], metrics['f1_score']]
        
        colors = ['#ff7f0e' if name in ['ç²¾ç¡®åº¦', 'å¬å›ç‡'] else '#1f77b4' for name in metric_names]
        bars = ax2.bar(metric_names, metric_values, color=colors, alpha=0.7)
        
        # æ·»åŠ ç›®æ ‡çº¿
        ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='ç²¾ç¡®åº¦ç›®æ ‡(0.8)')
        ax2.axhline(y=0.3, color='orange', linestyle='--', alpha=0.7, label='å¬å›ç‡ç›®æ ‡(0.3)')
        
        ax2.set_title('æ€§èƒ½æŒ‡æ ‡', fontsize=16, fontweight='bold')
        ax2.set_ylabel('å¾—åˆ†', fontsize=12)
        ax2.set_ylim(0, 1)
        ax2.legend()
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ··æ·†çŸ©é˜µå¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}")
        plt.show()
        
        return fig
    
    def generate_sample_summary_table(self):
        """ç”Ÿæˆæ ·æœ¬æ±‡æ€»è¡¨"""
        print(f"\nğŸ“‹ ç”Ÿæˆæ ·æœ¬æ±‡æ€»è¡¨")
        
        all_samples = []
        
        for day_result in self.detailed_results:
            date = day_result['date']
            
            # å¤„ç†æ­£æ ·æœ¬
            for sample in day_result['positive_samples']:
                pred_details = sample['prediction_details']
                all_samples.append({
                    'Date': date.date(),
                    'Road': sample['road'],
                    'Type': 'Positive',
                    'True_Label': sample['true_label'],
                    'Marginal_Prob': pred_details['marginal_prob'],
                    'Inferred_Prob': pred_details['prob_flood'],
                    'Prediction': pred_details['prediction'],
                    'Decision': pred_details['decision'],
                    'Confidence': pred_details['confidence'],
                    'Result_Type': self._get_result_type(pred_details['prediction'], sample['true_label']),
                    'Evidence_Count': len(sample['evidence']),
                    'Parents': len(pred_details['parents']),
                    'Relevant_Evidence': len(pred_details['relevant_evidence'])
                })
            
            # å¤„ç†è´Ÿæ ·æœ¬
            for sample in day_result['negative_samples']:
                pred_details = sample['prediction_details']
                all_samples.append({
                    'Date': date.date(),
                    'Road': sample['road'],
                    'Type': 'Negative',
                    'True_Label': sample['true_label'],
                    'Marginal_Prob': pred_details['marginal_prob'],
                    'Inferred_Prob': pred_details['prob_flood'],
                    'Prediction': pred_details['prediction'],
                    'Decision': pred_details['decision'],
                    'Confidence': pred_details['confidence'],
                    'Result_Type': self._get_result_type(pred_details['prediction'], sample['true_label']),
                    'Evidence_Count': len(sample['evidence']),
                    'Parents': len(pred_details['parents']),
                    'Relevant_Evidence': len(pred_details['relevant_evidence'])
                })
        
        # åˆ›å»ºDataFrame
        df_summary = pd.DataFrame(all_samples)
        
        # ä¿å­˜åˆ°CSV
        csv_path = "sample_by_sample_results.csv"
        df_summary.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… æ ·æœ¬æ±‡æ€»è¡¨å·²ä¿å­˜è‡³: {csv_path}")
        
        # æ˜¾ç¤ºå‰10è¡Œ
        print(f"\nğŸ“Š æ ·æœ¬æ±‡æ€»è¡¨é¢„è§ˆ (å‰10è¡Œ):")
        print(df_summary.head(10).to_string(index=False))
        
        # ç»Ÿè®¡æ±‡æ€»
        print(f"\nğŸ“ˆ æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(df_summary)}")
        print(f"   æ­£æ ·æœ¬æ•°: {len(df_summary[df_summary['Type'] == 'Positive'])}")
        print(f"   è´Ÿæ ·æœ¬æ•°: {len(df_summary[df_summary['Type'] == 'Negative'])}")
        
        result_counts = df_summary['Result_Type'].value_counts()
        for result_type, count in result_counts.items():
            print(f"   {result_type}: {count}")
        
        return df_summary
    
    def _get_result_type(self, prediction, true_label):
        """è·å–ç»“æœç±»å‹"""
        if prediction == -1:
            return "UNCERTAIN"
        elif prediction == 1 and true_label == 1:
            return "TP"
        elif prediction == 1 and true_label == 0:
            return "FP"
        elif prediction == 0 and true_label == 1:
            return "FN"
        elif prediction == 0 and true_label == 0:
            return "TN"
        else:
            return "UNKNOWN"

def load_test_system():
    """åŠ è½½æµ‹è¯•ç³»ç»Ÿ"""
    # åŠ è½½æ•°æ®
    df = pd.read_csv("Road_Closures_2024.csv")
    df = df[df["REASON"].str.upper() == "FLOOD"].copy()
    
    # é¢„å¤„ç†
    df["time_create"] = pd.to_datetime(df["START"], utc=True)
    df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
    df["link_id"] = df["link_id"].astype(str)
    df["id"] = df["OBJECTID"].astype(str)
    
    # æ—¶åºåˆ†å‰²
    df_sorted = df.sort_values('time_create')
    split_idx = int(len(df_sorted) * 0.7)
    train_df = df_sorted.iloc[:split_idx].copy()
    test_df = df_sorted.iloc[split_idx:].copy()
    
    # æ„å»ºç½‘ç»œ
    flood_net = FloodBayesNetwork(t_window="D")
    flood_net.fit_marginal(train_df)
    flood_net.build_network_by_co_occurrence(
        train_df, occ_thr=3, edge_thr=2, weight_thr=0.3, report=False
    )
    flood_net.fit_conditional(train_df, max_parents=2, alpha=1.0)
    flood_net.build_bayes_network()
    
    return flood_net, test_df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ Charlestonæ´ªæ°´é¢„æµ‹ - é€æ ·æœ¬è¯¦ç»†è¯„ä¼°")
    print("=" * 80)
    
    # åŠ è½½ç³»ç»Ÿ
    flood_net, test_df = load_test_system()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SampleBySeamleEvaluator(flood_net, test_df)
    
    print(f"ğŸ“Š ç³»ç»Ÿé…ç½®:")
    print(f"   ç½‘ç»œèŠ‚ç‚¹æ•°: {len(evaluator.bn_nodes)}")
    print(f"   æµ‹è¯•å¤©æ•°: {test_df['time_create'].dt.floor('D').nunique()}")
    print(f"   æ­£é¢„æµ‹é˜ˆå€¼: {evaluator.positive_threshold}")
    print(f"   è´Ÿé¢„æµ‹é˜ˆå€¼: {evaluator.negative_threshold}")
    
    # è¿è¡Œè¯¦ç»†è¯„ä¼°ï¼ˆé™åˆ¶å¤©æ•°ä»¥é¿å…è¾“å‡ºè¿‡é•¿ï¼‰
    detailed_results = evaluator.run_detailed_evaluation(max_days=10)  # å¯ä»¥è°ƒæ•´å¤©æ•°
    
    # ç”Ÿæˆæ··æ·†çŸ©é˜µåˆ†æ
    analysis_results = evaluator.generate_confusion_matrix_analysis()
    
    # åˆ›å»ºå¯è§†åŒ–
    fig = evaluator.create_confusion_matrix_visualization()
    
    # ç”Ÿæˆæ ·æœ¬æ±‡æ€»è¡¨
    df_summary = evaluator.generate_sample_summary_table()
    
    print(f"\nğŸ‰ é€æ ·æœ¬è¯¦ç»†è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - confusion_matrix.png (æ··æ·†çŸ©é˜µå¯è§†åŒ–)")
    print(f"   - sample_by_sample_results.csv (è¯¦ç»†æ ·æœ¬ç»“æœ)")
    
    return evaluator, analysis_results, df_summary

if __name__ == "__main__":
    evaluator, results, summary_df = main()