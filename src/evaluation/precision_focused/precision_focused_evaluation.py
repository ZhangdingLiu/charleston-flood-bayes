#!/usr/bin/env python3
"""
Precision-Focused Evaluation Strategy for Charleston Flood Prediction

Designed to handle observational bias in police data:
- Flood=1 records are reliable (police observed flooding)
- Flood=0 (no records) may be observation gaps, not true negatives

Key Features:
1. Multi-evidence inference (40-60% roads as evidence)
2. Conservative negative sampling from low-probability roads
3. Dual-threshold system (high for positive, low for negative)
4. Confidence-based evaluation with abstention
5. Precision optimization targeting â‰¥0.8, Recall 0.3-0.5
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
from collections import defaultdict, Counter
from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class PrecisionFocusedEvaluator:
    """
    ç²¾ç¡®åº¦ä¼˜å…ˆçš„è¯„ä¼°å™¨ï¼Œä¸“é—¨å¤„ç†Charlestonè­¦å¯Ÿæ•°æ®çš„è§‚æµ‹åå·®
    """
    
    def __init__(self, flood_net, test_df):
        self.flood_net = flood_net
        self.test_df = test_df
        self.bn_nodes = set(flood_net.network_bayes.nodes()) if flood_net.network_bayes else set()
        self.marginals_dict = dict(zip(
            flood_net.marginals['link_id'], 
            flood_net.marginals['p']
        )) if flood_net.marginals is not None else {}
        
        # ç­–ç•¥å‚æ•°
        self.evidence_ratio = 0.5  # ç”¨ä½œevidenceçš„é“è·¯æ¯”ä¾‹
        self.positive_threshold = 0.7  # æ­£é¢„æµ‹é˜ˆå€¼ï¼ˆé«˜ç²¾åº¦ï¼‰
        self.negative_threshold = 0.3  # è´Ÿé¢„æµ‹é˜ˆå€¼ï¼ˆä¿å®ˆï¼‰
        self.min_marginal_for_negative = 0.15  # è´Ÿæ ·æœ¬å€™é€‰çš„æœ€å¤§è¾¹é™…æ¦‚ç‡
        
    def identify_reliable_negative_candidates(self):
        """
        è¯†åˆ«å¯é çš„è´Ÿæ ·æœ¬å€™é€‰ï¼šç½‘ç»œä¸­è¾¹é™…æ¦‚ç‡å¾ˆä½çš„é“è·¯
        """
        negative_candidates = []
        for road, prob in self.marginals_dict.items():
            if road in self.bn_nodes and prob <= self.min_marginal_for_negative:
                negative_candidates.append(road)
        
        return negative_candidates
    
    def select_evidence_roads(self, flooded_roads, strategy='centrality'):
        """
        é€‰æ‹©evidenceé“è·¯çš„ç­–ç•¥
        
        Args:
            flooded_roads: å½“å¤©æ´ªæ°´é“è·¯åˆ—è¡¨
            strategy: é€‰æ‹©ç­–ç•¥ ('first', 'random', 'centrality', 'high_marginal')
        """
        if len(flooded_roads) < 2:
            return [], flooded_roads
            
        evidence_count = max(1, int(len(flooded_roads) * self.evidence_ratio))
        evidence_count = min(evidence_count, len(flooded_roads) - 1)  # è‡³å°‘ä¿ç•™1ä¸ªä½œä¸ºç›®æ ‡
        
        if strategy == 'first':
            evidence_roads = flooded_roads[:evidence_count]
        elif strategy == 'random':
            evidence_roads = random.sample(flooded_roads, evidence_count)
        elif strategy == 'centrality':
            # æŒ‰ç½‘ç»œä¸­å¿ƒæ€§æ’åºï¼ˆå…¥åº¦+å‡ºåº¦ï¼‰
            centrality_scores = []
            for road in flooded_roads:
                in_deg = self.flood_net.network.in_degree(road) if road in self.flood_net.network else 0
                out_deg = self.flood_net.network.out_degree(road) if road in self.flood_net.network else 0
                centrality_scores.append((road, in_deg + out_deg))
            
            centrality_scores.sort(key=lambda x: x[1], reverse=True)
            evidence_roads = [road for road, _ in centrality_scores[:evidence_count]]
        elif strategy == 'high_marginal':
            # æŒ‰è¾¹é™…æ¦‚ç‡æ’åº
            marginal_scores = [(road, self.marginals_dict.get(road, 0)) for road in flooded_roads]
            marginal_scores.sort(key=lambda x: x[1], reverse=True)
            evidence_roads = [road for road, _ in marginal_scores[:evidence_count]]
        else:
            evidence_roads = flooded_roads[:evidence_count]
            
        target_roads = [road for road in flooded_roads if road not in evidence_roads]
        return evidence_roads, target_roads
    
    def get_temporal_negative_samples(self, date, flooded_roads, max_samples=3):
        """
        è·å–æ—¶é—´è´Ÿæ ·æœ¬ï¼šå½“å¤©æ²¡æœ‰æ´ªæ°´ä½†åœ¨ç½‘ç»œä¸­çš„é“è·¯
        åªé€‰æ‹©è¾¹é™…æ¦‚ç‡è¾ƒä½çš„ä½œä¸ºè´Ÿæ ·æœ¬å€™é€‰
        """
        # å½“å¤©æ²¡æœ‰æ´ªæ°´çš„ç½‘ç»œé“è·¯
        non_flooded = [road for road in self.bn_nodes if road not in flooded_roads]
        
        # åªé€‰æ‹©è¾¹é™…æ¦‚ç‡ä½çš„é“è·¯ä½œä¸ºè´Ÿæ ·æœ¬å€™é€‰
        reliable_negatives = [
            road for road in non_flooded 
            if self.marginals_dict.get(road, 1.0) <= self.min_marginal_for_negative
        ]
        
        # éšæœºé‡‡æ ·ï¼Œé™åˆ¶è´Ÿæ ·æœ¬æ•°é‡
        max_samples = min(max_samples, len(reliable_negatives))
        if max_samples > 0:
            return random.sample(reliable_negatives, max_samples)
        return []
    
    def make_prediction(self, target_road, evidence, return_prob=False):
        """
        è¿›è¡Œé¢„æµ‹å¹¶åº”ç”¨åŒé˜ˆå€¼ç­–ç•¥
        
        Returns:
            prediction: 1 (flood), 0 (no flood), -1 (uncertain/abstain)
            confidence: prediction confidence level
        """
        try:
            result = self.flood_net.infer_w_evidence(target_road, evidence)
            prob_flood = result['flooded']
            
            if prob_flood >= self.positive_threshold:
                prediction = 1
                confidence = prob_flood
            elif prob_flood <= self.negative_threshold:
                prediction = 0
                confidence = 1 - prob_flood
            else:
                prediction = -1  # uncertain, abstain
                confidence = 0.5
                
            if return_prob:
                return prediction, confidence, prob_flood
            return prediction, confidence
            
        except Exception as e:
            if return_prob:
                return -1, 0.0, 0.5
            return -1, 0.0
    
    def evaluate_precision_focused(self, evidence_strategy='centrality', include_negatives=True, verbose=False):
        """
        ç²¾ç¡®åº¦ä¼˜å…ˆçš„è¯„ä¼°æ–¹æ³•
        
        Args:
            evidence_strategy: evidenceé€‰æ‹©ç­–ç•¥
            include_negatives: æ˜¯å¦åŒ…å«è´Ÿæ ·æœ¬æµ‹è¯•
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        results = {
            'total_days': 0,
            'evaluated_days': 0,
            'positive_samples': [],  # (prediction, confidence, true_label, prob)
            'negative_samples': [],
            'uncertain_samples': [],
            'evidence_strategy': evidence_strategy
        }
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        
        for date, day_group in test_by_date:
            results['total_days'] += 1
            
            # å½“å¤©æ´ªæ°´é“è·¯åˆ—è¡¨ï¼ˆåªè€ƒè™‘åœ¨è´å¶æ–¯ç½‘ç»œä¸­çš„é“è·¯ï¼‰
            flooded_roads = list(day_group["link_id"].unique())
            flooded_in_bn = [road for road in flooded_roads if road in self.bn_nodes]
            
            if len(flooded_in_bn) < 2:
                continue  # éœ€è¦è‡³å°‘2æ¡é“è·¯æ‰èƒ½åšæ¨ç†
                
            results['evaluated_days'] += 1
            
            # é€‰æ‹©evidenceå’Œtargeté“è·¯
            evidence_roads, target_roads = self.select_evidence_roads(
                flooded_in_bn, strategy=evidence_strategy
            )
            
            if len(target_roads) == 0:
                continue
                
            evidence = {road: 1 for road in evidence_roads}
            
            if verbose and results['evaluated_days'] <= 3:
                print(f"ğŸ“… {date.date()}: æ´ªæ°´é“è·¯{len(flooded_in_bn)}, "
                      f"evidence{len(evidence_roads)}, target{len(target_roads)}")
                print(f"   Evidence: {evidence_roads}")
                print(f"   Targets: {target_roads}")
            
            # æµ‹è¯•æ­£æ ·æœ¬ï¼ˆçœŸå®æ´ªæ°´é“è·¯ï¼‰
            for target_road in target_roads:
                pred, conf, prob = self.make_prediction(target_road, evidence, return_prob=True)
                
                if pred == 1:
                    results['positive_samples'].append((pred, conf, 1, prob))
                elif pred == 0:
                    results['positive_samples'].append((pred, conf, 1, prob))  # é”™è¯¯é¢„æµ‹
                else:
                    results['uncertain_samples'].append((pred, conf, 1, prob))
                    
                if verbose and results['evaluated_days'] <= 3:
                    status = "âœ…" if pred == 1 else "âŒ" if pred == 0 else "â“"
                    print(f"   {target_road}: P={prob:.3f} â†’ pred={pred} {status}")
            
            # æµ‹è¯•è´Ÿæ ·æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if include_negatives:
                negative_candidates = self.get_temporal_negative_samples(
                    date, flooded_in_bn, max_samples=min(3, len(target_roads))
                )
                
                for neg_road in negative_candidates:
                    pred, conf, prob = self.make_prediction(neg_road, evidence, return_prob=True)
                    
                    if pred == 0:
                        results['negative_samples'].append((pred, conf, 0, prob))
                    elif pred == 1:
                        results['negative_samples'].append((pred, conf, 0, prob))  # é”™è¯¯é¢„æµ‹
                    else:
                        results['uncertain_samples'].append((pred, conf, 0, prob))
                        
                    if verbose and results['evaluated_days'] <= 3:
                        status = "âœ…" if pred == 0 else "âŒ" if pred == 1 else "â“"
                        print(f"   {neg_road} (neg): P={prob:.3f} â†’ pred={pred} {status}")
        
        return results
    
    def calculate_metrics(self, evaluation_results, include_uncertain=False):
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        """
        pos_samples = evaluation_results['positive_samples']
        neg_samples = evaluation_results['negative_samples']
        uncertain_samples = evaluation_results['uncertain_samples']
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬
        all_samples = pos_samples + neg_samples
        if include_uncertain:
            all_samples += uncertain_samples
        
        if len(all_samples) == 0:
            return {
                'precision': 0.0, 'recall': 0.0, 'f1': 0.0,
                'accuracy': 0.0, 'samples': 0, 'abstention_rate': 0.0
            }
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        tp = sum(1 for pred, _, true, _ in all_samples if pred == 1 and true == 1)
        tn = sum(1 for pred, _, true, _ in all_samples if pred == 0 and true == 0)
        fp = sum(1 for pred, _, true, _ in all_samples if pred == 1 and true == 0)
        fn = sum(1 for pred, _, true, _ in all_samples if pred == 0 and true == 1)
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = (tp + tn) / len(all_samples) if len(all_samples) > 0 else 0.0
        
        # å¼ƒæƒç‡
        total_predictions = len(pos_samples) + len(neg_samples) + len(uncertain_samples)
        abstention_rate = len(uncertain_samples) / total_predictions if total_predictions > 0 else 0.0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,
            'samples': len(all_samples),
            'positive_samples': len(pos_samples),
            'negative_samples': len(neg_samples),
            'uncertain_samples': len(uncertain_samples),
            'abstention_rate': abstention_rate,
            'total_days': evaluation_results['total_days'],
            'evaluated_days': evaluation_results['evaluated_days']
        }
    
    def optimize_thresholds_for_precision(self, target_precision=0.8, min_recall=0.3):
        """
        ä¼˜åŒ–é˜ˆå€¼ä»¥è¾¾åˆ°ç›®æ ‡ç²¾ç¡®åº¦
        """
        print(f"\nğŸ¯ ä¼˜åŒ–é˜ˆå€¼ä»¥è¾¾åˆ°ç²¾ç¡®åº¦â‰¥{target_precision}, å¬å›ç‡â‰¥{min_recall}")
        
        # æµ‹è¯•ä¸åŒçš„é˜ˆå€¼ç»„åˆ
        pos_thresholds = np.arange(0.5, 0.95, 0.05)
        neg_thresholds = np.arange(0.05, 0.5, 0.05)
        
        best_config = None
        best_score = -1
        
        for pos_thr in pos_thresholds:
            for neg_thr in neg_thresholds:
                if neg_thr >= pos_thr:
                    continue
                    
                # æ›´æ–°é˜ˆå€¼
                self.positive_threshold = pos_thr
                self.negative_threshold = neg_thr
                
                # è¯„ä¼°
                results = self.evaluate_precision_focused(verbose=False)
                metrics = self.calculate_metrics(results)
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶
                if (metrics['precision'] >= target_precision and 
                    metrics['recall'] >= min_recall and
                    metrics['samples'] > 10):  # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
                    
                    # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆä¼˜å…ˆè€ƒè™‘recallï¼Œå› ä¸ºprecisionå·²æ»¡è¶³ï¼‰
                    score = metrics['recall'] + 0.1 * (metrics['precision'] - target_precision)
                    
                    if score > best_score:
                        best_score = score
                        best_config = {
                            'pos_threshold': pos_thr,
                            'neg_threshold': neg_thr,
                            'metrics': metrics.copy()
                        }
        
        if best_config:
            self.positive_threshold = best_config['pos_threshold']
            self.negative_threshold = best_config['neg_threshold']
            print(f"âœ… æ‰¾åˆ°æœ€ä½³é˜ˆå€¼é…ç½®:")
            print(f"   æ­£é¢„æµ‹é˜ˆå€¼: {self.positive_threshold:.2f}")
            print(f"   è´Ÿé¢„æµ‹é˜ˆå€¼: {self.negative_threshold:.2f}")
            print(f"   ç²¾ç¡®åº¦: {best_config['metrics']['precision']:.3f}")
            print(f"   å¬å›ç‡: {best_config['metrics']['recall']:.3f}")
            print(f"   F1: {best_config['metrics']['f1']:.3f}")
            print(f"   å¼ƒæƒç‡: {best_config['metrics']['abstention_rate']:.3f}")
            return best_config
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„é˜ˆå€¼é…ç½®")
            return None

def load_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    print("ğŸš€ åŠ è½½Charlestonæ´ªæ°´æ•°æ®")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv("Road_Closures_2024.csv")
    df = df[df["REASON"].str.upper() == "FLOOD"].copy()
    
    # æ•°æ®é¢„å¤„ç†
    df["time_create"] = pd.to_datetime(df["START"], utc=True)
    df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
    df["link_id"] = df["link_id"].astype(str)
    df["id"] = df["OBJECTID"].astype(str)
    
    # æ—¶åºåˆ†å‰²é¿å…æ•°æ®æ³„éœ²
    df_sorted = df.sort_values('time_create')
    split_idx = int(len(df_sorted) * 0.7)
    train_df = df_sorted.iloc[:split_idx].copy()
    test_df = df_sorted.iloc[split_idx:].copy()
    
    print(f"æ€»æ´ªæ°´è®°å½•: {len(df)}æ¡")
    print(f"è®­ç»ƒé›†: {len(train_df)}æ¡")
    print(f"æµ‹è¯•é›†: {len(test_df)}æ¡")
    
    return train_df, test_df

def build_network(train_df):
    """æ„å»ºè´å¶æ–¯ç½‘ç»œ"""
    print("\nğŸ“Š æ„å»ºè´å¶æ–¯ç½‘ç»œ")
    
    flood_net = FloodBayesNetwork(t_window="D")
    flood_net.fit_marginal(train_df)
    
    # ä½¿ç”¨è¾ƒä¼˜å‚æ•°æ„å»ºç½‘ç»œ
    flood_net.build_network_by_co_occurrence(
        train_df,
        occ_thr=3,
        edge_thr=2,
        weight_thr=0.3,
        report=False
    )
    
    print(f"èŠ‚ç‚¹æ•°: {flood_net.network.number_of_nodes()}")
    print(f"è¾¹æ•°: {flood_net.network.number_of_edges()}")
    
    if flood_net.network.number_of_nodes() == 0:
        print("âŒ ç½‘ç»œä¸ºç©º")
        return None
    
    # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡å’Œæ„å»ºè´å¶æ–¯ç½‘ç»œ
    flood_net.fit_conditional(train_df, max_parents=2, alpha=1.0)
    flood_net.build_bayes_network()
    
    return flood_net

def main():
    """ä¸»å‡½æ•°"""
    # 1. åŠ è½½æ•°æ®
    train_df, test_df = load_data()
    
    # 2. æ„å»ºç½‘ç»œ
    flood_net = build_network(train_df)
    if flood_net is None:
        print("âŒ æ— æ³•æ„å»ºæœ‰æ•ˆç½‘ç»œ")
        return
    
    # 3. åˆ›å»ºç²¾ç¡®åº¦ä¼˜å…ˆè¯„ä¼°å™¨
    print("\nğŸ¯ åˆ›å»ºç²¾ç¡®åº¦ä¼˜å…ˆè¯„ä¼°å™¨")
    evaluator = PrecisionFocusedEvaluator(flood_net, test_df)
    
    print(f"ç½‘ç»œèŠ‚ç‚¹æ•°: {len(evaluator.bn_nodes)}")
    print(f"ä½æ¦‚ç‡è´Ÿæ ·æœ¬å€™é€‰: {len(evaluator.identify_reliable_negative_candidates())}ä¸ª")
    
    # 4. é˜ˆå€¼ä¼˜åŒ–
    best_config = evaluator.optimize_thresholds_for_precision(
        target_precision=0.8, 
        min_recall=0.3
    )
    
    if best_config is None:
        print("ä½¿ç”¨é»˜è®¤é˜ˆå€¼ç»§ç»­è¯„ä¼°...")
        evaluator.positive_threshold = 0.7
        evaluator.negative_threshold = 0.3
    
    # 5. æµ‹è¯•ä¸åŒçš„evidenceç­–ç•¥
    print("\nğŸ“ˆ æµ‹è¯•ä¸åŒEvidenceé€‰æ‹©ç­–ç•¥")
    strategies = ['centrality', 'high_marginal', 'first', 'random']
    
    strategy_results = {}
    for strategy in strategies:
        print(f"\n--- {strategy.upper()} ç­–ç•¥ ---")
        results = evaluator.evaluate_precision_focused(
            evidence_strategy=strategy, 
            include_negatives=True,
            verbose=(strategy == 'centrality')  # åªå¯¹ç¬¬ä¸€ä¸ªç­–ç•¥æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        )
        metrics = evaluator.calculate_metrics(results)
        strategy_results[strategy] = metrics
        
        print(f"ç²¾ç¡®åº¦: {metrics['precision']:.3f}")
        print(f"å¬å›ç‡: {metrics['recall']:.3f}")
        print(f"F1åˆ†æ•°: {metrics['f1']:.3f}")
        print(f"å¼ƒæƒç‡: {metrics['abstention_rate']:.3f}")
        print(f"æ ·æœ¬æ•°: {metrics['samples']} (æ­£:{metrics['positive_samples']}, è´Ÿ:{metrics['negative_samples']}, ä¸ç¡®å®š:{metrics['uncertain_samples']})")
    
    # 6. æ€»ç»“æœ€ä½³ç­–ç•¥
    print("\nğŸ† ç­–ç•¥å¯¹æ¯”æ€»ç»“")
    print(f"{'ç­–ç•¥':<12} {'ç²¾ç¡®åº¦':<8} {'å¬å›ç‡':<8} {'F1':<8} {'å¼ƒæƒç‡':<8} {'æ ·æœ¬æ•°':<8}")
    print("-" * 65)
    
    best_strategy = None
    best_f1 = -1
    
    for strategy, metrics in strategy_results.items():
        print(f"{strategy:<12} {metrics['precision']:<8.3f} {metrics['recall']:<8.3f} "
              f"{metrics['f1']:<8.3f} {metrics['abstention_rate']:<8.3f} {metrics['samples']:<8}")
        
        if metrics['f1'] > best_f1 and metrics['precision'] >= 0.8:
            best_f1 = metrics['f1']
            best_strategy = strategy
    
    print(f"\nâœ… æ¨èç­–ç•¥: {best_strategy.upper() if best_strategy else 'CENTRALITY'}")
    print(f"ğŸ¯ æˆåŠŸå®ç°ç²¾ç¡®åº¦ä¼˜å…ˆè¯„ä¼°ï¼Œé€‚åˆCharlestonè­¦å¯Ÿæ•°æ®çš„è§‚æµ‹åå·®ç‰¹å¾")
    
    return evaluator, strategy_results

if __name__ == "__main__":
    evaluator, results = main()