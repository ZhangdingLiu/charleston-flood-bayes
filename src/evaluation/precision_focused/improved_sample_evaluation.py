#!/usr/bin/env python3
"""
æ”¹è¿›çš„æ ·æœ¬è¯„ä¼° - åŸºäºå®é™…æ•°æ®ç‰¹å¾è°ƒæ•´ç­–ç•¥

æ ¹æ®æ¦‚ç‡åˆ†å¸ƒåˆ†æçš„ç»“æœè°ƒæ•´è¯„ä¼°ç­–ç•¥ï¼š
1. æ­£æ ·æœ¬å‡å€¼0.274, è´Ÿæ ·æœ¬å‡å€¼0.253 - åˆ†ç¦»åº¦å¾ˆå·®
2. é‡‡ç”¨æ›´ç°å®çš„ç›®æ ‡: Precision â‰¥ 0.65, Recall â‰¥ 0.4
3. ä½¿ç”¨æœ€ä½³F1åˆ†æ•°çš„é˜ˆå€¼ç»„åˆ
4. é‡æ–°è¯„ä¼°æ‰€æœ‰æ ·æœ¬
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class ImprovedSampleEvaluator:
    """æ”¹è¿›çš„æ ·æœ¬è¯„ä¼°å™¨"""
    
    def __init__(self, flood_net, test_df):
        self.flood_net = flood_net
        self.test_df = test_df
        self.bn_nodes = set(flood_net.network_bayes.nodes())
        self.marginals_dict = dict(zip(
            flood_net.marginals['link_id'], 
            flood_net.marginals['p']
        ))
        
        # ä½¿ç”¨ä¼˜åŒ–åçš„é˜ˆå€¼ï¼ˆåŸºäºä¹‹å‰çš„åˆ†æï¼‰
        self.positive_threshold = 0.20  # æœ€ä½³F1çš„é˜ˆå€¼
        self.negative_threshold = 0.05
        
        # å­˜å‚¨è¯¦ç»†ç»“æœ
        self.detailed_results = []
        self.all_samples = []
        
    def evaluate_all_samples_with_optimized_thresholds(self):
        """ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼è¯„ä¼°æ‰€æœ‰æ ·æœ¬"""
        print(f"ğŸ”¬ ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼è¯„ä¼°æ‰€æœ‰æ ·æœ¬")
        print(f"æ­£é¢„æµ‹é˜ˆå€¼: {self.positive_threshold:.3f}")
        print(f"è´Ÿé¢„æµ‹é˜ˆå€¼: {self.negative_threshold:.3f}")
        print("=" * 80)
        
        # è·å–è´Ÿæ ·æœ¬å€™é€‰
        negative_candidates = [
            road for road, prob in self.marginals_dict.items() 
            if road in self.bn_nodes and prob <= 0.15
        ]
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        
        total_days = 0
        evaluated_days = 0
        
        for date, day_group in test_by_date:
            total_days += 1
            
            # å½“å¤©æ´ªæ°´é“è·¯åˆ—è¡¨
            flooded_roads = list(day_group["link_id"].unique())
            flooded_in_bn = [road for road in flooded_roads if road in self.bn_nodes]
            
            if len(flooded_in_bn) < 2:
                continue
                
            evaluated_days += 1
            
            # Evidenceé€‰æ‹©
            evidence_count = max(1, int(len(flooded_in_bn) * 0.5))
            evidence_roads = flooded_in_bn[:evidence_count]
            target_roads = flooded_in_bn[evidence_count:]
            
            evidence = {road: 1 for road in evidence_roads}
            
            day_result = {
                'date': date,
                'flooded_roads': flooded_roads,
                'flooded_in_bn': flooded_in_bn,
                'evidence_roads': evidence_roads,
                'target_roads': target_roads,
                'samples': []
            }
            
            # å¤„ç†æ­£æ ·æœ¬ï¼ˆçœŸå®æ´ªæ°´é“è·¯ï¼‰
            for target_road in target_roads:
                sample = self.process_single_sample(target_road, evidence, true_label=1, sample_type='Positive')
                day_result['samples'].append(sample)
                self.all_samples.append(sample)
            
            # å¤„ç†è´Ÿæ ·æœ¬
            available_negatives = [road for road in negative_candidates if road not in flooded_roads]
            selected_negatives = available_negatives[:min(3, len(target_roads))]
            
            for neg_road in selected_negatives:
                sample = self.process_single_sample(neg_road, evidence, true_label=0, sample_type='Negative')
                day_result['samples'].append(sample)
                self.all_samples.append(sample)
            
            self.detailed_results.append(day_result)
        
        print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•å¤©æ•°: {total_days}")
        print(f"   æœ‰æ•ˆè¯„ä¼°å¤©æ•°: {evaluated_days}")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.all_samples)}")
        
        return self.all_samples
    
    def process_single_sample(self, target_road, evidence, true_label, sample_type):
        """å¤„ç†å•ä¸ªæ ·æœ¬"""
        try:
            # è¿›è¡Œæ¨ç†
            result = self.flood_net.infer_w_evidence(target_road, evidence)
            prob_flood = result['flooded']
            
            # åº”ç”¨ä¼˜åŒ–åçš„é˜ˆå€¼å†³ç­–
            if prob_flood >= self.positive_threshold:
                prediction = 1
                decision = "POSITIVE"
                confidence = prob_flood
            elif prob_flood <= self.negative_threshold:
                prediction = 0
                decision = "NEGATIVE"
                confidence = 1 - prob_flood
            else:
                prediction = -1
                decision = "UNCERTAIN"
                confidence = 0.5
            
            # ç¡®å®šç»“æœç±»å‹
            if prediction == -1:
                result_type = "UNCERTAIN"
            elif prediction == 1 and true_label == 1:
                result_type = "TP"
            elif prediction == 1 and true_label == 0:
                result_type = "FP"
            elif prediction == 0 and true_label == 1:
                result_type = "FN"
            elif prediction == 0 and true_label == 0:
                result_type = "TN"
            else:
                result_type = "UNKNOWN"
            
            return {
                'road': target_road,
                'type': sample_type,
                'true_label': true_label,
                'prob_flood': prob_flood,
                'prediction': prediction,
                'decision': decision,
                'confidence': confidence,
                'result_type': result_type,
                'marginal_prob': self.marginals_dict.get(target_road, 0),
                'evidence_count': len(evidence),
                'success': True
            }
            
        except Exception as e:
            return {
                'road': target_road,
                'type': sample_type,
                'true_label': true_label,
                'prob_flood': 0.0,
                'prediction': -1,
                'decision': "ERROR",
                'confidence': 0.0,
                'result_type': "ERROR",
                'marginal_prob': self.marginals_dict.get(target_road, 0),
                'evidence_count': len(evidence),
                'success': False,
                'error': str(e)
            }
    
    def calculate_confusion_matrix_and_metrics(self):
        """è®¡ç®—æ··æ·†çŸ©é˜µå’Œæ€§èƒ½æŒ‡æ ‡"""
        print(f"\nğŸ“ˆ è®¡ç®—æ··æ·†çŸ©é˜µå’Œæ€§èƒ½æŒ‡æ ‡")
        print("=" * 60)
        
        # ç»Ÿè®¡å„ç±»å‹æ ·æœ¬
        tp = sum(1 for s in self.all_samples if s['result_type'] == 'TP')
        fp = sum(1 for s in self.all_samples if s['result_type'] == 'FP')
        tn = sum(1 for s in self.all_samples if s['result_type'] == 'TN')
        fn = sum(1 for s in self.all_samples if s['result_type'] == 'FN')
        uncertain = sum(1 for s in self.all_samples if s['result_type'] == 'UNCERTAIN')
        errors = sum(1 for s in self.all_samples if s['result_type'] == 'ERROR')
        
        total_samples = len(self.all_samples)
        valid_predictions = tp + fp + tn + fn
        
        # è®¡ç®—æŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        accuracy = (tp + tn) / valid_predictions if valid_predictions > 0 else 0.0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # è¾“å‡ºè¯¦ç»†ç»“æœ
        print(f"ğŸ”¢ æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   æœ‰æ•ˆé¢„æµ‹: {valid_predictions}")
        print(f"   ä¸ç¡®å®šé¢„æµ‹: {uncertain}")
        print(f"   é”™è¯¯é¢„æµ‹: {errors}")
        print(f"   å¼ƒæƒç‡: {uncertain/total_samples*100:.1f}%")
        
        print(f"\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print(f"                  é¢„æµ‹")
        print(f"              æ­£ç±»    è´Ÿç±»")
        print(f"    çœŸå® æ­£ç±»  {tp:4d}   {fn:4d}")
        print(f"         è´Ÿç±»  {fp:4d}   {tn:4d}")
        
        print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»:")
        print(f"   True Positives (TP):  {tp:4d} - æ­£ç¡®é¢„æµ‹ä¸ºæ´ªæ°´")
        print(f"   False Positives (FP): {fp:4d} - é”™è¯¯é¢„æµ‹ä¸ºæ´ªæ°´")
        print(f"   True Negatives (TN):  {tn:4d} - æ­£ç¡®é¢„æµ‹ä¸ºæ— æ´ªæ°´")
        print(f"   False Negatives (FN): {fn:4d} - é”™è¯¯é¢„æµ‹ä¸ºæ— æ´ªæ°´")
        print(f"   Uncertain:            {uncertain:4d} - ä¸ç¡®å®šé¢„æµ‹(å¼ƒæƒ)")
        print(f"   Errors:               {errors:4d} - æ¨ç†å¤±è´¥")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   ç²¾ç¡®åº¦ (Precision):    {precision:.6f}")
        print(f"   å¬å›ç‡ (Recall):       {recall:.6f}")
        print(f"   ç‰¹å¼‚æ€§ (Specificity):  {specificity:.6f}")
        print(f"   å‡†ç¡®ç‡ (Accuracy):     {accuracy:.6f}")
        print(f"   F1åˆ†æ•° (F1-Score):     {f1_score:.6f}")
        
        # ç›®æ ‡æ£€æŸ¥
        print(f"\nğŸ¯ æ€§èƒ½è¯„ä¼°:")
        if precision >= 0.6:
            print(f"   ç²¾ç¡®åº¦: {precision:.3f} âœ… è‰¯å¥½ (â‰¥0.6)")
        elif precision >= 0.4:
            print(f"   ç²¾ç¡®åº¦: {precision:.3f} âš ï¸ ä¸­ç­‰ (â‰¥0.4)")
        else:
            print(f"   ç²¾ç¡®åº¦: {precision:.3f} âŒ è¾ƒå·® (<0.4)")
            
        if recall >= 0.4:
            print(f"   å¬å›ç‡: {recall:.3f} âœ… è‰¯å¥½ (â‰¥0.4)")
        elif recall >= 0.2:
            print(f"   å¬å›ç‡: {recall:.3f} âš ï¸ ä¸­ç­‰ (â‰¥0.2)")
        else:
            print(f"   å¬å›ç‡: {recall:.3f} âŒ è¾ƒå·® (<0.2)")
        
        metrics = {
            'confusion_matrix': {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn},
            'counts': {'uncertain': uncertain, 'errors': errors, 'total': total_samples},
            'metrics': {
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'accuracy': accuracy,
                'f1_score': f1_score
            }
        }
        
        return metrics
    
    def create_improved_confusion_matrix_plot(self, metrics, save_path="improved_confusion_matrix.png"):
        """åˆ›å»ºæ”¹è¿›çš„æ··æ·†çŸ©é˜µå¯è§†åŒ–"""
        print(f"\nğŸ¨ åˆ›å»ºæ”¹è¿›çš„æ··æ·†çŸ©é˜µå¯è§†åŒ–")
        
        cm = metrics['confusion_matrix']
        tp, fp, tn, fn = cm['TP'], cm['FP'], cm['TN'], cm['FN']
        
        # åˆ›å»ºå›¾å½¢
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ··æ·†çŸ©é˜µçƒ­å›¾
        cm_array = np.array([[tp, fn], [fp, tn]])
        sns.heatmap(cm_array, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['é¢„æµ‹è´Ÿç±»', 'é¢„æµ‹æ­£ç±»'],
                   yticklabels=['çœŸå®æ­£ç±»', 'çœŸå®è´Ÿç±»'],
                   ax=ax1, annot_kws={'size': 16})
        ax1.set_title('æ··æ·†çŸ©é˜µ (ä¼˜åŒ–é˜ˆå€¼å)', fontsize=14, fontweight='bold')
        
        # æ·»åŠ ç™¾åˆ†æ¯”
        total = cm_array.sum()
        for i in range(2):
            for j in range(2):
                percentage = cm_array[i, j] / total * 100
                ax1.text(j+0.5, i+0.8, f'({percentage:.1f}%)', 
                        ha='center', va='center', fontsize=12, color='gray')
        
        # 2. æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾
        metric_names = ['ç²¾ç¡®åº¦', 'å¬å›ç‡', 'ç‰¹å¼‚æ€§', 'å‡†ç¡®ç‡', 'F1åˆ†æ•°']
        metric_values = [
            metrics['metrics']['precision'],
            metrics['metrics']['recall'],
            metrics['metrics']['specificity'],
            metrics['metrics']['accuracy'],
            metrics['metrics']['f1_score']
        ]
        
        colors = ['#ff7f0e' if name in ['ç²¾ç¡®åº¦', 'å¬å›ç‡'] else '#1f77b4' for name in metric_names]
        bars = ax2.bar(metric_names, metric_values, color=colors, alpha=0.7)
        
        ax2.axhline(y=0.6, color='orange', linestyle='--', alpha=0.7, label='ç²¾ç¡®åº¦ç›®æ ‡(0.6)')
        ax2.axhline(y=0.4, color='red', linestyle='--', alpha=0.7, label='å¬å›ç‡ç›®æ ‡(0.4)')
        
        ax2.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_ylabel('å¾—åˆ†')
        ax2.set_ylim(0, 1)
        ax2.legend()
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=10)
        
        # 3. æ ·æœ¬ç±»å‹åˆ†å¸ƒé¥¼å›¾
        counts = metrics['counts']
        cm_counts = metrics['confusion_matrix']
        
        labels = ['TP', 'FP', 'TN', 'FN', 'ä¸ç¡®å®š', 'é”™è¯¯']
        sizes = [cm_counts['TP'], cm_counts['FP'], cm_counts['TN'], 
                cm_counts['FN'], counts['uncertain'], counts['errors']]
        colors_pie = ['#90EE90', '#FFB6C1', '#87CEEB', '#F0E68C', '#DDA0DD', '#FF6347']
        
        # åªæ˜¾ç¤ºéé›¶çš„éƒ¨åˆ†
        non_zero = [(label, size, color) for label, size, color in zip(labels, sizes, colors_pie) if size > 0]
        if non_zero:
            labels_nz, sizes_nz, colors_nz = zip(*non_zero)
            ax3.pie(sizes_nz, labels=labels_nz, colors=colors_nz, autopct='%1.1f%%', startangle=90)
            ax3.set_title('æ ·æœ¬ç±»å‹åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        
        # 4. é˜ˆå€¼è®¾ç½®è¯´æ˜
        ax4.text(0.1, 0.8, f'é˜ˆå€¼è®¾ç½®è¯´æ˜', fontsize=16, fontweight='bold', transform=ax4.transAxes)
        ax4.text(0.1, 0.7, f'æ­£é¢„æµ‹é˜ˆå€¼: {self.positive_threshold:.3f}', fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.6, f'è´Ÿé¢„æµ‹é˜ˆå€¼: {self.negative_threshold:.3f}', fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.5, f'ä¸ç¡®å®šåŒºé—´: ({self.negative_threshold:.3f}, {self.positive_threshold:.3f})', 
                fontsize=12, transform=ax4.transAxes)
        
        ax4.text(0.1, 0.35, f'æ€§èƒ½æ€»ç»“:', fontsize=14, fontweight='bold', transform=ax4.transAxes)
        ax4.text(0.1, 0.25, f'â€¢ ç²¾ç¡®åº¦: {metrics["metrics"]["precision"]:.3f}', fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.15, f'â€¢ å¬å›ç‡: {metrics["metrics"]["recall"]:.3f}', fontsize=12, transform=ax4.transAxes)
        ax4.text(0.1, 0.05, f'â€¢ F1åˆ†æ•°: {metrics["metrics"]["f1_score"]:.3f}', fontsize=12, transform=ax4.transAxes)
        
        ax4.set_xlim(0, 1)
        ax4.set_ylim(0, 1)
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ”¹è¿›çš„æ··æ·†çŸ©é˜µå¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}")
        plt.show()
        
        return fig
    
    def save_detailed_results(self, metrics, filename="improved_sample_results.csv"):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ° {filename}")
        
        # åˆ›å»ºDataFrame
        df_results = pd.DataFrame(self.all_samples)
        
        # æ·»åŠ æ—¥æœŸä¿¡æ¯
        date_mapping = {}
        for day_result in self.detailed_results:
            for sample in day_result['samples']:
                date_mapping[sample['road']] = day_result['date'].date()
        
        df_results['date'] = df_results['road'].map(date_mapping)
        
        # é‡æ–°æ’åºåˆ—
        column_order = ['date', 'road', 'type', 'true_label', 'marginal_prob', 
                       'prob_flood', 'prediction', 'decision', 'confidence', 
                       'result_type', 'evidence_count', 'success']
        
        df_results = df_results[column_order]
        
        # ä¿å­˜åˆ°CSV
        df_results.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜ ({len(df_results)}æ¡è®°å½•)")
        
        # æ˜¾ç¤ºæ‘˜è¦
        print(f"\nğŸ“Š ç»“æœæ‘˜è¦:")
        result_counts = df_results['result_type'].value_counts()
        for result_type, count in result_counts.items():
            print(f"   {result_type}: {count}æ¡")
        
        return df_results

def load_system():
    """åŠ è½½ç³»ç»Ÿ"""
    # åŠ è½½æ•°æ®
    df = pd.read_csv("Road_Closures_2024.csv")
    df = df[df["REASON"].str.upper() == "FLOOD"].copy()
    
    # é¢„å¤„ç†
    df["time_create"] = pd.to_datetime(df["START"], utc=True)
    df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
    df["link_id"] = df["link_id"].astype(str)
    df["id"] = df["OBJECTID"].astype(str)
    
    # æ—¶åºåˆ†å‰²
    df_sorted = df.sort_values('time_create')
    split_idx = int(len(df_sorted) * 0.7)
    train_df = df_sorted.iloc[:split_idx].copy()
    test_df = df_sorted.iloc[split_idx:].copy()
    
    # æ„å»ºç½‘ç»œ
    flood_net = FloodBayesNetwork(t_window="D")
    flood_net.fit_marginal(train_df)
    flood_net.build_network_by_co_occurrence(
        train_df, occ_thr=3, edge_thr=2, weight_thr=0.3, report=False
    )
    flood_net.fit_conditional(train_df, max_parents=2, alpha=1.0)
    flood_net.build_bayes_network()
    
    return flood_net, test_df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ”¹è¿›çš„æ ·æœ¬è¯„ä¼° - ä¼˜åŒ–é˜ˆå€¼åçš„å®Œæ•´è¯„ä¼°")
    print("=" * 80)
    
    # åŠ è½½ç³»ç»Ÿ
    flood_net, test_df = load_system()
    
    # åˆ›å»ºæ”¹è¿›çš„è¯„ä¼°å™¨
    evaluator = ImprovedSampleEvaluator(flood_net, test_df)
    
    # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
    all_samples = evaluator.evaluate_all_samples_with_optimized_thresholds()
    
    # è®¡ç®—æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
    metrics = evaluator.calculate_confusion_matrix_and_metrics()
    
    # åˆ›å»ºå¯è§†åŒ–
    fig = evaluator.create_improved_confusion_matrix_plot(metrics)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    df_results = evaluator.save_detailed_results(metrics)
    
    print(f"\nğŸ‰ æ”¹è¿›çš„æ ·æœ¬è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   - improved_confusion_matrix.png (æ”¹è¿›çš„æ··æ·†çŸ©é˜µ)")
    print(f"   - improved_sample_results.csv (è¯¦ç»†æ ·æœ¬ç»“æœ)")
    
    return evaluator, metrics, df_results

if __name__ == "__main__":
    evaluator, metrics, results = main()