#!/usr/bin/env python3
"""
å¿«é€Ÿæ”¹è¿›è¯„ä¼° - ä¸å«å¯è§†åŒ–
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

def print_detailed_results(all_samples):
    """æ‰“å°æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†é¢„æµ‹è¿‡ç¨‹"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ è¯¦ç»†æ ·æœ¬é¢„æµ‹ç»“æœ")
    print("=" * 80)
    
    for i, sample in enumerate(all_samples, 1):
        sample_type = sample['type']
        evidence_roads = ', '.join(sample['evidence_roads'])
        target_road = sample['target_road']
        prob_flood = sample['prob_flood']
        prediction = sample['prediction']
        true_label = sample['true_label']
        is_correct = sample['is_correct']
        date = sample['date']
        
        # é¢„æµ‹ç»“æœæ–‡å­—æè¿°
        if prediction == 1:
            pred_text = "æ´ªæ°´ (1)"
        elif prediction == 0:
            pred_text = "æ— æ´ªæ°´ (0)"
        else:
            pred_text = "ä¸ç¡®å®š (-1)"
        
        # çœŸå®æ ‡ç­¾æ–‡å­—æè¿°
        true_text = "æ´ªæ°´ (1)" if true_label == 1 else "æ— æ´ªæ°´ (0)"
        
        # æ­£ç¡®æ€§æ ‡è®°
        if prediction == -1:
            correctness = "âš ï¸ ä¸ç¡®å®š"
        elif is_correct:
            correctness = "âœ… æ­£ç¡®"
        else:
            correctness = "âŒ é”™è¯¯"
        
        print(f"\næ ·æœ¬ #{i} [{sample_type}] æ—¥æœŸ: {date}")
        print(f"  Evidenceé“è·¯: [{evidence_roads}]")
        print(f"  ç›®æ ‡é“è·¯: {target_road}")
        print(f"  é¢„æµ‹æ¦‚ç‡: {prob_flood:.3f} â†’ é¢„æµ‹: {pred_text}")
        print(f"  çœŸå®æ ‡ç­¾: {true_text} â†’ {correctness}")

def quick_evaluation():
    """å¿«é€Ÿè¯„ä¼°"""
    print("ğŸ¯ å¿«é€Ÿæ”¹è¿›è¯„ä¼°")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv("Road_Closures_2024.csv")
    df = df[df["REASON"].str.upper() == "FLOOD"].copy()
    
    # é¢„å¤„ç†
    df["time_create"] = pd.to_datetime(df["START"], utc=True)
    df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
    df["link_id"] = df["link_id"].astype(str)
    df["id"] = df["OBJECTID"].astype(str)
    
    # æ—¶åºåˆ†å‰²
    df_sorted = df.sort_values('time_create')
    split_idx = int(len(df_sorted) * 0.7)
    train_df = df_sorted.iloc[:split_idx].copy()
    test_df = df_sorted.iloc[split_idx:].copy()
    
    # æ„å»ºç½‘ç»œ
    flood_net = FloodBayesNetwork(t_window="D")
    flood_net.fit_marginal(train_df)
    flood_net.build_network_by_co_occurrence(
        train_df, occ_thr=3, edge_thr=2, weight_thr=0.3, report=False
    )
    flood_net.fit_conditional(train_df, max_parents=2, alpha=1.0)
    flood_net.build_bayes_network()
    
    print("âœ… ç½‘ç»œæ„å»ºå®Œæˆ")
    
    # è·å–ç½‘ç»œä¿¡æ¯
    bn_nodes = set(flood_net.network_bayes.nodes())
    marginals_dict = dict(zip(flood_net.marginals['link_id'], flood_net.marginals['p']))
    
    # ä½¿ç”¨ä¼˜åŒ–åçš„é˜ˆå€¼
    positive_threshold = 0.20
    negative_threshold = 0.05
    
    print(f"ğŸ¯ ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼: æ­£é¢„æµ‹={positive_threshold:.3f}, è´Ÿé¢„æµ‹={negative_threshold:.3f}")
    
    # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
    all_samples = []
    negative_candidates = [road for road, prob in marginals_dict.items() if road in bn_nodes and prob <= 0.15]
    
    test_by_date = test_df.groupby(test_df["time_create"].dt.floor("D"))
    evaluated_days = 0
    
    for date, day_group in test_by_date:
        flooded_roads = list(day_group["link_id"].unique())
        flooded_in_bn = [road for road in flooded_roads if road in bn_nodes]
        
        if len(flooded_in_bn) < 2:
            continue
            
        evaluated_days += 1
        
        # Evidenceé€‰æ‹©
        evidence_count = max(1, int(len(flooded_in_bn) * 0.5))
        evidence_roads = flooded_in_bn[:evidence_count]
        target_roads = flooded_in_bn[evidence_count:]
        evidence = {road: 1 for road in evidence_roads}
        
        # å¤„ç†æ­£æ ·æœ¬
        for target_road in target_roads:
            try:
                result = flood_net.infer_w_evidence(target_road, evidence)
                prob_flood = result['flooded']
                
                if prob_flood >= positive_threshold:
                    prediction = 1
                elif prob_flood <= negative_threshold:
                    prediction = 0
                else:
                    prediction = -1
                
                # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
                is_correct = (prediction == 1)  # æ­£æ ·æœ¬çœŸå®æ ‡ç­¾æ˜¯1
                
                all_samples.append({
                    'type': 'Positive',
                    'true_label': 1,
                    'prob_flood': prob_flood,
                    'prediction': prediction,
                    'evidence_roads': list(evidence.keys()),
                    'target_road': target_road,
                    'date': date.strftime('%Y-%m-%d'),
                    'is_correct': is_correct
                })
            except:
                continue
        
        # å¤„ç†è´Ÿæ ·æœ¬
        available_negatives = [road for road in negative_candidates if road not in flooded_roads]
        selected_negatives = available_negatives[:min(3, len(target_roads))]
        
        for neg_road in selected_negatives:
            try:
                result = flood_net.infer_w_evidence(neg_road, evidence)
                prob_flood = result['flooded']
                
                if prob_flood >= positive_threshold:
                    prediction = 1
                elif prob_flood <= negative_threshold:
                    prediction = 0
                else:
                    prediction = -1
                
                # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
                is_correct = (prediction == 0)  # è´Ÿæ ·æœ¬çœŸå®æ ‡ç­¾æ˜¯0
                
                all_samples.append({
                    'type': 'Negative',
                    'true_label': 0,
                    'prob_flood': prob_flood,
                    'prediction': prediction,
                    'evidence_roads': list(evidence.keys()),
                    'target_road': neg_road,
                    'date': date.strftime('%Y-%m-%d'),
                    'is_correct': is_correct
                })
            except:
                continue
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    tp = fp = tn = fn = uncertain = 0
    
    for sample in all_samples:
        pred = sample['prediction']
        true = sample['true_label']
        
        if pred == -1:
            uncertain += 1
        elif pred == 1 and true == 1:
            tp += 1
        elif pred == 1 and true == 0:
            fp += 1
        elif pred == 0 and true == 1:
            fn += 1
        elif pred == 0 and true == 0:
            tn += 1
    
    # è®¡ç®—æŒ‡æ ‡
    total_samples = len(all_samples)
    valid_predictions = tp + fp + tn + fn
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    accuracy = (tp + tn) / valid_predictions if valid_predictions > 0 else 0.0
    
    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   è¯„ä¼°å¤©æ•°: {evaluated_days}")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   æœ‰æ•ˆé¢„æµ‹: {valid_predictions}")
    print(f"   ä¸ç¡®å®šé¢„æµ‹: {uncertain}")
    
    print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µ:")
    print(f"                  é¢„æµ‹")
    print(f"              æ­£ç±»    è´Ÿç±»")
    print(f"    çœŸå® æ­£ç±»  {tp:4d}   {fn:4d}")
    print(f"         è´Ÿç±»  {fp:4d}   {tn:4d}")
    
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»:")
    print(f"   TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}, ä¸ç¡®å®š: {uncertain}")
    
    print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"   ç²¾ç¡®åº¦ (Precision): {precision:.6f}")
    print(f"   å¬å›ç‡ (Recall):    {recall:.6f}")
    print(f"   F1åˆ†æ•° (F1-Score): {f1_score:.6f}")
    print(f"   å‡†ç¡®ç‡ (Accuracy):  {accuracy:.6f}")
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print_detailed_results(all_samples)
    
    # ä¿å­˜è¯¦ç»†çš„ç»“æœ
    df_detailed = pd.DataFrame(all_samples)
    df_detailed.to_csv("detailed_evaluation_results.csv", index=False)
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° detailed_evaluation_results.csv")
    
    return {
        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn, 'uncertain': uncertain,
        'precision': precision, 'recall': recall, 'f1_score': f1_score
    }

if __name__ == "__main__":
    results = quick_evaluation()