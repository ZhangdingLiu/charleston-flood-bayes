#!/usr/bin/env python3
"""
æ”¹è¿›çš„è´å¶æ–¯ç½‘ç»œè®­ç»ƒå’Œè¯„ä¼°ç³»ç»Ÿ

ä¸»è¦æ”¹è¿›ï¼š
1. åŸºäºè®­ç»ƒæ•°æ®æ„å»ºè´å¶æ–¯ç½‘ç»œï¼Œé¿å…æ•°æ®æ³„éœ²
2. å®ç°ç‰¹æ®Šçš„è¯„ä¼°ç­–ç•¥ï¼šåªè€ƒè™‘æœ‰æ´ªæ°´è®°å½•çš„é“è·¯è¿›è¡Œæ¨ç†
3. ç³»ç»Ÿçš„å‚æ•°ä¼˜åŒ–å’Œç½‘æ ¼æœç´¢
4. è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œç»“æœè¾“å‡º

è¯„ä¼°ç­–ç•¥è¯´æ˜ï¼š
- æµ‹è¯•æ—¶åªè€ƒè™‘æœ‰æ´ªæ°´è®°å½•(=1)çš„é“è·¯ï¼Œå› ä¸ºè¿™äº›æ˜¯å¯é çš„æ­£æ ·æœ¬
- å¿½ç•¥æ— è®°å½•(=0)çš„æƒ…å†µï¼Œå› ä¸ºå¯èƒ½æ˜¯è§‚æµ‹ç¼ºå¤±è€ŒéçœŸå®æ— æ´ªæ°´
- æ¨ç†ç»“æœæ¦‚ç‡â‰¥é˜ˆå€¼æ‰ç®—positive prediction
- è®¡ç®—precisionå’Œrecallåœ¨è¿™ç§è®¾å®šä¸‹
"""

import random
import numpy as np
import pandas as pd
import os
import json
from datetime import datetime
from collections import defaultdict
from itertools import product

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

from model import FloodBayesNetwork
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score

class ImprovedFloodPredictor:
    """æ”¹è¿›çš„æ´ªæ°´é¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, data_path="Road_Closures_2024.csv"):
        self.data_path = data_path
        self.train_df = None
        self.test_df = None
        
        # å‚æ•°æœç´¢ç©ºé—´
        self.param_grid = {
            'occ_thr': [2, 3, 5, 10],           # é“è·¯å‡ºç°æ¬¡æ•°é˜ˆå€¼
            'edge_thr': [2, 3, 4],              # å…±ç°æ¬¡æ•°é˜ˆå€¼  
            'weight_thr': [0.2, 0.3, 0.4, 0.5], # æ¡ä»¶æ¦‚ç‡é˜ˆå€¼
            'max_parents': [1, 2, 3],           # æœ€å¤§çˆ¶èŠ‚ç‚¹æ•°
            'alpha': [0.5, 1.0, 2.0],           # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
            'prob_thr': [0.3, 0.4, 0.5, 0.6, 0.7] # æ¨ç†æ¦‚ç‡é˜ˆå€¼
        }
        
        # å­˜å‚¨ç»“æœ
        self.results = []
        self.best_config = None
        self.best_network = None
        
    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print("ğŸš€ å¯åŠ¨æ”¹è¿›çš„è´å¶æ–¯ç½‘ç»œæ´ªæ°´é¢„æµ‹ç³»ç»Ÿ")
        print("="*60)
        print("1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_path)
        df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        
        # æ•°æ®é¢„å¤„ç†
        df["time_create"] = pd.to_datetime(df["START"], utc=True)
        df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
        df["link_id"] = df["link_id"].astype(str)
        df["id"] = df["OBJECTID"].astype(str)
        
        # ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²ï¼Œé¿å…æ•°æ®æ³„éœ²
        df_sorted = df.sort_values('time_create')
        split_idx = int(len(df_sorted) * 0.7)
        self.train_df = df_sorted.iloc[:split_idx].copy()
        self.test_df = df_sorted.iloc[split_idx:].copy()
        
        print(f"   æ€»æ´ªæ°´è®°å½•: {len(df)}æ¡")
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡ (æ—¶é—´: {self.train_df['time_create'].min()} è‡³ {self.train_df['time_create'].max()})")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡ (æ—¶é—´: {self.test_df['time_create'].min()} è‡³ {self.test_df['time_create'].max()})")
        
        # åˆ†ææ•°æ®ç‰¹å¾
        train_roads = set(self.train_df['link_id'].unique())
        test_roads = set(self.test_df['link_id'].unique())
        overlap_roads = train_roads & test_roads
        
        print(f"   è®­ç»ƒé›†ç‹¬ç‰¹é“è·¯: {len(train_roads)}æ¡")
        print(f"   æµ‹è¯•é›†ç‹¬ç‰¹é“è·¯: {len(test_roads)}æ¡")
        print(f"   é‡å é“è·¯: {len(overlap_roads)}æ¡")
        
        return True
        
    def evaluate_flood_only(self, flood_net, evidence_ratio=0.5, prob_thr=0.5, verbose=False):
        """
        åªå¯¹æœ‰æ´ªæ°´è®°å½•çš„é“è·¯è¿›è¡Œæ¨ç†å’Œè¯„ä¼°
        
        Args:
            flood_net: è®­ç»ƒå¥½çš„è´å¶æ–¯ç½‘ç»œ
            evidence_ratio: ç”¨ä½œevidenceçš„æ´ªæ°´é“è·¯æ¯”ä¾‹
            prob_thr: é¢„æµ‹æ¦‚ç‡é˜ˆå€¼
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«precision, recall, f1ç­‰æŒ‡æ ‡çš„å­—å…¸
        """
        bn_nodes = set(flood_net.network_bayes.nodes()) if flood_net.network_bayes else set()
        
        if len(bn_nodes) == 0:
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'samples': 0, 'valid_days': 0}
        
        all_predictions = []
        all_true_labels = []
        evaluated_days = 0
        total_samples = 0\n        \n        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•æ•°æ®\n        test_by_date = self.test_df.groupby(self.test_df[\"time_create\"].dt.floor(\"D\"))\n        \n        for date, day_group in test_by_date:\n            # å½“å¤©æ´ªæ°´é“è·¯åˆ—è¡¨\n            flooded_roads = list(day_group[\"link_id\"].unique())\n            \n            # åªè€ƒè™‘åœ¨è´å¶æ–¯ç½‘ç»œä¸­çš„é“è·¯\n            flooded_in_bn = [road for road in flooded_roads if road in bn_nodes]\n            \n            if len(flooded_in_bn) < 2:\n                continue  # éœ€è¦è‡³å°‘2æ¡é“è·¯æ‰èƒ½åšæ¨ç†\n            \n            evaluated_days += 1\n            \n            # é€‰æ‹©evidenceé“è·¯ï¼ˆå‰Nä¸ªæˆ–éšæœºé€‰æ‹©ï¼‰\n            evidence_count = max(1, int(len(flooded_in_bn) * evidence_ratio))\n            evidence_roads = flooded_in_bn[:evidence_count]\n            target_roads = flooded_in_bn[evidence_count:]\n            \n            if len(target_roads) == 0:\n                continue\n            \n            # æ„å»ºevidenceå­—å…¸\n            evidence = {road: 1 for road in evidence_roads}\n            \n            if verbose and evaluated_days <= 3:\n                print(f\"     ğŸ“… {date.date()}: æ´ªæ°´é“è·¯{len(flooded_in_bn)}, evidence{len(evidence_roads)}, target{len(target_roads)}\")\n            \n            # å¯¹æ¯ä¸ªç›®æ ‡é“è·¯è¿›è¡Œæ¨ç†\n            for target_road in target_roads:\n                try:\n                    # è´å¶æ–¯æ¨ç†\n                    result = flood_net.infer_w_evidence(target_road, evidence)\n                    prob_flood = result['flooded']\n                    \n                    # é¢„æµ‹æ ‡ç­¾ï¼ˆæ ¹æ®æ¦‚ç‡é˜ˆå€¼ï¼‰\n                    pred_label = 1 if prob_flood >= prob_thr else 0\n                    true_label = 1  # ç›®æ ‡é“è·¯ç¡®å®å‘ç”Ÿäº†æ´ªæ°´\n                    \n                    all_predictions.append(pred_label)\n                    all_true_labels.append(true_label)\n                    total_samples += 1\n                    \n                    if verbose and evaluated_days <= 3:\n                        print(f\"       {target_road}: P(flood)={prob_flood:.3f}, pred={pred_label}, true={true_label}\")\n                        \n                except Exception as e:\n                    if verbose:\n                        print(f\"       {target_road}: æ¨ç†å¤±è´¥ - {e}\")\n                    continue\n        \n        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡\n        if total_samples == 0:\n            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'samples': 0, 'valid_days': 0}\n        \n        # åœ¨è¿™ç§ç‰¹æ®Šè®¾å®šä¸‹ï¼Œæ‰€æœ‰true_labeléƒ½æ˜¯1ï¼Œæ‰€ä»¥ï¼š\n        # - TP = é¢„æµ‹ä¸º1çš„æ•°é‡\n        # - FN = é¢„æµ‹ä¸º0çš„æ•°é‡  \n        # - TN = FP = 0ï¼ˆå› ä¸ºæ²¡æœ‰çœŸå®çš„è´Ÿæ ·æœ¬ï¼‰\n        \n        tp = sum(all_predictions)  # é¢„æµ‹ä¸ºæ­£çš„æ•°é‡\n        fn = len(all_predictions) - tp  # é¢„æµ‹ä¸ºè´Ÿçš„æ•°é‡\n        \n        precision = tp / (tp) if tp > 0 else 0.0  # TP / (TP + FP), ä½†FP=0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # TP / (TP + FN)\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n        \n        return {\n            'precision': precision,\n            'recall': recall, \n            'f1': f1,\n            'samples': total_samples,\n            'valid_days': evaluated_days,\n            'tp': tp,\n            'fn': fn\n        }\n        \n    def build_and_evaluate_network(self, occ_thr, edge_thr, weight_thr, max_parents, alpha, prob_thr, verbose=False):\n        \"\"\"æ„å»ºå’Œè¯„ä¼°å•ä¸ªç½‘ç»œé…ç½®\"\"\"\n        try:\n            # æ„å»ºç½‘ç»œ\n            flood_net = FloodBayesNetwork(t_window=\"D\")\n            flood_net.fit_marginal(self.train_df)\n            \n            # æ„å»ºå…±ç°ç½‘ç»œ\n            flood_net.build_network_by_co_occurrence(\n                self.train_df,\n                occ_thr=occ_thr,\n                edge_thr=edge_thr,\n                weight_thr=weight_thr,\n                report=False\n            )\n            \n            # å¦‚æœç½‘ç»œä¸ºç©ºï¼Œè¿”å›é»˜è®¤ç»“æœ\n            if flood_net.network.number_of_nodes() == 0:\n                return {\n                    'occ_thr': occ_thr, 'edge_thr': edge_thr, 'weight_thr': weight_thr,\n                    'max_parents': max_parents, 'alpha': alpha, 'prob_thr': prob_thr,\n                    'nodes': 0, 'edges': 0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0,\n                    'samples': 0, 'valid_days': 0, 'status': 'empty_network'\n                }\n            \n            # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡\n            flood_net.fit_conditional(self.train_df, max_parents=max_parents, alpha=alpha)\n            \n            # æ„å»ºè´å¶æ–¯ç½‘ç»œ\n            flood_net.build_bayes_network()\n            \n            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n            metrics = self.evaluate_flood_only(flood_net, evidence_ratio=0.5, prob_thr=prob_thr, verbose=verbose)\n            \n            # ç»„åˆç»“æœ\n            result = {\n                'occ_thr': occ_thr,\n                'edge_thr': edge_thr, \n                'weight_thr': weight_thr,\n                'max_parents': max_parents,\n                'alpha': alpha,\n                'prob_thr': prob_thr,\n                'nodes': flood_net.network.number_of_nodes(),\n                'edges': flood_net.network.number_of_edges(),\n                'status': 'success'\n            }\n            result.update(metrics)\n            \n            return result, flood_net\n            \n        except Exception as e:\n            if verbose:\n                print(f\"   âŒ é…ç½®å¤±è´¥: {e}\")\n            return {\n                'occ_thr': occ_thr, 'edge_thr': edge_thr, 'weight_thr': weight_thr,\n                'max_parents': max_parents, 'alpha': alpha, 'prob_thr': prob_thr,\n                'nodes': 0, 'edges': 0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0,\n                'samples': 0, 'valid_days': 0, 'status': f'error: {str(e)}'\n            }, None\n            \n    def run_parameter_optimization(self, max_configs=50):\n        \"\"\"è¿è¡Œå‚æ•°ä¼˜åŒ–\"\"\"\n        print(\"\\n2. å¼€å§‹å‚æ•°ä¼˜åŒ–...\")\n        \n        # ç”Ÿæˆå‚æ•°ç»„åˆï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡é•¿æ—¶é—´ï¼‰\n        param_combinations = list(product(\n            self.param_grid['occ_thr'],\n            self.param_grid['edge_thr'], \n            self.param_grid['weight_thr'],\n            self.param_grid['max_parents'],\n            self.param_grid['alpha'],\n            self.param_grid['prob_thr']\n        ))\n        \n        # éšæœºé‡‡æ ·ä»¥é™åˆ¶è®¡ç®—æ—¶é—´\n        if len(param_combinations) > max_configs:\n            np.random.shuffle(param_combinations)\n            param_combinations = param_combinations[:max_configs]\n        \n        print(f\"   æµ‹è¯•{len(param_combinations)}ä¸ªå‚æ•°é…ç½®...\")\n        print(f\"   å‚æ•°ç©ºé—´: occ_thr{self.param_grid['occ_thr']}, edge_thr{self.param_grid['edge_thr']}, weight_thr{self.param_grid['weight_thr']}\")\n        print(f\"   è¯„ä¼°ç­–ç•¥: åªè€ƒè™‘æœ‰æ´ªæ°´è®°å½•çš„é“è·¯ï¼Œæ¨ç†ç»“æœâ‰¥é˜ˆå€¼æ‰ç®—æ­£é¢„æµ‹\")\n        \n        successful_configs = 0\n        \n        for i, (occ_thr, edge_thr, weight_thr, max_parents, alpha, prob_thr) in enumerate(param_combinations):\n            if i % 10 == 0:\n                print(f\"   è¿›åº¦: {i+1}/{len(param_combinations)}\")\n            \n            verbose = i < 3  # åªå¯¹å‰å‡ ä¸ªé…ç½®æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯\n            result, network = self.build_and_evaluate_network(\n                occ_thr, edge_thr, weight_thr, max_parents, alpha, prob_thr, verbose=verbose\n            )\n            \n            self.results.append(result)\n            \n            if result['status'] == 'success' and result['f1'] > 0:\n                successful_configs += 1\n                \n                # æ›´æ–°æœ€ä½³é…ç½®\n                if (self.best_config is None or \n                    result['f1'] > self.best_config['f1'] or\n                    (result['f1'] == self.best_config['f1'] and result['nodes'] < self.best_config['nodes'])):\n                    self.best_config = result.copy()\n                    self.best_network = network\n        \n        print(f\"   âœ… å®Œæˆå‚æ•°ä¼˜åŒ–: {successful_configs}/{len(param_combinations)}ä¸ªé…ç½®æˆåŠŸ\")\n        \n        return self.results\n        \n    def analyze_results(self):\n        \"\"\"åˆ†æå’Œå±•ç¤ºç»“æœ\"\"\"\n        print(\"\\n3. ç»“æœåˆ†æ...\")\n        \n        if not self.results:\n            print(\"   âŒ æ²¡æœ‰æœ‰æ•ˆç»“æœ\")\n            return\n        \n        # è¿‡æ»¤æˆåŠŸçš„ç»“æœ\n        successful_results = [r for r in self.results if r['status'] == 'success' and r['f1'] > 0]\n        \n        if not successful_results:\n            print(\"   âŒ æ²¡æœ‰æˆåŠŸçš„é…ç½®\")\n            return\n        \n        print(f\"   ğŸ“Š æˆåŠŸé…ç½®æ•°é‡: {len(successful_results)}\")\n        \n        # æŒ‰F1åˆ†æ•°æ’åº\n        top_results = sorted(successful_results, key=lambda x: (-x['f1'], x['nodes']))[:10]\n        \n        print(f\"\\n   ğŸ† Top-10 é…ç½® (æŒ‰F1åˆ†æ•°æ’åº):\")\n        print(f\"   {'Rank':<4} {'F1':<6} {'Prec':<6} {'Rec':<6} {'Nodes':<6} {'Edges':<6} {'occ':<4} {'edge':<4} {'wt':<4} {'par':<3} {'Î±':<4} {'thr':<4}\")\n        print(\"-\" * 85)\n        \n        for i, result in enumerate(top_results, 1):\n            print(f\"   {i:<4} {result['f1']:<6.3f} {result['precision']:<6.3f} {result['recall']:<6.3f} \"\n                  f\"{result['nodes']:<6} {result['edges']:<6} {result['occ_thr']:<4} {result['edge_thr']:<4} \"\n                  f\"{result['weight_thr']:<4.1f} {result['max_parents']:<3} {result['alpha']:<4.1f} {result['prob_thr']:<4.1f}\")\n        \n        # åˆ†ææœ€ä½³é…ç½®\n        if self.best_config:\n            print(f\"\\n   ğŸ¯ æœ€ä½³é…ç½®è¯¦ç»†åˆ†æ:\")\n            best = self.best_config\n            print(f\"     å‚æ•°: occ_thr={best['occ_thr']}, edge_thr={best['edge_thr']}, weight_thr={best['weight_thr']}\")\n            print(f\"           max_parents={best['max_parents']}, alpha={best['alpha']}, prob_thr={best['prob_thr']}\")\n            print(f\"     ç½‘ç»œ: {best['nodes']}ä¸ªèŠ‚ç‚¹, {best['edges']}æ¡è¾¹\")\n            print(f\"     æ€§èƒ½: F1={best['f1']:.3f}, Precision={best['precision']:.3f}, Recall={best['recall']:.3f}\")\n            print(f\"     æ ·æœ¬: {best['samples']}ä¸ªé¢„æµ‹, {best['valid_days']}ä¸ªæœ‰æ•ˆå¤©æ•°\")\n            print(f\"     é¢„æµ‹: TP={best['tp']}, FN={best['fn']}\")\n        \n        # å‚æ•°å½±å“åˆ†æ\n        self.analyze_parameter_effects(successful_results)\n        \n    def analyze_parameter_effects(self, results):\n        \"\"\"åˆ†æå‚æ•°å¯¹æ€§èƒ½çš„å½±å“\"\"\"\n        print(f\"\\n   ğŸ“ˆ å‚æ•°å½±å“åˆ†æ:\")\n        \n        params = ['occ_thr', 'edge_thr', 'weight_thr', 'max_parents', 'alpha', 'prob_thr']\n        \n        for param in params:\n            values = list(set(r[param] for r in results))\n            if len(values) <= 1:\n                continue\n                \n            print(f\"\\n     {param}:\")\n            for value in sorted(values):\n                subset = [r for r in results if r[param] == value]\n                if subset:\n                    avg_f1 = np.mean([r['f1'] for r in subset])\n                    avg_nodes = np.mean([r['nodes'] for r in subset])\n                    print(f\"       {value}: F1={avg_f1:.3f}, å¹³å‡èŠ‚ç‚¹æ•°={avg_nodes:.1f} ({len(subset)}ä¸ªé…ç½®)\")\n    \n    def save_results(self, filename=\"flood_prediction_results.json\"):\n        \"\"\"ä¿å­˜ç»“æœåˆ°æ–‡ä»¶\"\"\"\n        print(f\"\\n4. ä¿å­˜ç»“æœåˆ° {filename}...\")\n        \n        output = {\n            'timestamp': datetime.now().isoformat(),\n            'evaluation_strategy': 'flood_only',\n            'description': 'åªè€ƒè™‘æœ‰æ´ªæ°´è®°å½•çš„é“è·¯è¿›è¡Œæ¨ç†å’Œè¯„ä¼°',\n            'data_split': 'temporal_70_30',\n            'total_configs': len(self.results),\n            'successful_configs': len([r for r in self.results if r['status'] == 'success']),\n            'best_config': self.best_config,\n            'all_results': self.results,\n            'parameter_grid': self.param_grid\n        }\n        \n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(output, f, indent=2, ensure_ascii=False)\n        \n        print(f\"   âœ… ç»“æœå·²ä¿å­˜\")\n        \n    def demonstrate_best_network(self):\n        \"\"\"æ¼”ç¤ºæœ€ä½³ç½‘ç»œçš„æ¨ç†è¿‡ç¨‹\"\"\"\n        if not self.best_network or not self.best_config:\n            print(\"   âŒ æ²¡æœ‰æœ€ä½³ç½‘ç»œå¯æ¼”ç¤º\")\n            return\n            \n        print(f\"\\n5. æœ€ä½³ç½‘ç»œæ¨ç†æ¼”ç¤º...\")\n        \n        # è·å–æµ‹è¯•é›†ä¸­çš„å‡ ä¸ªæ´ªæ°´æ—¥æœŸ\n        test_by_date = self.test_df.groupby(self.test_df[\"time_create\"].dt.floor(\"D\"))\n        bn_nodes = set(self.best_network.network_bayes.nodes())\n        \n        demo_count = 0\n        for date, day_group in test_by_date:\n            if demo_count >= 3:\n                break\n                \n            flooded_roads = list(day_group[\"link_id\"].unique())\n            flooded_in_bn = [road for road in flooded_roads if road in bn_nodes]\n            \n            if len(flooded_in_bn) >= 3:\n                print(f\"\\n   ğŸ“… {date.date()} æ¨ç†æ¼”ç¤º:\")\n                print(f\"     å½“å¤©æ´ªæ°´é“è·¯: {flooded_in_bn}\")\n                \n                # é€‰æ‹©å‰2ä¸ªä½œä¸ºevidence\n                evidence = {flooded_in_bn[0]: 1, flooded_in_bn[1]: 1}\n                targets = flooded_in_bn[2:]\n                \n                print(f\"     Evidence: {list(evidence.keys())}\")\n                print(f\"     æ¨ç†ç›®æ ‡: {targets}\")\n                \n                for target in targets:\n                    try:\n                        result = self.best_network.infer_w_evidence(target, evidence)\n                        prob = result['flooded']\n                        pred = \"âœ…æ´ªæ°´\" if prob >= self.best_config['prob_thr'] else \"âŒæ— æ´ªæ°´\"\n                        print(f\"       {target}: P(æ´ªæ°´)={prob:.3f} â†’ {pred} (å®é™…: âœ…æ´ªæ°´)\")\n                    except Exception as e:\n                        print(f\"       {target}: æ¨ç†å¤±è´¥ - {e}\")\n                \n                demo_count += 1\n        \n    def run_complete_analysis(self):\n        \"\"\"è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹\"\"\"\n        # 1. åŠ è½½æ•°æ®\n        self.load_and_preprocess_data()\n        \n        # 2. å‚æ•°ä¼˜åŒ–\n        self.run_parameter_optimization()\n        \n        # 3. ç»“æœåˆ†æ\n        self.analyze_results()\n        \n        # 4. ä¿å­˜ç»“æœ\n        self.save_results()\n        \n        # 5. æ¼”ç¤ºæœ€ä½³ç½‘ç»œ\n        self.demonstrate_best_network()\n        \n        print(f\"\\nâœ… å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼\")\n        print(f\"ğŸ¯ æ ¸å¿ƒå‘ç°: ç‰¹æ®Šè¯„ä¼°ç­–ç•¥æ›´ç¬¦åˆå®é™…åº”ç”¨åœºæ™¯\")\n        print(f\"ğŸ“Š æœ€ä½³é…ç½®å·²è¯†åˆ«å¹¶ä¿å­˜\")\n        \n        return self.best_config, self.best_network\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    predictor = ImprovedFloodPredictor()\n    best_config, best_network = predictor.run_complete_analysis()\n    \n    return predictor\n\nif __name__ == \"__main__\":\n    predictor = main()