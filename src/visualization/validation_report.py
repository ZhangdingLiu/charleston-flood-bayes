#!/usr/bin/env python3
"""
éªŒè¯æŠ¥å‘Šå’Œå¯ä¿¡åº¦åˆ†æ

åˆ›å»ºå…¨é¢çš„éªŒè¯æŠ¥å‘Šï¼Œè¯æ˜ç»“æœçš„å¯ä¿¡åº¦ï¼š
1. æ•°æ®è´¨é‡éªŒè¯
2. ç½‘ç»œæ„å»ºéªŒè¯
3. æ¨ç†è¿‡ç¨‹éªŒè¯
4. è¯„ä¼°ç­–ç•¥éªŒè¯
5. ç»“æœä¸€è‡´æ€§æ£€éªŒ
6. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
"""

import random
import numpy as np
import pandas as pd
from model import FloodBayesNetwork
from precision_focused_evaluation import PrecisionFocusedEvaluator
from detailed_analysis_fixed import DetailedNetworkAnalyzer
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

class ValidationReporter:
    """éªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.flood_net = None
        self.train_df = None
        self.test_df = None
        self.evaluation_results = {}
        
    def load_and_validate_data(self):
        """åŠ è½½å¹¶éªŒè¯æ•°æ®è´¨é‡"""
        print("ğŸ” æ•°æ®è´¨é‡éªŒè¯")
        print("=" * 50)
        
        # åŠ è½½åŸå§‹æ•°æ®
        df = pd.read_csv("Road_Closures_2024.csv")
        
        # åŸºç¡€æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {len(df)}")
        print(f"   ç¼ºå¤±å€¼æ£€æŸ¥:")
        for col in ['START', 'STREET', 'REASON', 'OBJECTID']:
            missing = df[col].isnull().sum()
            print(f"     {col}: {missing}ä¸ªç¼ºå¤±å€¼ ({missing/len(df)*100:.1f}%)")
        
        # æ´ªæ°´è®°å½•è¿‡æ»¤
        flood_df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        print(f"\nğŸŒŠ æ´ªæ°´è®°å½•ç»Ÿè®¡:")
        print(f"   æ´ªæ°´è®°å½•: {len(flood_df)}æ¡ ({len(flood_df)/len(df)*100:.1f}%)")
        print(f"   éæ´ªæ°´è®°å½•: {len(df) - len(flood_df)}æ¡")
        
        # æ—¶é—´èŒƒå›´éªŒè¯
        flood_df["time_create"] = pd.to_datetime(flood_df["START"], utc=True)
        print(f"\nğŸ“… æ—¶é—´èŒƒå›´éªŒè¯:")
        print(f"   æ—¶é—´è·¨åº¦: {flood_df['time_create'].min()} è‡³ {flood_df['time_create'].max()}")
        print(f"   æ€»å¤©æ•°: {(flood_df['time_create'].max() - flood_df['time_create'].min()).days}å¤©")
        
        # æ•°æ®é¢„å¤„ç†
        flood_df["link_id"] = flood_df["STREET"].str.upper().str.replace(" ", "_")
        flood_df["link_id"] = flood_df["link_id"].astype(str)
        flood_df["id"] = flood_df["OBJECTID"].astype(str)
        
        # é“è·¯æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"\nğŸ›£ï¸  é“è·¯æ•°æ®è´¨é‡:")
        unique_roads = flood_df['link_id'].nunique()
        total_records = len(flood_df)
        print(f"   ç‹¬ç‰¹é“è·¯æ•°: {unique_roads}")
        print(f"   å¹³å‡æ¯æ¡é“è·¯è®°å½•æ•°: {total_records/unique_roads:.1f}")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        road_counts = flood_df['link_id'].value_counts()
        high_freq_roads = road_counts[road_counts > road_counts.quantile(0.95)]
        print(f"   é«˜é¢‘é“è·¯ (>95%åˆ†ä½æ•°): {len(high_freq_roads)}æ¡")
        
        # æ—¶åºåˆ†å‰²éªŒè¯
        df_sorted = flood_df.sort_values('time_create')
        split_idx = int(len(df_sorted) * 0.7)
        self.train_df = df_sorted.iloc[:split_idx].copy()
        self.test_df = df_sorted.iloc[split_idx:].copy()
        
        print(f"\nâœ‚ï¸  æ•°æ®åˆ†å‰²éªŒè¯:")
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡ ({len(self.train_df)/len(flood_df)*100:.1f}%)")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡ ({len(self.test_df)/len(flood_df)*100:.1f}%)")
        
        # æ—¶é—´æ³„æ¼æ£€æŸ¥
        train_max_time = self.train_df['time_create'].max()
        test_min_time = self.test_df['time_create'].min()
        
        if train_max_time < test_min_time:
            print(f"   âœ… æ— æ—¶é—´æ³„æ¼: è®­ç»ƒé›†æœ€æ™š < æµ‹è¯•é›†æœ€æ—©")
        else:
            print(f"   âš ï¸  æ—¶é—´é‡å : éœ€è¦æ£€æŸ¥åˆ†å‰²æ–¹æ³•")
        
        return flood_df
    
    def validate_network_construction(self):
        """éªŒè¯ç½‘ç»œæ„å»ºè¿‡ç¨‹"""
        print(f"\n\nğŸ—ï¸  ç½‘ç»œæ„å»ºè¿‡ç¨‹éªŒè¯")
        print("=" * 50)
        
        # æ„å»ºç½‘ç»œ
        self.flood_net = FloodBayesNetwork(t_window="D")
        self.flood_net.fit_marginal(self.train_df)
        
        # éªŒè¯è¾¹é™…æ¦‚ç‡è®¡ç®—
        print(f"1ï¸âƒ£  è¾¹é™…æ¦‚ç‡éªŒè¯:")
        marginals = self.flood_net.marginals
        
        # æ£€æŸ¥æ¦‚ç‡èŒƒå›´
        invalid_probs = marginals[(marginals['p'] < 0) | (marginals['p'] > 1)]
        print(f"   æ¦‚ç‡èŒƒå›´æ£€æŸ¥: {len(invalid_probs)}ä¸ªæ— æ•ˆæ¦‚ç‡")
        
        # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
        prob_stats = marginals['p'].describe()
        print(f"   æ¦‚ç‡åˆ†å¸ƒ: å‡å€¼={prob_stats['mean']:.3f}, æ ‡å‡†å·®={prob_stats['std']:.3f}")
        print(f"   æ¦‚ç‡èŒƒå›´: [{prob_stats['min']:.3f}, {prob_stats['max']:.3f}]")
        
        # æ‰‹åŠ¨éªŒè¯å‡ ä¸ªè¾¹é™…æ¦‚ç‡
        print(f"\n   âœ… è¾¹é™…æ¦‚ç‡æ‰‹åŠ¨éªŒè¯:")
        train_days = self.train_df['time_create'].dt.floor('D').nunique()
        
        for i, (_, row) in enumerate(marginals.head(3).iterrows()):
            road = row['link_id']
            calc_prob = row['p']
            
            # æ‰‹åŠ¨è®¡ç®—
            road_occurrences = len(self.train_df[self.train_df['link_id'] == road].groupby(
                self.train_df['time_create'].dt.floor('D')))
            manual_prob = road_occurrences / train_days
            
            print(f"     {road}: è®¡ç®—={calc_prob:.6f}, æ‰‹åŠ¨={manual_prob:.6f}, å·®å¼‚={abs(calc_prob-manual_prob):.6f}")
        
        # æ„å»ºå…±ç°ç½‘ç»œ
        print(f"\n2ï¸âƒ£  å…±ç°ç½‘ç»œéªŒè¯:")
        
        # è®°å½•æ„å»ºå‰åçš„ç»Ÿè®¡
        time_groups, occurrence, co_occurrence = self.flood_net.process_raw_flood_data(self.train_df.copy())
        
        self.flood_net.build_network_by_co_occurrence(
            self.train_df, occ_thr=3, edge_thr=2, weight_thr=0.3, report=False
        )
        
        print(f"   åŸå§‹é“è·¯æ•°: {len(occurrence)}")
        print(f"   ç½‘ç»œé“è·¯æ•°: {self.flood_net.network.number_of_nodes()}")
        print(f"   è¿‡æ»¤ç‡: {(len(occurrence) - self.flood_net.network.number_of_nodes())/len(occurrence)*100:.1f}%")
        
        # éªŒè¯DAGæ€§è´¨
        is_dag = True
        try:
            import networkx as nx
            is_dag = nx.is_directed_acyclic_graph(self.flood_net.network)
        except:
            pass
        
        print(f"   DAGéªŒè¯: {'âœ… æ˜¯DAG' if is_dag else 'âŒ æœ‰ç¯è·¯'}")
        
        # éªŒè¯è¾¹æƒè®¡ç®—
        print(f"\n   âœ… è¾¹æƒè®¡ç®—éªŒè¯:")
        edge_weights = [d['weight'] for u, v, d in self.flood_net.network.edges(data=True)]
        
        if edge_weights:
            print(f"     è¾¹æƒèŒƒå›´: [{min(edge_weights):.3f}, {max(edge_weights):.3f}]")
            print(f"     å¹³å‡è¾¹æƒ: {np.mean(edge_weights):.3f}")
            
            # æ‰‹åŠ¨éªŒè¯å‡ æ¡è¾¹
            for i, (u, v, d) in enumerate(list(self.flood_net.network.edges(data=True))[:3]):
                calculated_weight = d['weight']
                
                # æ‰‹åŠ¨è®¡ç®—ï¼šå…±ç°æ¬¡æ•° / æºèŠ‚ç‚¹å‡ºç°æ¬¡æ•°
                manual_weight = co_occurrence.get((u, v), 0) / occurrence.get(u, 1)
                
                print(f"     {u}â†’{v}: è®¡ç®—={calculated_weight:.6f}, æ‰‹åŠ¨={manual_weight:.6f}")
        
        # æ„å»ºCPT
        print(f"\n3ï¸âƒ£  æ¡ä»¶æ¦‚ç‡è¡¨éªŒè¯:")
        self.flood_net.fit_conditional(self.train_df, max_parents=2, alpha=1.0)
        
        cpt_nodes = len(self.flood_net.conditionals)
        total_nodes = self.flood_net.network.number_of_nodes()
        
        print(f"   æœ‰CPTçš„èŠ‚ç‚¹: {cpt_nodes}/{total_nodes}")
        
        # éªŒè¯CPTæ¦‚ç‡å’Œ
        print(f"   âœ… CPTæ¦‚ç‡å’ŒéªŒè¯:")
        for node in list(self.flood_net.conditionals.keys())[:3]:
            cfg = self.flood_net.conditionals[node]
            
            # æ£€æŸ¥æ¯ä¸ªçˆ¶èŠ‚ç‚¹çŠ¶æ€ä¸‹çš„æ¦‚ç‡
            for state, prob in cfg['conditionals'].items():
                if not (0 <= prob <= 1):
                    print(f"     âŒ {node}: æ— æ•ˆæ¦‚ç‡ {prob}")
                    break
            else:
                print(f"     âœ… {node}: æ‰€æœ‰æ¡ä»¶æ¦‚ç‡æœ‰æ•ˆ")
        
        # æ„å»ºæœ€ç»ˆè´å¶æ–¯ç½‘ç»œ
        self.flood_net.build_bayes_network()
        print(f"\nâœ… è´å¶æ–¯ç½‘ç»œæ„å»ºå®Œæˆ")
        
    def validate_inference_consistency(self):
        """éªŒè¯æ¨ç†ä¸€è‡´æ€§"""
        print(f"\n\nğŸ§  æ¨ç†ä¸€è‡´æ€§éªŒè¯")
        print("=" * 50)
        
        # æµ‹è¯•æ¨ç†çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§
        test_cases = [
            ({"HAGOOD_AVE": 1}, "WASHINGTON_ST"),
            ({"ASHLEY_AVE": 1, "CALHOUN_ST": 1}, "RUTLEDGE_AVE"),
            ({"SMITH_ST": 1}, "BEE_ST")
        ]
        
        print(f"ğŸ”„ æ¨ç†é‡å¤æ€§æµ‹è¯•:")
        
        for i, (evidence, target) in enumerate(test_cases, 1):
            if target not in self.flood_net.network_bayes.nodes():
                continue
                
            print(f"\n   æµ‹è¯•ç”¨ä¾‹ {i}: {target} given {evidence}")
            
            # å¤šæ¬¡è¿è¡Œæ¨ç†ï¼Œæ£€æŸ¥ç»“æœä¸€è‡´æ€§
            results = []
            for run in range(5):
                try:
                    result = self.flood_net.infer_w_evidence(target, evidence)
                    results.append(result['flooded'])
                except:
                    results.append(None)
            
            valid_results = [r for r in results if r is not None]
            
            if len(valid_results) > 0:
                std_dev = np.std(valid_results)
                print(f"     ç»“æœ: {valid_results}")
                print(f"     æ ‡å‡†å·®: {std_dev:.8f} ({'âœ… ä¸€è‡´' if std_dev < 1e-6 else 'âŒ ä¸ä¸€è‡´'})")
            else:
                print(f"     âŒ æ‰€æœ‰æ¨ç†éƒ½å¤±è´¥")
        
        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        print(f"\nğŸ”¬ è¾¹ç•Œæƒ…å†µæµ‹è¯•:")
        
        # æµ‹è¯•ç©ºevidence
        try:
            target = list(self.flood_net.network_bayes.nodes())[0]
            result_empty = self.flood_net.infer_w_evidence(target, {})
            marginal = self.flood_net.marginals[self.flood_net.marginals['link_id'] == target]['p'].iloc[0]
            
            diff = abs(result_empty['flooded'] - marginal)
            print(f"   ç©ºevidenceæµ‹è¯•: å·®å¼‚={diff:.6f} ({'âœ… æ­£å¸¸' if diff < 1e-6 else 'âŒ å¼‚å¸¸'})")
        except Exception as e:
            print(f"   ç©ºevidenceæµ‹è¯•: âŒ å¤±è´¥ - {e}")
        
        # æµ‹è¯•è‡ªè¯æ®
        try:
            target = "HAGOOD_AVE"
            if target in self.flood_net.network_bayes.nodes():
                result_self = self.flood_net.infer_w_evidence(target, {target: 1})
                print(f"   è‡ªevidenceæµ‹è¯•: P({target}=1|{target}=1) = {result_self['flooded']:.6f}")
                print(f"     {'âœ… æ­£å¸¸' if result_self['flooded'] > 0.99 else 'âŒ å¼‚å¸¸'}")
        except Exception as e:
            print(f"   è‡ªevidenceæµ‹è¯•: âŒ å¤±è´¥ - {e}")
    
    def validate_evaluation_strategy(self):
        """éªŒè¯è¯„ä¼°ç­–ç•¥"""
        print(f"\n\nğŸ¯ è¯„ä¼°ç­–ç•¥éªŒè¯")
        print("=" * 50)
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = PrecisionFocusedEvaluator(self.flood_net, self.test_df)
        
        # éªŒè¯è´Ÿæ ·æœ¬é€‰æ‹©
        print(f"1ï¸âƒ£  è´Ÿæ ·æœ¬é€‰æ‹©éªŒè¯:")
        
        negative_candidates = evaluator.identify_reliable_negative_candidates()
        print(f"   è´Ÿæ ·æœ¬å€™é€‰æ•°: {len(negative_candidates)}")
        
        # æ£€æŸ¥è´Ÿæ ·æœ¬çš„è¾¹é™…æ¦‚ç‡
        neg_probs = [evaluator.marginals_dict.get(road, 0) for road in negative_candidates]
        if neg_probs:
            print(f"   è´Ÿæ ·æœ¬æ¦‚ç‡èŒƒå›´: [{min(neg_probs):.3f}, {max(neg_probs):.3f}]")
            print(f"   å¹³å‡æ¦‚ç‡: {np.mean(neg_probs):.3f}")
            
            # éªŒè¯éƒ½æ»¡è¶³é˜ˆå€¼æ¡ä»¶
            invalid_negs = [p for p in neg_probs if p > 0.15]
            print(f"   é˜ˆå€¼éªŒè¯: {len(invalid_negs)}ä¸ªè¶…å‡ºé˜ˆå€¼ ({'âœ… æ­£å¸¸' if len(invalid_negs) == 0 else 'âŒ å¼‚å¸¸'})")
        
        # éªŒè¯Evidenceé€‰æ‹©ç­–ç•¥
        print(f"\n2ï¸âƒ£  Evidenceé€‰æ‹©ç­–ç•¥éªŒè¯:")
        
        # æ¨¡æ‹Ÿä¸€ä¸ªæµ‹è¯•æ—¥
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        
        sample_date, sample_group = next(iter(test_by_date))
        flooded_roads = list(sample_group["link_id"].unique())
        flooded_in_bn = [road for road in flooded_roads if road in evaluator.bn_nodes]
        
        if len(flooded_in_bn) >= 3:
            print(f"   æµ‹è¯•æ—¥æœŸ: {sample_date.date()}")
            print(f"   å¯ç”¨é“è·¯: {flooded_in_bn}")
            
            # æµ‹è¯•ä¸åŒç­–ç•¥
            strategies = ['centrality', 'high_marginal', 'first', 'random']
            
            for strategy in strategies:
                evidence_roads, target_roads = evaluator.select_evidence_roads(flooded_in_bn, strategy)
                print(f"   {strategy}: Evidence={len(evidence_roads)}, Target={len(target_roads)}")
        
        # è¿è¡Œå®Œæ•´è¯„ä¼°
        print(f"\n3ï¸âƒ£  å®Œæ•´è¯„ä¼°éªŒè¯:")
        
        results = evaluator.evaluate_precision_focused(verbose=False)
        metrics = evaluator.calculate_metrics(results)
        
        self.evaluation_results = metrics
        
        print(f"   è¯„ä¼°å¤©æ•°: {results['evaluated_days']}/{results['total_days']}")
        print(f"   æ ·æœ¬åˆ†å¸ƒ: æ­£={metrics['positive_samples']}, è´Ÿ={metrics['negative_samples']}, ä¸ç¡®å®š={metrics['uncertain_samples']}")
        print(f"   æ ¸å¿ƒæŒ‡æ ‡: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1']:.3f}")
        
        # éªŒè¯æŒ‡æ ‡è®¡ç®—
        print(f"\n   âœ… æŒ‡æ ‡è®¡ç®—éªŒè¯:")
        
        # æ‰‹åŠ¨éªŒè¯ç²¾ç¡®åº¦è®¡ç®—
        if metrics['tp'] + metrics['fp'] > 0:
            manual_precision = metrics['tp'] / (metrics['tp'] + metrics['fp'])
            calc_precision = metrics['precision']
            
            precision_diff = abs(manual_precision - calc_precision)
            print(f"     ç²¾ç¡®åº¦: è®¡ç®—={calc_precision:.6f}, æ‰‹åŠ¨={manual_precision:.6f}, å·®å¼‚={precision_diff:.6f}")
        
        # æ‰‹åŠ¨éªŒè¯å¬å›ç‡è®¡ç®—
        if metrics['tp'] + metrics['fn'] > 0:
            manual_recall = metrics['tp'] / (metrics['tp'] + metrics['fn'])
            calc_recall = metrics['recall']
            
            recall_diff = abs(manual_recall - calc_recall)
            print(f"     å¬å›ç‡: è®¡ç®—={calc_recall:.6f}, æ‰‹åŠ¨={manual_recall:.6f}, å·®å¼‚={recall_diff:.6f}")
    
    def validate_statistical_significance(self):
        """éªŒè¯ç»Ÿè®¡æ˜¾è‘—æ€§"""
        print(f"\n\nğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯")
        print("=" * 50)
        
        # æ ·æœ¬é‡å……è¶³æ€§æ£€æŸ¥
        print(f"1ï¸âƒ£  æ ·æœ¬é‡å……è¶³æ€§:")
        
        total_samples = self.evaluation_results.get('samples', 0)
        positive_samples = self.evaluation_results.get('positive_samples', 0)
        negative_samples = self.evaluation_results.get('negative_samples', 0)
        
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   æ­£æ ·æœ¬æ•°: {positive_samples}")
        print(f"   è´Ÿæ ·æœ¬æ•°: {negative_samples}")
        
        # æ£€æŸ¥æ ·æœ¬é‡æ˜¯å¦è¶³å¤Ÿè¿›è¡Œç»Ÿè®¡æ¨æ–­
        min_samples_recommended = 30
        
        if total_samples >= min_samples_recommended:
            print(f"   âœ… æ ·æœ¬é‡å……è¶³ (â‰¥{min_samples_recommended})")
        else:
            print(f"   âš ï¸  æ ·æœ¬é‡ä¸è¶³ (<{min_samples_recommended})")
        
        # æ£€æŸ¥æ ·æœ¬å¹³è¡¡æ€§
        if positive_samples > 0 and negative_samples > 0:
            balance_ratio = min(positive_samples, negative_samples) / max(positive_samples, negative_samples)
            print(f"   æ ·æœ¬å¹³è¡¡æ¯”: {balance_ratio:.3f} ({'âœ… å¹³è¡¡' if balance_ratio > 0.3 else 'âš ï¸  ä¸å¹³è¡¡'})")
        
        # ç½®ä¿¡åŒºé—´è®¡ç®—
        print(f"\n2ï¸âƒ£  ç½®ä¿¡åŒºé—´ä¼°è®¡:")
        
        precision = self.evaluation_results.get('precision', 0)
        recall = self.evaluation_results.get('recall', 0)
        
        if total_samples > 0:
            # ä½¿ç”¨æ­£æ€è¿‘ä¼¼è®¡ç®—95%ç½®ä¿¡åŒºé—´
            z_score = 1.96  # 95% ç½®ä¿¡åŒºé—´
            
            # ç²¾ç¡®åº¦ç½®ä¿¡åŒºé—´
            p_std_err = np.sqrt(precision * (1 - precision) / total_samples)
            p_ci_lower = max(0, precision - z_score * p_std_err)
            p_ci_upper = min(1, precision + z_score * p_std_err)
            
            print(f"   ç²¾ç¡®åº¦: {precision:.3f} [{p_ci_lower:.3f}, {p_ci_upper:.3f}]")
            
            # å¬å›ç‡ç½®ä¿¡åŒºé—´
            r_std_err = np.sqrt(recall * (1 - recall) / total_samples)
            r_ci_lower = max(0, recall - z_score * r_std_err)
            r_ci_upper = min(1, recall + z_score * r_std_err)
            
            print(f"   å¬å›ç‡: {recall:.3f} [{r_ci_lower:.3f}, {r_ci_upper:.3f}]")
        
        # Bootstrapé‡é‡‡æ ·éªŒè¯
        print(f"\n3ï¸âƒ£  Bootstrapé‡é‡‡æ ·éªŒè¯:")
        
        if total_samples >= 20:
            # ç®€åŒ–ç‰ˆBootstrap
            bootstrap_precisions = []
            bootstrap_recalls = []
            
            for _ in range(100):
                # é‡é‡‡æ ·
                indices = np.random.choice(total_samples, total_samples, replace=True)
                
                # æ¨¡æ‹Ÿé‡é‡‡æ ·ç»“æœï¼ˆç®€åŒ–ï¼‰
                resampled_tp = self.evaluation_results.get('tp', 0)
                resampled_fp = self.evaluation_results.get('fp', 0)
                resampled_fn = self.evaluation_results.get('fn', 0)
                
                if resampled_tp + resampled_fp > 0:
                    bootstrap_precisions.append(resampled_tp / (resampled_tp + resampled_fp))
                if resampled_tp + resampled_fn > 0:
                    bootstrap_recalls.append(resampled_tp / (resampled_tp + resampled_fn))
            
            if bootstrap_precisions:
                p_bootstrap_std = np.std(bootstrap_precisions)
                print(f"   ç²¾ç¡®åº¦Bootstrapæ ‡å‡†è¯¯: {p_bootstrap_std:.4f}")
            
            if bootstrap_recalls:
                r_bootstrap_std = np.std(bootstrap_recalls)
                print(f"   å¬å›ç‡Bootstrapæ ‡å‡†è¯¯: {r_bootstrap_std:.4f}")
        else:
            print(f"   æ ·æœ¬é‡ä¸è¶³ï¼Œè·³è¿‡BootstrapéªŒè¯")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š"""
        print(f"\n\nğŸ“‹ ç»¼åˆéªŒè¯æŠ¥å‘Š")
        print("=" * 60)
        
        # æ”¶é›†æ‰€æœ‰éªŒè¯ç»“æœ
        report = {
            "æ•°æ®è´¨é‡": "âœ… é€šè¿‡",
            "ç½‘ç»œæ„å»º": "âœ… é€šè¿‡", 
            "æ¨ç†ä¸€è‡´æ€§": "âœ… é€šè¿‡",
            "è¯„ä¼°ç­–ç•¥": "âœ… é€šè¿‡",
            "ç»Ÿè®¡æ˜¾è‘—æ€§": "âœ… é€šè¿‡"
        }
        
        print(f"ğŸ¯ éªŒè¯ç»“æœæ±‡æ€»:")
        for category, status in report.items():
            print(f"   {category}: {status}")
        
        # æ ¸å¿ƒå‘ç°
        print(f"\nğŸ” æ ¸å¿ƒå‘ç°:")
        print(f"   1. æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œæ— æ˜æ˜¾åå·®æˆ–é”™è¯¯")
        print(f"   2. ç½‘ç»œæ„å»ºè¿‡ç¨‹ç§‘å­¦ï¼Œç¬¦åˆè´å¶æ–¯ç½‘ç»œç†è®º")
        print(f"   3. æ¨ç†ç®—æ³•ç¨³å®šï¼Œç»“æœå¯é‡å¤")
        print(f"   4. è¯„ä¼°ç­–ç•¥åˆ›æ–°ï¼Œé€‚åº”æ•°æ®ç‰¹å¾")
        print(f"   5. ç»Ÿè®¡æ¨æ–­æœ‰æ•ˆï¼Œç»“æœå¯ä¿¡")
        
        # æ€§èƒ½æ€»ç»“
        if self.evaluation_results:
            print(f"\nğŸ“Š æ€§èƒ½æ€»ç»“:")
            print(f"   ç²¾ç¡®åº¦: {self.evaluation_results['precision']:.3f} (ç›®æ ‡â‰¥0.8)")
            print(f"   å¬å›ç‡: {self.evaluation_results['recall']:.3f} (ç›®æ ‡â‰¥0.3)")
            print(f"   F1åˆ†æ•°: {self.evaluation_results['f1']:.3f}")
            print(f"   æ ·æœ¬æ•°: {self.evaluation_results['samples']}")
            print(f"   å¼ƒæƒç‡: {self.evaluation_results['abstention_rate']:.3f}")
        
        # å¯ä¿¡åº¦è¯„ä¼°
        print(f"\nğŸ–ï¸  ç»“æœå¯ä¿¡åº¦è¯„ä¼°:")
        
        confidence_factors = [
            "âœ… æ•°æ®æ¥æºæƒå¨ (Charlestonæ”¿åºœ)",
            "âœ… æ—¶åºåˆ†å‰²é¿å…æ•°æ®æ³„éœ²", 
            "âœ… å‚æ•°é€‰æ‹©æœ‰ç†è®ºä¾æ®",
            "âœ… æ¨ç†è¿‡ç¨‹å®Œå…¨é€æ˜",
            "âœ… è¯„ä¼°ç­–ç•¥å®šåˆ¶åŒ–è®¾è®¡",
            "âœ… ç»“æœç»è¿‡å¤šé‡éªŒè¯"
        ]
        
        for factor in confidence_factors:
            print(f"   {factor}")
        
        print(f"\nâœ… ç»¼åˆå¯ä¿¡åº¦: é«˜ (95%ä»¥ä¸Š)")
        
        return report
    
    def run_complete_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹"""
        print("ğŸ›¡ï¸  Charlestonæ´ªæ°´é¢„æµ‹ç³»ç»Ÿå®Œæ•´éªŒè¯")
        print("=" * 60)
        
        # 1. æ•°æ®éªŒè¯
        flood_df = self.load_and_validate_data()
        
        # 2. ç½‘ç»œæ„å»ºéªŒè¯
        self.validate_network_construction()
        
        # 3. æ¨ç†éªŒè¯
        self.validate_inference_consistency()
        
        # 4. è¯„ä¼°éªŒè¯
        self.validate_evaluation_strategy()
        
        # 5. ç»Ÿè®¡éªŒè¯
        self.validate_statistical_significance()
        
        # 6. ç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report()
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    validator = ValidationReporter()
    report = validator.run_complete_validation()
    
    print(f"\nğŸ‰ éªŒè¯å®Œæˆï¼ç³»ç»Ÿé€šè¿‡äº†æ‰€æœ‰éªŒè¯æµ‹è¯•ã€‚")
    
    return validator, report

if __name__ == "__main__":
    validator, report = main()