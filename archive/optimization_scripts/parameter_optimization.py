import pandas as pd
import numpy as np
from model import FloodBayesNetwork
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    brier_score_loss, log_loss, roc_auc_score,
    average_precision_score, accuracy_score, f1_score,
    recall_score, precision_score
)
import math
import warnings
from collections import defaultdict, Counter
import os
import json
import random
from typing import Dict, List, Tuple

# è®¾ç½®éšæœºç§å­
random.seed(0)
np.random.seed(0)

class ParameterOptimizer:
    def __init__(self, csv_path: str = "Road_Closures_2024.csv"):
        self.csv_path = csv_path
        self.df = None
        self.train_df = None
        self.test_df = None
        self.load_data()
        
    def load_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
        self.df = pd.read_csv(self.csv_path)
        self.df = self.df[self.df["REASON"].str.upper() == "FLOOD"].copy()
        
        # æ•°æ®é¢„å¤„ç†
        self.df["time_create"] = pd.to_datetime(self.df["START"], utc=True)
        self.df["link_id"] = self.df["STREET"].str.upper().str.replace(" ", "_")
        self.df["link_id"] = self.df["link_id"].astype(str)
        self.df["id"] = self.df["OBJECTID"].astype(str)
        
        # è®­ç»ƒæµ‹è¯•é›†åˆ†å‰²
        self.train_df, self.test_df = train_test_split(
            self.df, test_size=0.3, random_state=42
        )
        
        print(f"æ€»æ•°æ®é‡: {len(self.df)}")
        print(f"è®­ç»ƒé›†: {len(self.train_df)}, æµ‹è¯•é›†: {len(self.test_df)}")
        
    def _binary_entropy(self, p: float) -> float:
        """è®¡ç®—äºŒå…ƒç†µ"""
        if p in (0, 1):
            return 0.0
        return -(p*math.log2(p) + (1-p)*math.log2(1-p))
        
    def evaluate_bn_enhanced(self, flood_net, test_df, evidence_size=3, k=5, prob_thr=0.5):
        """
        ä¼˜åŒ–åçš„è¯„ä¼°å‡½æ•°ï¼Œæ›´å¥½åœ°å¤„ç†ç¨€ç–æ•°æ®
        """
        bn_nodes = set(flood_net.network_bayes.nodes())
        y_true, y_prob = [], []
        
        # ç»Ÿè®¡ä¿¡æ¯
        hits_at_k = total_at_k = 0
        h_clim, h_post = 0.0, 0.0
        
        # è·å–é“è·¯é¢‘æ¬¡ä¿¡æ¯ç”¨äºé€‰æ‹©evidence
        road_counts = Counter(self.train_df['link_id'])
        frequent_roads = {road for road, count in road_counts.items() if count >= 2}
        
        # æŒ‰æ—¥æœŸè¿­ä»£
        for day, group in test_df.groupby(test_df["time_create"].dt.floor("D")):
            flooded = [r for r in group["link_id"] if r in bn_nodes]
            
            # ä¼˜åŒ–evidenceé€‰æ‹©ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©é¢‘æ¬¡>=2çš„é“è·¯
            frequent_flooded = [r for r in flooded if r in frequent_roads]
            if len(frequent_flooded) >= evidence_size:
                evidence_roads = frequent_flooded[:evidence_size]
            else:
                evidence_roads = frequent_flooded + flooded[:evidence_size-len(frequent_flooded)]
            
            evidence = {r: 1 for r in evidence_roads}
            
            # è®¡ç®—åŸºçº¿ç†µ
            for r in bn_nodes:
                p0 = float(flood_net.marginals.loc[
                           flood_net.marginals["link_id"] == r, "p"].values[0])
                h_clim += self._binary_entropy(p0)
            
            # é¢„æµ‹å…¶ä»–é“è·¯
            for r in bn_nodes:
                if r in evidence:
                    continue
                    
                try:
                    p_flood = flood_net.infer_w_evidence(r, evidence)["flooded"]
                    y_prob.append(p_flood)
                    y_true.append(1 if r in flooded else 0)
                    h_post += self._binary_entropy(p_flood)
                except Exception as e:
                    # è·³è¿‡æ¨ç†å¤±è´¥çš„æƒ…å†µ
                    continue
            
            # Recall@kè®¡ç®—
            try:
                topk = sorted(
                    ((r, flood_net.infer_w_evidence(r, evidence)['flooded'])
                     for r in bn_nodes if r not in evidence),
                    key=lambda x: x[1], reverse=True
                )[:k]
                
                hits_at_k += sum(1 for r, _ in topk if r in flooded)
                total_at_k += min(k, len(flooded))
            except Exception:
                continue
        
        # è®¡ç®—æŒ‡æ ‡
        if len(y_true) == 0:
            return {metric: 0.0 for metric in ["Brier", "LogLoss", "ROC-AUC", "PR-AUC", 
                                               "Accuracy", "Precision", "F1", f"Recall@{k}", 
                                               "Î”H(bits)", "BSS", "Samples"]}
        
        # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        pos_ratio = sum(y_true) / len(y_true)
        
        y_hat = (np.array(y_prob) >= prob_thr).astype(int)
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            
            metrics = {
                "Brier": brier_score_loss(y_true, y_prob),
                "LogLoss": log_loss(y_true, y_prob),
                "ROC-AUC": roc_auc_score(y_true, y_prob) if len(set(y_true)) > 1 else 0.5,
                "PR-AUC": average_precision_score(y_true, y_prob) if len(set(y_true)) > 1 else pos_ratio,
                "Accuracy": accuracy_score(y_true, y_hat),
                "Precision": precision_score(y_true, y_hat, zero_division=0),
                "F1": f1_score(y_true, y_hat, zero_division=0),
                f"Recall@{k}": hits_at_k / total_at_k if total_at_k > 0 else 0.0,
                "Î”H(bits)": h_clim - h_post,
                "Samples": len(y_true),
                "Pos_ratio": pos_ratio
            }
            
            # Brier Skill Score
            clim_bs = brier_score_loss(y_true, [pos_ratio]*len(y_true))
            metrics["BSS"] = 1 - metrics["Brier"]/clim_bs if clim_bs > 0 else 0.0
            
        return metrics
        
    def analyze_network_statistics(self, network):
        """åˆ†æç½‘ç»œç»Ÿè®¡ä¿¡æ¯"""
        if network.number_of_nodes() == 0:
            return {
                "nodes": 0, "edges": 0, "density": 0.0,
                "avg_degree": 0.0, "max_degree": 0,
                "avg_in_degree": 0.0, "max_in_degree": 0,
                "avg_out_degree": 0.0, "max_out_degree": 0,
                "isolated_nodes": 0
            }
        
        # åŸºæœ¬ç»Ÿè®¡
        n_nodes = network.number_of_nodes()
        n_edges = network.number_of_edges()
        
        # åº¦åˆ†å¸ƒ
        in_degrees = dict(network.in_degree())
        out_degrees = dict(network.out_degree())
        total_degrees = {node: in_degrees[node] + out_degrees[node] for node in network.nodes()}
        
        # å¯†åº¦
        max_edges = n_nodes * (n_nodes - 1)
        density = n_edges / max_edges if max_edges > 0 else 0.0
        
        # å­¤ç«‹èŠ‚ç‚¹
        isolated = sum(1 for node in network.nodes() if total_degrees[node] == 0)
        
        return {
            "nodes": n_nodes,
            "edges": n_edges,
            "density": density,
            "avg_degree": np.mean(list(total_degrees.values())),
            "max_degree": max(total_degrees.values()) if total_degrees else 0,
            "avg_in_degree": np.mean(list(in_degrees.values())),
            "max_in_degree": max(in_degrees.values()) if in_degrees else 0,
            "avg_out_degree": np.mean(list(out_degrees.values())),
            "max_out_degree": max(out_degrees.values()) if out_degrees else 0,
            "isolated_nodes": isolated
        }
        
    def analyze_edge_weights(self, network):
        """åˆ†æè¾¹æƒé‡åˆ†å¸ƒ"""
        if network.number_of_edges() == 0:
            return {
                "weight_mean": 0.0, "weight_std": 0.0, "weight_min": 0.0,
                "weight_max": 0.0, "weight_median": 0.0, "weight_q25": 0.0,
                "weight_q75": 0.0
            }
        
        weights = [network[u][v]['weight'] for u, v in network.edges()]
        
        return {
            "weight_mean": np.mean(weights),
            "weight_std": np.std(weights),
            "weight_min": np.min(weights),
            "weight_max": np.max(weights),
            "weight_median": np.median(weights),
            "weight_q25": np.percentile(weights, 25),
            "weight_q75": np.percentile(weights, 75)
        }
        
    def test_parameter_combinations(self):
        """æµ‹è¯•ä¸‰ç§å‚æ•°ç­–ç•¥ç»„åˆ"""
        
        # å®šä¹‰ä¸‰ç§ç­–ç•¥
        strategies = {
            "conservative": {
                "occ_thr": 3,
                "edge_thr": 4,
                "weight_thr": 0.5,
                "description": "ä¿å®ˆç­–ç•¥ - ä¸¥æ ¼ç­›é€‰ï¼Œç¡®ä¿è¿æ¥è´¨é‡"
            },
            "moderate": {
                "occ_thr": 2,
                "edge_thr": 3,
                "weight_thr": 0.4,
                "description": "ä¸­ç­‰ç­–ç•¥ - å¹³è¡¡ç½‘ç»œå¤§å°å’Œè¿æ¥è´¨é‡"
            },
            "relaxed": {
                "occ_thr": 2,
                "edge_thr": 3,
                "weight_thr": 0.3,
                "description": "å®½æ¾ç­–ç•¥ - ä¿ç•™æ›´å¤šè¿æ¥ï¼Œå¢åŠ ç½‘ç»œè¦†ç›–"
            }
        }
        
        results = {}
        
        for strategy_name, params in strategies.items():
            print(f"\n=== æµ‹è¯•{strategy_name}ç­–ç•¥ ===")
            print(f"å‚æ•°: {params}")
            
            try:
                # åˆ›å»ºç½‘ç»œ
                flood_net = FloodBayesNetwork(t_window="D")
                flood_net.fit_marginal(self.train_df)
                
                # æ„å»ºç½‘ç»œ
                flood_net.build_network_by_co_occurrence(
                    self.train_df,
                    occ_thr=params["occ_thr"],
                    edge_thr=params["edge_thr"],
                    weight_thr=params["weight_thr"],
                    report=False
                )
                
                # åˆ†æç½‘ç»œç»Ÿè®¡
                network_stats = self.analyze_network_statistics(flood_net.network)
                edge_stats = self.analyze_edge_weights(flood_net.network)
                
                print(f"ç½‘ç»œå¤§å°: {network_stats['nodes']}èŠ‚ç‚¹, {network_stats['edges']}è¾¹")
                print(f"ç½‘ç»œå¯†åº¦: {network_stats['density']:.4f}")
                print(f"å¹³å‡åº¦: {network_stats['avg_degree']:.2f}")
                
                # å¦‚æœç½‘ç»œå¤ªå°ï¼Œè·³è¿‡æ€§èƒ½è¯„ä¼°
                if network_stats['nodes'] < 3:
                    print("âš ï¸ ç½‘ç»œå¤ªå°ï¼Œè·³è¿‡æ€§èƒ½è¯„ä¼°")
                    results[strategy_name] = {
                        "params": params,
                        "network_stats": network_stats,
                        "edge_stats": edge_stats,
                        "performance": None,
                        "status": "network_too_small"
                    }
                    continue
                
                # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡
                flood_net.fit_conditional(self.train_df, max_parents=2, alpha=1)
                flood_net.build_bayes_network()
                
                # æ€§èƒ½è¯„ä¼°
                performance = self.evaluate_bn_enhanced(flood_net, self.test_df)
                
                print(f"æ€§èƒ½æŒ‡æ ‡:")
                print(f"  ROC-AUC: {performance['ROC-AUC']:.4f}")
                print(f"  PR-AUC: {performance['PR-AUC']:.4f}")
                print(f"  F1: {performance['F1']:.4f}")
                print(f"  Precision: {performance['Precision']:.4f}")
                print(f"  Recall@5: {performance['Recall@5']:.4f}")
                
                results[strategy_name] = {
                    "params": params,
                    "network_stats": network_stats,
                    "edge_stats": edge_stats,
                    "performance": performance,
                    "status": "success"
                }
                
            except Exception as e:
                print(f"âŒ ç­–ç•¥{strategy_name}å¤±è´¥: {e}")
                results[strategy_name] = {
                    "params": params,
                    "network_stats": None,
                    "edge_stats": None,
                    "performance": None,
                    "status": f"failed: {str(e)}"
                }
        
        return results
        
    def generate_comparison_report(self, results):
        """ç”Ÿæˆè¯¦ç»†çš„æ¯”è¾ƒæŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("è´å¶æ–¯ç½‘ç»œå‚æ•°ä¼˜åŒ–æ¯”è¾ƒæŠ¥å‘Š")
        report.append("=" * 80)
        
        # ç­–ç•¥æ€»è§ˆ
        report.append(f"\nã€ç­–ç•¥æ€»è§ˆã€‘")
        for strategy, data in results.items():
            if data["status"] == "success":
                stats = data["network_stats"]
                perf = data["performance"]
                report.append(f"{strategy:12s}: {stats['nodes']:2d}èŠ‚ç‚¹, {stats['edges']:2d}è¾¹, "
                             f"ROC-AUC={perf['ROC-AUC']:.3f}, F1={perf['F1']:.3f}")
            else:
                report.append(f"{strategy:12s}: {data['status']}")
        
        # è¯¦ç»†åˆ†æ
        for strategy, data in results.items():
            if data["status"] != "success":
                continue
                
            report.append(f"\nã€{strategy.upper()}ç­–ç•¥è¯¦ç»†åˆ†æã€‘")
            
            # å‚æ•°è®¾ç½®
            params = data["params"]
            report.append(f"å‚æ•°è®¾ç½®: occ_thr={params['occ_thr']}, edge_thr={params['edge_thr']}, "
                         f"weight_thr={params['weight_thr']}")
            
            # ç½‘ç»œç»Ÿè®¡
            stats = data["network_stats"]
            report.append(f"ç½‘ç»œç»Ÿè®¡:")
            report.append(f"  - èŠ‚ç‚¹æ•°: {stats['nodes']}")
            report.append(f"  - è¾¹æ•°: {stats['edges']}")
            report.append(f"  - å¯†åº¦: {stats['density']:.4f}")
            report.append(f"  - å¹³å‡åº¦: {stats['avg_degree']:.2f}")
            report.append(f"  - æœ€å¤§åº¦: {stats['max_degree']}")
            report.append(f"  - å­¤ç«‹èŠ‚ç‚¹: {stats['isolated_nodes']}")
            
            # è¾¹æƒé‡åˆ†å¸ƒ
            edge_stats = data["edge_stats"]
            report.append(f"è¾¹æƒé‡åˆ†å¸ƒ:")
            report.append(f"  - å¹³å‡æƒé‡: {edge_stats['weight_mean']:.4f}")
            report.append(f"  - æƒé‡æ ‡å‡†å·®: {edge_stats['weight_std']:.4f}")
            report.append(f"  - æƒé‡èŒƒå›´: [{edge_stats['weight_min']:.4f}, {edge_stats['weight_max']:.4f}]")
            
            # æ€§èƒ½æŒ‡æ ‡
            perf = data["performance"]
            report.append(f"æ€§èƒ½æŒ‡æ ‡:")
            report.append(f"  - ROC-AUC: {perf['ROC-AUC']:.4f}")
            report.append(f"  - PR-AUC: {perf['PR-AUC']:.4f}")
            report.append(f"  - F1åˆ†æ•°: {perf['F1']:.4f}")
            report.append(f"  - ç²¾ç¡®ç‡: {perf['Precision']:.4f}")
            report.append(f"  - Recall@5: {perf['Recall@5']:.4f}")
            report.append(f"  - Brieråˆ†æ•°: {perf['Brier']:.4f}")
            report.append(f"  - æ ·æœ¬æ•°: {perf['Samples']}")
            
        # æ¨èç­–ç•¥
        report.append(f"\nã€æ¨èç­–ç•¥ã€‘")
        
        # æ ¹æ®ä¸åŒç›®æ ‡æ¨èç­–ç•¥
        successful_strategies = {k: v for k, v in results.items() if v["status"] == "success"}
        
        if successful_strategies:
            # æŒ‰ç½‘ç»œå¤§å°æ¨è
            size_suitable = {k: v for k, v in successful_strategies.items() 
                           if 10 <= v["network_stats"]["nodes"] <= 30}
            
            if size_suitable:
                best_f1 = max(size_suitable.keys(), 
                             key=lambda x: size_suitable[x]["performance"]["F1"])
                best_auc = max(size_suitable.keys(), 
                              key=lambda x: size_suitable[x]["performance"]["ROC-AUC"])
                
                report.append(f"âœ… æ¨èç”¨äºå®é™…åº”ç”¨: {best_f1}ç­–ç•¥")
                report.append(f"   - åŸå› : ç½‘ç»œå¤§å°åˆé€‚ä¸”F1åˆ†æ•°æœ€é«˜")
                report.append(f"   - ç½‘ç»œè§„æ¨¡: {successful_strategies[best_f1]['network_stats']['nodes']}èŠ‚ç‚¹")
                report.append(f"   - F1åˆ†æ•°: {successful_strategies[best_f1]['performance']['F1']:.4f}")
                
                if best_auc != best_f1:
                    report.append(f"ğŸ”„ å¤‡é€‰æ–¹æ¡ˆ: {best_auc}ç­–ç•¥")
                    report.append(f"   - åŸå› : ROC-AUCæœ€é«˜")
                    report.append(f"   - ROC-AUC: {successful_strategies[best_auc]['performance']['ROC-AUC']:.4f}")
            else:
                report.append("âš ï¸ æ²¡æœ‰ç­–ç•¥äº§ç”Ÿç†æƒ³çš„ç½‘ç»œå¤§å°(10-30èŠ‚ç‚¹)")
                report.append("   å»ºè®®è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°æˆ–è€ƒè™‘å…¶ä»–å»ºæ¨¡æ–¹æ³•")
        
        return "\n".join(report)
        
    def save_results(self, results, output_path="parameter_optimization_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        results_cleaned = convert_numpy(results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_cleaned, f, indent=2, ensure_ascii=False)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    def run_optimization(self):
        """è¿è¡Œå®Œæ•´çš„å‚æ•°ä¼˜åŒ–åˆ†æ"""
        print("å¼€å§‹è´å¶æ–¯ç½‘ç»œå‚æ•°ä¼˜åŒ–åˆ†æ...")
        
        # æµ‹è¯•å‚æ•°ç»„åˆ
        results = self.test_parameter_combinations()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_comparison_report(results)
        print("\n" + report)
        
        # ä¿å­˜ç»“æœ
        self.save_results(results)
        
        return results, report

def main():
    """ä¸»å‡½æ•°"""
    try:
        optimizer = ParameterOptimizer()
        results, report = optimizer.run_optimization()
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open("parameter_optimization_report.txt", 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("\n" + "="*60)
        print("å‚æ•°ä¼˜åŒ–åˆ†æå®Œæˆ!")
        print("è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: parameter_optimization_report.txt")
        print("JSONç»“æœå·²ä¿å­˜åˆ°: parameter_optimization_results.json")
        
    except Exception as e:
        print(f"å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()