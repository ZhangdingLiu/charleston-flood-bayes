#!/usr/bin/env python3
"""
Charlestonæ´ªæ°´è´å¶æ–¯ç½‘ç»œæµ‹è¯•é›†è¯„ä¼°è„šæœ¬

åŠŸèƒ½ï¼š
1. åŠ è½½ä¸main_clean.pyç›¸åŒçš„æ•°æ®åˆ†å‰²ï¼ˆä½¿ç”¨ç›¸åŒéšæœºç§å­ï¼‰
2. é‡å»ºä¼˜åŒ–åçš„è´å¶æ–¯ç½‘ç»œ
3. åœ¨æµ‹è¯•é›†ä¸Šç³»ç»Ÿè¯„ä¼°ç½‘ç»œæ€§èƒ½
4. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–

ç”¨æ³•ï¼š
    python evaluate_testset.py

è¾“å‡ºï¼š
    - ç»ˆç«¯è¾“å‡ºè¯„ä¼°æŒ‡æ ‡è¡¨æ ¼
    - results/test_metrics.json - è¯¦ç»†è¯„ä¼°ç»“æœ
    - figs/confusion_matrix.png - æ··æ·†çŸ©é˜µçƒ­å›¾
    - figs/metric_bar.png - æŒ‡æ ‡æŸ±çŠ¶å›¾

æ³¨æ„ï¼š
    ç¡®ä¿å·²è¿è¡Œmain_clean.pyç”Ÿæˆç½‘ç»œæ–‡ä»¶
"""

import random
import numpy as np
import pandas as pd
import json
import os
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any

# æœºå™¨å­¦ä¹ å’Œè¯„ä¼°
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score, average_precision_score,
    brier_score_loss, classification_report
)

# å¯è§†åŒ–
import matplotlib.pyplot as plt
import seaborn as sns

# è´å¶æ–¯ç½‘ç»œ
from model import FloodBayesNetwork

# è®¾ç½®éšæœºç§å­ï¼ˆä¸main_clean.pyä¿æŒä¸€è‡´ï¼‰
RANDOM_SEED = 42
random.seed(0)
np.random.seed(0)

class TestSetEvaluator:
    """æµ‹è¯•é›†è¯„ä¼°å™¨"""
    
    def __init__(self, 
                 network_csv_path="charleston_flood_network.csv",
                 data_csv_path="Road_Closures_2024.csv",
                 results_dir="results",
                 figs_dir="figs"):
        self.network_csv_path = network_csv_path
        self.data_csv_path = data_csv_path
        self.results_dir = results_dir
        self.figs_dir = figs_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(figs_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®å’Œæ¨¡å‹
        self.train_df = None
        self.test_df = None
        self.flood_net = None
        self.network_nodes = set()
        
        # è¯„ä¼°ç»“æœ
        self.y_true = []
        self.y_prob = []
        self.y_pred = []
        self.evaluation_details = []
        
    def load_and_split_data(self):
        """åŠ è½½æ•°æ®å¹¶æŒ‰ç›¸åŒæ–¹å¼åˆ†å‰²ï¼ˆä¸main_clean.pyä¿æŒä¸€è‡´ï¼‰"""
        print("1. åŠ è½½å’Œåˆ†å‰²æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_csv_path)
        df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        
        # æ•°æ®é¢„å¤„ç†ï¼ˆä¸main_clean.pyå®Œå…¨ä¸€è‡´ï¼‰
        df["time_create"] = pd.to_datetime(df["START"], utc=True)
        df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
        df["link_id"] = df["link_id"].astype(str)
        df["id"] = df["OBJECTID"].astype(str)
        
        # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­åˆ†å‰²
        self.train_df, self.test_df = train_test_split(
            df, test_size=0.3, random_state=RANDOM_SEED
        )
        
        print(f"   æ€»æ´ªæ°´è®°å½•: {len(df)}")
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡")
        
        return self.train_df, self.test_df
        
    def rebuild_bayesian_network(self):
        """é‡å»ºè´å¶æ–¯ç½‘ç»œï¼ˆä½¿ç”¨ä¸main_clean.pyç›¸åŒçš„å‚æ•°ï¼‰"""
        print("2. é‡å»ºè´å¶æ–¯ç½‘ç»œ...")
        
        # åˆ›å»ºç½‘ç»œå®ä¾‹
        self.flood_net = FloodBayesNetwork(t_window="D")
        
        # æ‹Ÿåˆè¾¹é™…æ¦‚ç‡
        self.flood_net.fit_marginal(self.train_df)
        
        # ä½¿ç”¨ä¼˜åŒ–å‚æ•°æ„å»ºç½‘ç»œ
        self.flood_net.build_network_by_co_occurrence(
            self.train_df,
            occ_thr=10,    # ä¸main_clean.pyä¸€è‡´
            edge_thr=3,
            weight_thr=0.4,
            report=False
        )
        
        # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡
        self.flood_net.fit_conditional(self.train_df, max_parents=2, alpha=1)
        
        # æ„å»ºæœ€ç»ˆè´å¶æ–¯ç½‘ç»œ
        try:
            self.flood_net.build_bayes_network()
            self.flood_net.check_bayesian_network()
            print("   âœ… è´å¶æ–¯ç½‘ç»œé‡å»ºæˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸ è´å¶æ–¯ç½‘ç»œæ„å»ºè­¦å‘Š: {e}")
            print("   ç»§ç»­ä½¿ç”¨ç½‘ç»œç»“æ„è¿›è¡Œè¯„ä¼°")
        
        # è·å–ç½‘ç»œèŠ‚ç‚¹
        self.network_nodes = set(self.flood_net.network.nodes())
        print(f"   ç½‘ç»œèŠ‚ç‚¹æ•°: {len(self.network_nodes)}")
        
        return self.flood_net
        
    def get_road_frequencies(self):
        """è·å–è®­ç»ƒé›†ä¸­çš„é“è·¯é¢‘æ¬¡"""
        return Counter(self.train_df['link_id'])
        
    def evaluate_on_test_set(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°"""
        print("3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        
        road_frequencies = self.get_road_frequencies()
        
        # æŒ‰æ—¥æœŸåˆ†ç»„æµ‹è¯•é›†
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        
        total_days = len(test_by_date)
        evaluated_days = 0
        
        for date, day_group in test_by_date:
            # å½“å¤©æ´ªæ°´é“è·¯
            flooded_roads = set(day_group["link_id"].unique())
            
            # è¿‡æ»¤ï¼šåªè€ƒè™‘åœ¨ç½‘ç»œä¸­çš„é“è·¯
            flooded_in_network = flooded_roads & self.network_nodes
            
            if len(flooded_in_network) == 0:
                continue  # è·³è¿‡æ²¡æœ‰ç½‘ç»œèŠ‚ç‚¹çš„æ—¥æœŸ
                
            # é€‰æ‹©evidenceï¼šé¢‘æ¬¡â‰¥2ä¸”åœ¨ç½‘ç»œä¸­çš„é“è·¯
            potential_evidence = {road for road in flooded_in_network 
                                if road_frequencies.get(road, 0) >= 2}
            
            if len(potential_evidence) == 0:
                continue  # è·³è¿‡æ²¡æœ‰åˆé€‚evidenceçš„æ—¥æœŸ
                
            # ä½¿ç”¨å‰3ä¸ªä½œä¸ºevidenceï¼ˆæˆ–æ‰€æœ‰å¦‚æœå°‘äº3ä¸ªï¼‰
            evidence_roads = list(potential_evidence)[:3]
            evidence = {road: 1 for road in evidence_roads}
            
            # ç›®æ ‡é“è·¯ï¼šç½‘ç»œä¸­çš„å…¶ä»–é“è·¯
            target_roads = self.network_nodes - set(evidence_roads)
            
            if len(target_roads) == 0:
                continue  # è·³è¿‡æ²¡æœ‰ç›®æ ‡é“è·¯çš„æ—¥æœŸ
                
            # å¯¹æ¯ä¸ªç›®æ ‡é“è·¯è¿›è¡Œé¢„æµ‹
            for road in target_roads:
                try:
                    # è·å–æ´ªæ°´æ¦‚ç‡
                    result = self.flood_net.infer_w_evidence(road, evidence)
                    prob_flood = result.get("flooded", 0.5)  # é»˜è®¤0.5å¦‚æœæ¨ç†å¤±è´¥
                    
                    # çœŸå®æ ‡ç­¾
                    true_flood = 1 if road in flooded_roads else 0
                    
                    # é¢„æµ‹æ ‡ç­¾ï¼ˆé˜ˆå€¼0.5ï¼‰
                    pred_flood = 1 if prob_flood >= 0.5 else 0
                    
                    # è®°å½•ç»“æœ
                    self.y_true.append(true_flood)
                    self.y_prob.append(prob_flood)
                    self.y_pred.append(pred_flood)
                    
                    # è¯¦ç»†è®°å½•
                    self.evaluation_details.append({
                        "date": str(date.date()),
                        "target_road": road,
                        "evidence": evidence.copy(),
                        "true_flood": true_flood,
                        "prob_flood": prob_flood,
                        "pred_flood": pred_flood
                    })
                    
                except Exception as e:
                    print(f"   âš ï¸ æ¨ç†å¤±è´¥ - é“è·¯: {road}, é”™è¯¯: {e}")
                    continue
                    
            evaluated_days += 1
            
        print(f"   è¯„ä¼°æ ·æœ¬æ•°: {len(self.y_true)}")
        print(f"   è¯„ä¼°å¤©æ•°: {evaluated_days}/{total_days}")
        
        if len(self.y_true) == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œç½‘ç»œè®¾ç½®")
            
    def calculate_metrics(self) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print("4. è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        if len(self.y_true) == 0:
            raise ValueError("æ²¡æœ‰è¯„ä¼°æ•°æ®")
            
        # åŸºæœ¬åˆ†ç±»æŒ‡æ ‡
        accuracy = accuracy_score(self.y_true, self.y_pred)
        precision = precision_score(self.y_true, self.y_pred, zero_division=0)
        recall = recall_score(self.y_true, self.y_pred, zero_division=0)
        f1 = f1_score(self.y_true, self.y_pred, zero_division=0)
        
        # æ¦‚ç‡æŒ‡æ ‡
        try:
            roc_auc = roc_auc_score(self.y_true, self.y_prob) if len(set(self.y_true)) > 1 else 0.5
            pr_auc = average_precision_score(self.y_true, self.y_prob) if len(set(self.y_true)) > 1 else np.mean(self.y_true)
            brier = brier_score_loss(self.y_true, self.y_prob)
        except ValueError:
            roc_auc = pr_auc = brier = 0.0
            
        # æ•°æ®ç»Ÿè®¡
        positive_ratio = np.mean(self.y_true)
        total_samples = len(self.y_true)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_true, self.y_pred)
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
        
        metrics = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "roc_auc": roc_auc,
            "pr_auc": pr_auc,
            "brier_score": brier,
            "positive_ratio": positive_ratio,
            "total_samples": total_samples,
            "true_positives": int(tp),
            "true_negatives": int(tn),
            "false_positives": int(fp),
            "false_negatives": int(fn)
        }
        
        print(f"   è®¡ç®—å®Œæˆ - {total_samples}ä¸ªæ ·æœ¬")
        return metrics
        
    def print_metrics_table(self, metrics: Dict[str, float]):
        """æ‰“å°æ•´æ´çš„æŒ‡æ ‡è¡¨æ ¼"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ")
        print("="*60)
        
        # ä¸»è¦æŒ‡æ ‡
        print(f"{'æŒ‡æ ‡':<15} {'å€¼':<10} {'è¯´æ˜'}")
        print("-" * 50)
        print(f"{'Accuracy':<15} {metrics['accuracy']:<10.4f} æ•´ä½“å‡†ç¡®ç‡")
        print(f"{'Precision':<15} {metrics['precision']:<10.4f} ç²¾ç¡®ç‡")
        print(f"{'Recall':<15} {metrics['recall']:<10.4f} å¬å›ç‡")
        print(f"{'F1-Score':<15} {metrics['f1_score']:<10.4f} F1åˆ†æ•°")
        print(f"{'ROC-AUC':<15} {metrics['roc_auc']:<10.4f} ROCæ›²çº¿ä¸‹é¢ç§¯")
        print(f"{'PR-AUC':<15} {metrics['pr_auc']:<10.4f} PRæ›²çº¿ä¸‹é¢ç§¯")
        print(f"{'Brier Score':<15} {metrics['brier_score']:<10.4f} æ¦‚ç‡å‡†ç¡®æ€§")
        
        # æ•°æ®ç»Ÿè®¡
        print(f"\n{'æ•°æ®ç»Ÿè®¡':<15} {'å€¼':<10} {'è¯´æ˜'}")
        print("-" * 50)
        print(f"{'æ€»æ ·æœ¬':<15} {metrics['total_samples']:<10} è¯„ä¼°æ ·æœ¬æ•°")
        print(f"{'æ­£æ ·æœ¬æ¯”ä¾‹':<15} {metrics['positive_ratio']:<10.4f} æ´ªæ°´äº‹ä»¶æ¯”ä¾‹")
        print(f"{'True Pos':<15} {metrics['true_positives']:<10} æ­£ç¡®é¢„æµ‹æ´ªæ°´")
        print(f"{'True Neg':<15} {metrics['true_negatives']:<10} æ­£ç¡®é¢„æµ‹æ— æ´ªæ°´")
        print(f"{'False Pos':<15} {metrics['false_positives']:<10} è¯¯æŠ¥æ´ªæ°´")
        print(f"{'False Neg':<15} {metrics['false_negatives']:<10} æ¼æŠ¥æ´ªæ°´")
        
    def save_results(self, metrics: Dict[str, float]):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°JSON"""
        print("5. ä¿å­˜è¯„ä¼°ç»“æœ...")
        
        # å®Œæ•´ç»“æœ
        results = {
            "evaluation_summary": {
                "timestamp": pd.Timestamp.now().isoformat(),
                "network_file": self.network_csv_path,
                "data_file": self.data_csv_path,
                "train_samples": len(self.train_df),
                "test_samples": len(self.test_df),
                "evaluation_samples": len(self.y_true)
            },
            "metrics": metrics,
            "network_info": {
                "nodes": len(self.network_nodes),
                "edges": self.flood_net.network.number_of_edges(),
                "network_nodes": sorted(list(self.network_nodes))
            },
            "evaluation_details": self.evaluation_details[:100]  # åªä¿å­˜å‰100ä¸ªè¯¦ç»†è®°å½•
        }
        
        # ä¿å­˜åˆ°JSON
        results_path = os.path.join(self.results_dir, "test_metrics.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
            
        print(f"   âœ… ç»“æœä¿å­˜åˆ°: {results_path}")
        
    def create_visualizations(self, metrics: Dict[str, float]):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
        plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. æ··æ·†çŸ©é˜µçƒ­å›¾
        self._create_confusion_matrix()
        
        # 2. æŒ‡æ ‡æŸ±çŠ¶å›¾
        self._create_metrics_bar_chart(metrics)
        
        print("   âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        
    def _create_confusion_matrix(self):
        """åˆ›å»ºæ··æ·†çŸ©é˜µçƒ­å›¾"""
        cm = confusion_matrix(self.y_true, self.y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['No Flood', 'Flood'],
                   yticklabels=['No Flood', 'Flood'])
        plt.title('Confusion Matrix - Test Set Evaluation', fontsize=14, fontweight='bold')
        plt.xlabel('Predicted', fontsize=12)
        plt.ylabel('Actual', fontsize=12)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        total = cm.sum()
        accuracy = (cm[0,0] + cm[1,1]) / total if total > 0 else 0
        plt.figtext(0.02, 0.02, f'Total Samples: {total}, Accuracy: {accuracy:.3f}', 
                   fontsize=10, ha='left')
        
        plt.tight_layout()
        
        # ä¿å­˜
        cm_path = os.path.join(self.figs_dir, "confusion_matrix.png")
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“Š æ··æ·†çŸ©é˜µä¿å­˜åˆ°: {cm_path}")
        
    def _create_metrics_bar_chart(self, metrics: Dict[str, float]):
        """åˆ›å»ºæŒ‡æ ‡æŸ±çŠ¶å›¾"""
        # é€‰æ‹©ä¸»è¦æŒ‡æ ‡
        main_metrics = {
            'Accuracy': metrics['accuracy'],
            'Precision': metrics['precision'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1_score'],
            'ROC-AUC': metrics['roc_auc'],
            'PR-AUC': metrics['pr_auc']
        }
        
        plt.figure(figsize=(12, 8))
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        bars = plt.bar(main_metrics.keys(), main_metrics.values(), 
                      color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'],
                      alpha=0.8, edgecolor='black', linewidth=1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, main_metrics.values()):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.title('Test Set Evaluation Metrics', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('Score', fontsize=12)
        plt.ylim(0, 1.1)
        plt.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        plt.figtext(0.02, 0.02, 
                   f'Total Samples: {metrics["total_samples"]}, '
                   f'Positive Ratio: {metrics["positive_ratio"]:.3f}',
                   fontsize=10, ha='left')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        # ä¿å­˜
        bar_path = os.path.join(self.figs_dir, "metric_bar.png")
        plt.savefig(bar_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“ˆ æŒ‡æ ‡æŸ±çŠ¶å›¾ä¿å­˜åˆ°: {bar_path}")
        
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        print("="*60)
        
        try:
            # 1. æ•°æ®å‡†å¤‡
            self.load_and_split_data()
            
            # 2. é‡å»ºç½‘ç»œ
            self.rebuild_bayesian_network()
            
            # 3. æµ‹è¯•é›†è¯„ä¼°
            self.evaluate_on_test_set()
            
            # 4. è®¡ç®—æŒ‡æ ‡
            metrics = self.calculate_metrics()
            
            # 5. è¾“å‡ºç»“æœ
            self.print_metrics_table(metrics)
            
            # 6. ä¿å­˜ç»“æœ
            self.save_results(metrics)
            
            # 7. ç”Ÿæˆå¯è§†åŒ–
            self.create_visualizations(metrics)
            
            print("\n" + "="*60)
            print("âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ è¯¦ç»†ç»“æœ: {self.results_dir}/test_metrics.json")
            print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: {self.figs_dir}/")
            print("="*60)
            
            return metrics
            
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = TestSetEvaluator()
    
    # è¿è¡Œè¯„ä¼°
    metrics = evaluator.run_evaluation()
    
    if metrics:
        print("\nğŸ¯ è¯„ä¼°æˆåŠŸå®Œæˆï¼")
        print(f"ä¸»è¦æŒ‡æ ‡ - F1: {metrics['f1_score']:.3f}, "
              f"Accuracy: {metrics['accuracy']:.3f}, "
              f"ROC-AUC: {metrics['roc_auc']:.3f}")
    else:
        print("\nâŒ è¯„ä¼°å¤±è´¥")

if __name__ == "__main__":
    main()