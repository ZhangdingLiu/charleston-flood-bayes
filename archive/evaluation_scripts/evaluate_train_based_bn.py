#!/usr/bin/env python3
"""
è¯„ä¼°åŸºäºè®­ç»ƒé›†æ„å»ºçš„è´å¶æ–¯ç½‘ç»œ

åŠŸèƒ½ï¼š
1. åŠ è½½train_based_bn.pklæ¨¡å‹æ–‡ä»¶
2. åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œevidence-basedæ¨ç†
3. æµ‹è¯•å¤šä¸ªæ¦‚ç‡é˜ˆå€¼çš„æ€§èƒ½
4. ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Šå’Œæ··æ·†çŸ©é˜µå¯è§†åŒ–
5. å¯¹æ¯”è®­ç»ƒé›†ç»Ÿè®¡ä¸æµ‹è¯•é›†å®é™…è¡¨ç°

ç½‘ç»œç»“æ„ï¼š
   PITT ST â†’ ASHLEY AVE
   HARLESTON VILLAGE â†’ WASHINGTON ST
   AIKEN ST â†’ CALHOUN ST

è¯„ä¼°ç­–ç•¥ï¼š
- Evidence: æµ‹è¯•é›†ä¸­å½“å¤©è§‚æµ‹åˆ°æ´ªæ°´ä¸”åœ¨ç½‘ç»œèŠ‚ç‚¹å†…çš„é“è·¯
- é˜ˆå€¼æµ‹è¯•: 0.2, 0.3, 0.4, 0.5, 0.6, 0.7
- æŒ‡æ ‡: Precision, Recall, F1, Accuracy, æ··æ·†çŸ©é˜µ

ç”¨æ³•ï¼š
    python evaluate_train_based_bn.py

è¾“å‡ºï¼š
    - results/train_based_bn_metrics.json - è¯¦ç»†è¯„ä¼°ç»“æœ
    - figs/train_based_confmat.png - æ··æ·†çŸ©é˜µå¯è§†åŒ–
    - ç»ˆç«¯è¾“å‡ºä¸åŒé˜ˆå€¼çš„æ€§èƒ½å¯¹æ¯”
"""

import pandas as pd
import numpy as np
import pickle
import json
import os
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    precision_score, recall_score, f1_score, confusion_matrix,
    accuracy_score, classification_report
)

# è´å¶æ–¯ç½‘ç»œæ¨ç†
try:
    from pgmpy.inference import VariableElimination
    try:
        from pgmpy.models import DiscreteBayesianNetwork as BayesianNetwork
    except ImportError:
        from pgmpy.models import BayesianNetwork
except ImportError:
    print("è¯·å®‰è£…pgmpy: pip install pgmpy")
    exit(1)

# è®¾ç½®éšæœºç§å­
RANDOM_SEED = 42

class TrainBasedBayesianNetworkEvaluator:
    """åŸºäºè®­ç»ƒé›†çš„è´å¶æ–¯ç½‘ç»œè¯„ä¼°å™¨"""
    
    def __init__(self, 
                 model_path="train_based_bn.pkl",
                 data_csv_path="Road_Closures_2024.csv",
                 results_dir="results",
                 figs_dir="figs"):
        self.model_path = model_path
        self.data_csv_path = data_csv_path
        self.results_dir = results_dir
        self.figs_dir = figs_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(results_dir, exist_ok=True)
        os.makedirs(figs_dir, exist_ok=True)
        
        # è¯„ä¼°å‚æ•°
        self.prob_thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
        
        # æ•°æ®å’Œæ¨¡å‹
        self.train_df = None
        self.test_df = None
        self.model_data = None
        self.bayesian_network = None
        self.inference_engine = None
        self.selected_roads = None
        self.node_mapping = None
        
        # é¢„æµ‹ç»“æœç¼“å­˜
        self.prediction_cache = []
        
    def load_and_split_data(self):
        """åŠ è½½æ•°æ®å¹¶åˆ†å‰²ï¼ˆä¸æ„å»ºè„šæœ¬ä¿æŒä¸€è‡´ï¼‰"""
        print("1. åŠ è½½å’Œåˆ†å‰²æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_csv_path)
        df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        
        # æ•°æ®é¢„å¤„ç†
        df["time_create"] = pd.to_datetime(df["START"], utc=True)
        df["road"] = df["STREET"].str.upper().str.strip()
        df["date"] = df["time_create"].dt.floor("D")
        df["id"] = df["OBJECTID"].astype(str)
        
        # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­åˆ†å‰²
        self.train_df, self.test_df = train_test_split(
            df, test_size=0.3, random_state=RANDOM_SEED
        )
        
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡")
        
        return self.train_df, self.test_df
        
    def load_model(self):
        """åŠ è½½è´å¶æ–¯ç½‘ç»œæ¨¡å‹"""
        print("2. åŠ è½½è´å¶æ–¯ç½‘ç»œæ¨¡å‹...")
        
        try:
            with open(self.model_path, 'rb') as f:
                self.model_data = pickle.load(f)
            
            self.bayesian_network = self.model_data['bayesian_network']
            self.selected_roads = self.model_data['selected_roads']
            self.node_mapping = self.model_data['node_mapping']
            
            # åˆ›å»ºæ¨ç†å¼•æ“
            self.inference_engine = VariableElimination(self.bayesian_network)
            
            print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   é€‰å®šé“è·¯: {self.selected_roads}")
            print(f"   ç½‘ç»œèŠ‚ç‚¹: {len(self.bayesian_network.nodes())}")
            print(f"   ç½‘ç»œè¾¹: {len(self.bayesian_network.edges())}")
            
            # æ˜¾ç¤ºç½‘ç»œç»“æ„
            print(f"   ç½‘ç»œç»“æ„:")
            for parent, child, conf in self.model_data['network_edges']:
                print(f"     {parent} â†’ {child} (è®­ç»ƒé›†æ¡ä»¶æ¦‚ç‡: {conf:.4f})")
            
            return True
            
        except FileNotFoundError:
            print(f"   âŒ æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_path}")
            print("   è¯·å…ˆè¿è¡Œ build_train_based_bn.py ç”Ÿæˆæ¨¡å‹")
            return False
        except Exception as e:
            print(f"   âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False
            
    def compute_test_predictions(self):
        """è®¡ç®—æµ‹è¯•é›†é¢„æµ‹ç»“æœ"""
        print("3. è®¡ç®—æµ‹è¯•é›†é¢„æµ‹...")
        
        # è¿‡æ»¤æµ‹è¯•é›†æ•°æ®
        test_filtered = self.test_df[self.test_df['road'].isin(self.selected_roads)].copy()
        
        print(f"   æµ‹è¯•é›†ä¸­é€‰å®šé“è·¯çš„è®°å½•: {len(test_filtered)}æ¡")
        
        # æ˜¾ç¤ºæµ‹è¯•é›†ä¸­å„é“è·¯çš„é¢‘ç‡
        print(f"   æµ‹è¯•é›†ä¸­å„é“è·¯å‡ºç°æ¬¡æ•°:")
        for road in self.selected_roads:
            count = len(test_filtered[test_filtered['road'] == road])
            print(f"     {road}: {count}æ¬¡")
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        test_by_date = test_filtered.groupby(test_filtered["date"])
        
        total_predictions = 0
        evaluated_days = 0
        days_with_evidence = 0
        
        for date, day_group in test_by_date:
            # å½“å¤©æ´ªæ°´é“è·¯
            flooded_roads = set(day_group["road"].unique())
            
            # åœ¨é€‰å®šé“è·¯ä¸­çš„æ´ªæ°´é“è·¯
            flooded_selected = flooded_roads & set(self.selected_roads)
            
            if len(flooded_selected) == 0:
                continue  # æ²¡æœ‰é€‰å®šé“è·¯æ´ªæ°´ï¼Œè·³è¿‡
            
            evaluated_days += 1
            
            # æ„å»ºevidenceï¼ˆè§‚æµ‹åˆ°çš„æ´ªæ°´é“è·¯ï¼‰
            evidence = {}
            for road in flooded_selected:
                node_name = self.node_mapping[road]
                evidence[node_name] = 1  # æ´ªæ°´çŠ¶æ€
            
            if len(evidence) > 0:
                days_with_evidence += 1
            
            # å¯¹æ‰€æœ‰é€‰å®šé“è·¯è¿›è¡Œé¢„æµ‹
            day_predictions = []
            for road in self.selected_roads:
                node_name = self.node_mapping[road]
                
                # å¦‚æœè¯¥é“è·¯å·²åœ¨evidenceä¸­ï¼Œè·³è¿‡é¢„æµ‹
                if node_name in evidence:
                    continue
                
                try:
                    # ä½¿ç”¨è´å¶æ–¯æ¨ç†è®¡ç®—æ¦‚ç‡
                    if len(evidence) > 0:
                        query_result = self.inference_engine.query(
                            variables=[node_name], 
                            evidence=evidence
                        )
                        prob_flood = query_result.values[1]  # P(flood=1)
                    else:
                        # æ²¡æœ‰evidenceï¼Œä½¿ç”¨å…ˆéªŒæ¦‚ç‡
                        query_result = self.inference_engine.query(variables=[node_name])
                        prob_flood = query_result.values[1]
                    
                    # çœŸå®æ ‡ç­¾
                    true_flood = 1 if road in flooded_roads else 0
                    
                    day_predictions.append({
                        "date": str(date.date()),
                        "road": road,
                        "node_name": node_name,
                        "prob_flood": prob_flood,
                        "true_flood": true_flood,
                        "evidence": evidence.copy(),
                        "evidence_roads": [road for road in self.selected_roads 
                                         if self.node_mapping[road] in evidence]
                    })
                    total_predictions += 1
                    
                except Exception as e:
                    print(f"   âš ï¸ æ¨ç†å¤±è´¥ - {road}: {e}")
                    # ä½¿ç”¨é»˜è®¤æ¦‚ç‡
                    day_predictions.append({
                        "date": str(date.date()),
                        "road": road,
                        "node_name": node_name,
                        "prob_flood": 0.5,
                        "true_flood": 1 if road in flooded_roads else 0,
                        "evidence": evidence.copy(),
                        "evidence_roads": []
                    })
                    total_predictions += 1
            
            if len(day_predictions) > 0:
                self.prediction_cache.extend(day_predictions)
        
        print(f"   é¢„æµ‹æ ·æœ¬: {total_predictions}ä¸ª")
        print(f"   æœ‰æ´ªæ°´çš„è¯„ä¼°å¤©æ•°: {evaluated_days}å¤©")
        print(f"   æœ‰evidenceçš„å¤©æ•°: {days_with_evidence}å¤©")
        
        if total_predictions == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹æ ·æœ¬")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªé¢„æµ‹æ ·æœ¬
        print(f"   å‰5ä¸ªé¢„æµ‹æ ·æœ¬:")
        for i, pred in enumerate(self.prediction_cache[:5]):
            print(f"     {pred['date']}: {pred['road']} "
                  f"(prob={pred['prob_flood']:.3f}, true={pred['true_flood']}, "
                  f"evidence={pred['evidence_roads']})")
            
    def evaluate_thresholds(self):
        """è¯„ä¼°ä¸åŒæ¦‚ç‡é˜ˆå€¼çš„æ€§èƒ½"""
        print("4. è¯„ä¼°ä¸åŒæ¦‚ç‡é˜ˆå€¼...")
        
        results = {}
        
        for threshold in self.prob_thresholds:
            print(f"   è¯„ä¼°é˜ˆå€¼ {threshold}...")
            
            # ç”Ÿæˆé¢„æµ‹æ ‡ç­¾
            y_true = []
            y_pred = []
            y_prob = []
            
            for pred in self.prediction_cache:
                y_true.append(pred["true_flood"])
                y_prob.append(pred["prob_flood"])
                y_pred.append(1 if pred["prob_flood"] >= threshold else 0)
            
            if len(y_true) == 0:
                continue
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (len(y_true), 0, 0, 0)
            
            results[threshold] = {
                "threshold": threshold,
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1": f1,
                "confusion_matrix": cm.tolist(),
                "true_positives": int(tp),
                "true_negatives": int(tn),
                "false_positives": int(fp),
                "false_negatives": int(fn),
                "total_samples": len(y_true),
                "positive_samples": sum(y_true)
            }
        
        return results
        
    def print_evaluation_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print("5. æ˜¾ç¤ºè¯„ä¼°ç»“æœ...")
        
        print(f"\n{'='*80}")
        print("ğŸ“Š åŸºäºè®­ç»ƒé›†çš„è´å¶æ–¯ç½‘ç»œè¯„ä¼°ç»“æœ")
        print(f"{'='*80}")
        
        # ç½‘ç»œä¿¡æ¯
        print(f"ç½‘ç»œç»“æ„:")
        print(f"  èŠ‚ç‚¹: {self.selected_roads}")
        print(f"  è¾¹: {[(edge[0], edge[1]) for edge in self.model_data['network_edges']]}")
        
        # æµ‹è¯•é›†ç»Ÿè®¡
        total_positive = sum(pred["true_flood"] for pred in self.prediction_cache)
        total_samples = len(self.prediction_cache)
        print(f"\næµ‹è¯•é›†ç»Ÿè®¡:")
        print(f"  æ€»é¢„æµ‹æ ·æœ¬: {total_samples}")
        print(f"  æ­£æ ·æœ¬(æ´ªæ°´): {total_positive} ({total_positive/total_samples*100:.1f}%)")
        print(f"  è´Ÿæ ·æœ¬(æ— æ´ªæ°´): {total_samples-total_positive} ({(total_samples-total_positive)/total_samples*100:.1f}%)")
        
        # æ€§èƒ½å¯¹æ¯”è¡¨
        print(f"\nä¸åŒæ¦‚ç‡é˜ˆå€¼æ€§èƒ½å¯¹æ¯”:")
        print(f"{'é˜ˆå€¼':<6} {'å‡†ç¡®ç‡':<8} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'TP':<4} {'TN':<4} {'FP':<4} {'FN':<4}")
        print("-" * 80)
        
        for threshold in self.prob_thresholds:
            if threshold in results:
                r = results[threshold]
                print(f"{threshold:<6.1f} {r['accuracy']:<8.4f} {r['precision']:<8.4f} "
                      f"{r['recall']:<8.4f} {r['f1']:<8.4f} "
                      f"{r['true_positives']:<4} {r['true_negatives']:<4} "
                      f"{r['false_positives']:<4} {r['false_negatives']:<4}")
        
        # æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
        valid_results = {k: v for k, v in results.items() if v['f1'] > 0}
        if valid_results:
            best_threshold = max(valid_results.keys(), key=lambda x: valid_results[x]['f1'])
            best_result = valid_results[best_threshold]
            print(f"\nğŸ¯ æœ€ä½³é˜ˆå€¼: {best_threshold} (F1: {best_result['f1']:.4f})")
        else:
            # å¦‚æœæ²¡æœ‰F1>0çš„ç»“æœï¼Œé€‰æ‹©å‡†ç¡®ç‡æœ€é«˜çš„
            best_threshold = max(results.keys(), key=lambda x: results[x]['accuracy'])
            best_result = results[best_threshold]
            print(f"\nğŸ¯ æœ€ä½³é˜ˆå€¼: {best_threshold} (å‡†ç¡®ç‡: {best_result['accuracy']:.4f})")
        
        # è¯¦ç»†ç»“æœ
        print(f"\næœ€ä½³é˜ˆå€¼è¯¦ç»†ç»“æœ:")
        print(f"  å‡†ç¡®ç‡: {best_result['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡: {best_result['precision']:.4f}")
        print(f"  å¬å›ç‡: {best_result['recall']:.4f}")
        print(f"  F1åˆ†æ•°: {best_result['f1']:.4f}")
        
        print(f"\næ··æ·†çŸ©é˜µ (é˜ˆå€¼ {best_threshold}):")
        cm = np.array(best_result['confusion_matrix'])
        print(f"              é¢„æµ‹")
        print(f"           æ— æ´ªæ°´  æ´ªæ°´")
        
        # å¤„ç†æ··æ·†çŸ©é˜µçš„ä¸åŒå½¢çŠ¶
        if cm.shape == (1, 1):
            # åªæœ‰ä¸€ä¸ªç±»åˆ«çš„æƒ…å†µ
            print(f"å®é™… æ— æ´ªæ°´   {cm[0,0]:4d}   0")
            print(f"     æ´ªæ°´     0      0")
        elif cm.shape == (2, 2):
            # æ ‡å‡†2x2æ··æ·†çŸ©é˜µ
            print(f"å®é™… æ— æ´ªæ°´   {cm[0,0]:4d}   {cm[0,1]:4d}")
            print(f"     æ´ªæ°´     {cm[1,0]:4d}   {cm[1,1]:4d}")
        else:
            # å…¶ä»–æƒ…å†µ
            print(f"æ··æ·†çŸ©é˜µå½¢çŠ¶: {cm.shape}")
            print(cm)
        
        return best_threshold, best_result
        
    def create_confusion_matrix_visualization(self, results, best_threshold):
        """åˆ›å»ºæ··æ·†çŸ©é˜µå¯è§†åŒ–"""
        print("6. ç”Ÿæˆæ··æ·†çŸ©é˜µå¯è§†åŒ–...")
        
        # åˆ›å»ºå­å›¾ï¼šå±•ç¤ºæ‰€æœ‰é˜ˆå€¼çš„æ··æ·†çŸ©é˜µ
        n_thresholds = len(self.prob_thresholds)
        cols = 3
        rows = (n_thresholds + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))
        fig.suptitle('Train-Based Bayesian Network - Confusion Matrices', 
                    fontsize=16, fontweight='bold')
        
        axes = axes.flatten() if n_thresholds > 1 else [axes]
        
        for i, threshold in enumerate(self.prob_thresholds):
            if threshold not in results:
                continue
                
            ax = axes[i] if i < len(axes) else None
            if ax is None:
                break
            
            cm_data = np.array(results[threshold]['confusion_matrix'])
            
            # åˆ›å»ºçƒ­å›¾
            sns.heatmap(cm_data, annot=True, fmt='d', cmap='Blues',
                       xticklabels=['No Flood', 'Flood'],
                       yticklabels=['No Flood', 'Flood'],
                       ax=ax, cbar=False)
            
            # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
            title = f'Threshold {threshold:.1f}'
            if threshold == best_threshold:
                title += ' â˜… Best'
            ax.set_title(title, fontweight='bold')
            ax.set_xlabel('Predicted')
            ax.set_ylabel('Actual')
            
            # æ·»åŠ æŒ‡æ ‡ä¿¡æ¯
            f1 = results[threshold]['f1']
            precision = results[threshold]['precision']
            recall = results[threshold]['recall']
            
            ax.text(0.5, -0.1, f'F1: {f1:.3f}, P: {precision:.3f}, R: {recall:.3f}',
                   transform=ax.transAxes, ha='center', va='top', fontsize=10)
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_thresholds, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        confmat_path = os.path.join(self.figs_dir, "train_based_confmat.png")
        plt.savefig(confmat_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“Š æ··æ·†çŸ©é˜µå¯è§†åŒ–ä¿å­˜åˆ°: {confmat_path}")
        
        return confmat_path
        
    def save_results(self, results, best_threshold):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°JSON"""
        print("7. ä¿å­˜è¯„ä¼°ç»“æœ...")
        
        # æ„å»ºå®Œæ•´ç»“æœ
        output_results = {
            "evaluation_summary": {
                "timestamp": pd.Timestamp.now().isoformat(),
                "model_file": self.model_path,
                "selected_roads": self.selected_roads,
                "network_edges": [(edge[0], edge[1], edge[2]) for edge in self.model_data['network_edges']],
                "test_samples": len(self.prediction_cache),
                "positive_samples": sum(pred["true_flood"] for pred in self.prediction_cache),
                "best_threshold": best_threshold
            },
            "threshold_results": results,
            "best_performance": results[best_threshold],
            "prediction_samples": self.prediction_cache[:50]  # ä¿å­˜å‰50ä¸ªæ ·æœ¬
        }
        
        # ä¿å­˜åˆ°JSON
        results_path = os.path.join(self.results_dir, "train_based_bn_metrics.json")
        
        # è½¬æ¢numpyç±»å‹
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        output_results_clean = convert_numpy(output_results)
        
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(output_results_clean, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… ç»“æœä¿å­˜åˆ°: {results_path}")
        return results_path
        
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹åŸºäºè®­ç»ƒé›†çš„è´å¶æ–¯ç½‘ç»œè¯„ä¼°...")
        print("="*60)
        
        try:
            # 1. æ•°æ®åŠ è½½
            self.load_and_split_data()
            
            # 2. æ¨¡å‹åŠ è½½
            if not self.load_model():
                return None
            
            # 3. è®¡ç®—é¢„æµ‹
            self.compute_test_predictions()
            
            # 4. è¯„ä¼°é˜ˆå€¼
            results = self.evaluate_thresholds()
            
            # 5. æ˜¾ç¤ºç»“æœ
            best_threshold, best_result = self.print_evaluation_results(results)
            
            # 6. åˆ›å»ºå¯è§†åŒ–
            self.create_confusion_matrix_visualization(results, best_threshold)
            
            # 7. ä¿å­˜ç»“æœ
            self.save_results(results, best_threshold)
            
            print(f"\nâœ… åŸºäºè®­ç»ƒé›†çš„è´å¶æ–¯ç½‘ç»œè¯„ä¼°å®Œæˆï¼")
            print(f"ğŸ“ è¯¦ç»†ç»“æœ: {self.results_dir}/train_based_bn_metrics.json")
            print(f"ğŸ“Š æ··æ·†çŸ©é˜µ: {self.figs_dir}/train_based_confmat.png")
            
            return results
            
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    evaluator = TrainBasedBayesianNetworkEvaluator()
    results = evaluator.run_evaluation()
    
    if results:
        print(f"\nğŸ¯ è¯„ä¼°æ€»ç»“:")
        print(f"   åŸºäºè®­ç»ƒé›†æ„å»ºçš„è´å¶æ–¯ç½‘ç»œåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å·²é‡åŒ–")
        print(f"   é¿å…äº†æ•°æ®æ³„éœ²ï¼Œç¡®ä¿äº†è¯„ä¼°çš„å¯é æ€§")
        print(f"   ç½‘ç»œç»“æ„ç®€æ´ï¼Œå…·æœ‰è‰¯å¥½çš„å¯è§£é‡Šæ€§")
        print(f"   ä¸ºå®é™…åº”ç”¨æä¾›äº†å‚è€ƒåŸºå‡†")

if __name__ == "__main__":
    main()