#!/usr/bin/env python3
"""
è¯„ä¼°æç®€æ´ªæ°´è´å¶æ–¯ç½‘ç»œ

åŠŸèƒ½ï¼š
1. åŠ è½½ä¸main_clean.pyç›¸åŒçš„æ•°æ®åˆ†å‰²
2. ä½¿ç”¨ultra_core_network.csvæ„å»ºè´å¶æ–¯ç½‘ç»œ
3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°Top-3å’Œé˜ˆå€¼0.3ç­–ç•¥
4. è®¡ç®—æ€§èƒ½æŒ‡æ ‡å¹¶ä¿å­˜ç»“æœ

ç”¨æ³•ï¼š
    python evaluate_ultra_core.py

è¾“å‡ºï¼š
    - ç»ˆç«¯è¾“å‡ºæ€§èƒ½æŒ‡æ ‡
    - results/ultra_core_metrics.json - è¯¦ç»†è¯„ä¼°ç»“æœ

æ³¨æ„ï¼š
    ç¡®ä¿å·²è¿è¡Œbuild_ultra_core.pyç”Ÿæˆç½‘ç»œæ–‡ä»¶
"""

import pandas as pd
import numpy as np
import json
import os
import networkx as nx
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any

# æœºå™¨å­¦ä¹ å’Œè¯„ä¼°
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    average_precision_score, roc_auc_score
)

# è´å¶æ–¯ç½‘ç»œç›¸å…³
from model import FloodBayesNetwork

# è®¾ç½®éšæœºç§å­ï¼ˆä¸main_clean.pyä¿æŒä¸€è‡´ï¼‰
RANDOM_SEED = 42

class UltraCoreEvaluator:
    """æç®€æ ¸å¿ƒç½‘ç»œè¯„ä¼°å™¨"""
    
    def __init__(self, 
                 network_csv_path="ultra_core_network.csv",
                 data_csv_path="Road_Closures_2024.csv",
                 results_dir="results"):
        self.network_csv_path = network_csv_path
        self.data_csv_path = data_csv_path
        self.results_dir = results_dir
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(results_dir, exist_ok=True)
        
        # åˆå§‹åŒ–
        self.train_df = None
        self.test_df = None
        self.flood_net = None
        self.core_nodes = set()
        self.network_edges = []
        
        # è¯„ä¼°å‚æ•°
        self.topk = 3
        self.prob_threshold = 0.3
        
        # é¢„æµ‹ç¼“å­˜
        self.prediction_cache = []
        
    def load_and_split_data(self):
        """åŠ è½½æ•°æ®å¹¶åˆ†å‰²ï¼ˆä¸main_clean.pyå®Œå…¨ä¸€è‡´ï¼‰"""
        print("1. åŠ è½½å’Œåˆ†å‰²æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(self.data_csv_path)
        df = df[df["REASON"].str.upper() == "FLOOD"].copy()
        
        # æ•°æ®é¢„å¤„ç†
        df["time_create"] = pd.to_datetime(df["START"], utc=True)
        df["link_id"] = df["STREET"].str.upper().str.replace(" ", "_")
        df["link_id"] = df["link_id"].astype(str)
        df["id"] = df["OBJECTID"].astype(str)
        
        # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­åˆ†å‰²
        self.train_df, self.test_df = train_test_split(
            df, test_size=0.3, random_state=RANDOM_SEED
        )
        
        print(f"   è®­ç»ƒé›†: {len(self.train_df)}æ¡")
        print(f"   æµ‹è¯•é›†: {len(self.test_df)}æ¡")
        
        return self.train_df, self.test_df
        
    def load_ultra_core_network(self):
        """åŠ è½½æç®€æ ¸å¿ƒç½‘ç»œ"""
        print("2. åŠ è½½æç®€æ ¸å¿ƒç½‘ç»œ...")
        
        try:
            # è¯»å–ç½‘ç»œæ–‡ä»¶
            network_df = pd.read_csv(self.network_csv_path)
            
            if len(network_df) == 0:
                print("   âš ï¸ ç½‘ç»œæ–‡ä»¶ä¸ºç©ºï¼Œå°†åˆ›å»ºå­¤ç«‹èŠ‚ç‚¹ç½‘ç»œ")
                # ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºé«˜é¢‘èŠ‚ç‚¹ä½œä¸ºæ ¸å¿ƒèŠ‚ç‚¹
                road_counts = Counter(self.train_df['link_id'])
                self.core_nodes = {road for road, count in road_counts.items() if count >= 15}
                self.network_edges = []
            else:
                # æå–èŠ‚ç‚¹å’Œè¾¹
                all_nodes = set(network_df['source'].tolist() + network_df['target'].tolist())
                self.core_nodes = all_nodes
                self.network_edges = network_df.to_dict('records')
            
            print(f"   æ ¸å¿ƒèŠ‚ç‚¹æ•°: {len(self.core_nodes)}")
            print(f"   ç½‘ç»œè¾¹æ•°: {len(self.network_edges)}")
            
            # æ˜¾ç¤ºæ ¸å¿ƒèŠ‚ç‚¹
            print("   æ ¸å¿ƒèŠ‚ç‚¹:")
            for node in sorted(self.core_nodes):
                print(f"     {node.replace('_', ' ')}")
                
            return True
            
        except FileNotFoundError:
            print(f"   âŒ æ‰¾ä¸åˆ°ç½‘ç»œæ–‡ä»¶: {self.network_csv_path}")
            print("   è¯·å…ˆè¿è¡Œ build_ultra_core.py ç”Ÿæˆç½‘ç»œ")
            return False
        except Exception as e:
            print(f"   âŒ åŠ è½½ç½‘ç»œæ—¶å‡ºé”™: {e}")
            return False
            
    def build_bayesian_network(self):
        """æ„å»ºè´å¶æ–¯ç½‘ç»œ"""
        print("3. æ„å»ºè´å¶æ–¯ç½‘ç»œ...")
        
        # åˆ›å»ºFloodBayesNetworkå®ä¾‹
        self.flood_net = FloodBayesNetwork(t_window="D")
        
        # æ‹Ÿåˆè¾¹é™…æ¦‚ç‡ï¼ˆä»…ä½¿ç”¨æ ¸å¿ƒèŠ‚ç‚¹çš„æ•°æ®ï¼‰
        core_train_df = self.train_df[self.train_df['link_id'].isin(self.core_nodes)].copy()
        if len(core_train_df) == 0:
            raise ValueError("è®­ç»ƒé›†ä¸­æ²¡æœ‰æ ¸å¿ƒèŠ‚ç‚¹çš„æ•°æ®")
            
        self.flood_net.fit_marginal(core_train_df)
        
        # æ‰‹åŠ¨æ„å»ºç½‘ç»œç»“æ„
        import networkx as nx
        self.flood_net.network = nx.DiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        for node in self.core_nodes:
            self.flood_net.network.add_node(node)
        
        # æ·»åŠ è¾¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        for edge_data in self.network_edges:
            source = edge_data['source']
            target = edge_data['target']
            weight = edge_data.get('mutual_info', edge_data.get('weight', 0.5))
            
            self.flood_net.network.add_edge(
                source, target,
                weight=weight,
                mutual_info=edge_data.get('mutual_info', 0),
                cooccurrence=edge_data.get('cooccurrence', 0)
            )
        
        print(f"   ç½‘ç»œèŠ‚ç‚¹: {self.flood_net.network.number_of_nodes()}")
        print(f"   ç½‘ç»œè¾¹: {self.flood_net.network.number_of_edges()}")
        
        # æ‹Ÿåˆæ¡ä»¶æ¦‚ç‡
        try:
            self.flood_net.fit_conditional(core_train_df, max_parents=2, alpha=1)
            print("   âœ… æ¡ä»¶æ¦‚ç‡æ‹Ÿåˆå®Œæˆ")
        except Exception as e:
            print(f"   âš ï¸ æ¡ä»¶æ¦‚ç‡æ‹Ÿåˆè­¦å‘Š: {e}")
        
        # æ„å»ºè´å¶æ–¯ç½‘ç»œå¯¹è±¡
        try:
            self.flood_net.build_bayes_network()
            self.flood_net.check_bayesian_network()
            print("   âœ… è´å¶æ–¯ç½‘ç»œæ„å»ºæˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸ è´å¶æ–¯ç½‘ç»œæ„å»ºè­¦å‘Š: {e}")
            
        return self.flood_net
        
    def compute_predictions_cache(self):
        """é¢„è®¡ç®—æµ‹è¯•é›†é¢„æµ‹ç»“æœ"""
        print("4. è®¡ç®—æµ‹è¯•é›†é¢„æµ‹...")
        
        test_by_date = self.test_df.groupby(self.test_df["time_create"].dt.floor("D"))
        total_predictions = 0
        
        for date, day_group in test_by_date:
            # å½“å¤©æ´ªæ°´é“è·¯
            flooded_roads = set(day_group["link_id"].unique())
            
            # åœ¨æ ¸å¿ƒèŠ‚ç‚¹ä¸­çš„æ´ªæ°´é“è·¯ï¼ˆä½œä¸ºevidenceï¼‰
            flooded_core = flooded_roads & self.core_nodes
            
            if len(flooded_core) == 0:
                continue  # æ²¡æœ‰æ ¸å¿ƒèŠ‚ç‚¹æ´ªæ°´ï¼Œè·³è¿‡è¿™ä¸€å¤©
            
            # ä½¿ç”¨æ‰€æœ‰è§‚å¯Ÿåˆ°çš„æ ¸å¿ƒèŠ‚ç‚¹æ´ªæ°´ä½œä¸ºevidenceï¼ˆä¸é™åˆ¶é¢‘æ¬¡ï¼‰
            evidence = {road: 1 for road in flooded_core}
            
            # ç›®æ ‡é“è·¯ï¼šæœªä½œä¸ºevidenceçš„å…¶ä»–æ ¸å¿ƒèŠ‚ç‚¹
            target_roads = self.core_nodes - flooded_core
            
            if len(target_roads) == 0:
                continue  # æ²¡æœ‰ç›®æ ‡é“è·¯
            
            # å¯¹æ¯ä¸ªç›®æ ‡é“è·¯è¿›è¡Œé¢„æµ‹
            day_predictions = []
            for road in target_roads:
                try:
                    # é¢„æµ‹æ´ªæ°´æ¦‚ç‡
                    if hasattr(self.flood_net, 'infer_w_evidence'):
                        result = self.flood_net.infer_w_evidence(road, evidence)
                        prob_flood = result.get("flooded", 0.5)
                    else:
                        # å¦‚æœæ¨ç†å¤±è´¥ï¼Œä½¿ç”¨è¾¹é™…æ¦‚ç‡
                        marginal_row = self.flood_net.marginals[
                            self.flood_net.marginals["link_id"] == road
                        ]
                        prob_flood = float(marginal_row["p"].values[0]) if len(marginal_row) > 0 else 0.5
                    
                    # çœŸå®æ ‡ç­¾
                    true_flood = 1 if road in flooded_roads else 0
                    
                    day_predictions.append({
                        "date": str(date.date()),
                        "road": road,
                        "prob_flood": prob_flood,
                        "true_flood": true_flood,
                        "evidence": evidence.copy()
                    })
                    total_predictions += 1
                    
                except Exception as e:
                    # æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¦‚ç‡
                    day_predictions.append({
                        "date": str(date.date()),
                        "road": road,
                        "prob_flood": 0.5,
                        "true_flood": 1 if road in flooded_roads else 0,
                        "evidence": evidence.copy()
                    })
                    total_predictions += 1
            
            # æŒ‰æ¦‚ç‡æ’åº
            day_predictions.sort(key=lambda x: x["prob_flood"], reverse=True)
            self.prediction_cache.append(day_predictions)
        
        print(f"   é¢„æµ‹æ ·æœ¬: {total_predictions}ä¸ª")
        print(f"   è¯„ä¼°å¤©æ•°: {len(self.prediction_cache)}å¤©")
        
    def evaluate_topk_strategy(self) -> Dict[str, float]:
        """è¯„ä¼°Top-Kç­–ç•¥"""
        print(f"5a. è¯„ä¼°Top-{self.topk}ç­–ç•¥...")
        
        total_hits = 0
        total_possible = 0
        all_true = []
        all_pred = []
        
        for day_predictions in self.prediction_cache:
            if len(day_predictions) == 0:
                continue
            
            # å½“å¤©å®é™…æ´ªæ°´æ•°
            actual_floods = sum(1 for pred in day_predictions if pred["true_flood"] == 1)
            total_possible += min(self.topk, actual_floods) if actual_floods > 0 else 0
            
            # Top-Ké¢„æµ‹
            for i, pred in enumerate(day_predictions):
                if i < self.topk:
                    all_pred.append(1)
                else:
                    all_pred.append(0)
                all_true.append(pred["true_flood"])
            
            # è®¡ç®—å‘½ä¸­æ•°
            day_hits = sum(1 for pred in day_predictions[:self.topk] if pred["true_flood"] == 1)
            total_hits += day_hits
        
        # è®¡ç®—æŒ‡æ ‡
        hits_at_k = total_hits / total_possible if total_possible > 0 else 0
        
        if len(all_true) > 0:
            precision = precision_score(all_true, all_pred, zero_division=0)
            recall = recall_score(all_true, all_pred, zero_division=0)
            f1 = f1_score(all_true, all_pred, zero_division=0)
            pr_auc = average_precision_score(all_true, all_pred) if len(set(all_true)) > 1 else np.mean(all_true)
        else:
            precision = recall = f1 = pr_auc = 0
        
        print(f"   Hits@{self.topk}: {hits_at_k:.4f}")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall: {recall:.4f}")
        print(f"   F1: {f1:.4f}")
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "hits_at_k": hits_at_k,
            "pr_auc": pr_auc,
            "total_samples": len(all_true)
        }
        
    def evaluate_threshold_strategy(self) -> Dict[str, float]:
        """è¯„ä¼°æ¦‚ç‡é˜ˆå€¼ç­–ç•¥"""
        print(f"5b. è¯„ä¼°é˜ˆå€¼{self.prob_threshold}ç­–ç•¥...")
        
        y_true = []
        y_pred = []
        y_prob = []
        
        for day_predictions in self.prediction_cache:
            for pred in day_predictions:
                y_true.append(pred["true_flood"])
                y_prob.append(pred["prob_flood"])
                y_pred.append(1 if pred["prob_flood"] >= self.prob_threshold else 0)
        
        if len(y_true) == 0:
            return {"precision": 0, "recall": 0, "f1": 0, "pr_auc": 0, "total_samples": 0}
        
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        pr_auc = average_precision_score(y_true, y_prob) if len(set(y_true)) > 1 else np.mean(y_true)
        
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall: {recall:.4f}")
        print(f"   F1: {f1:.4f}")
        print(f"   PR-AUC: {pr_auc:.4f}")
        
        return {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "pr_auc": pr_auc,
            "total_samples": len(y_true)
        }
        
    def save_results(self, topk_metrics: Dict, threshold_metrics: Dict):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        print("6. ä¿å­˜è¯„ä¼°ç»“æœ...")
        
        results = {
            "evaluation_summary": {
                "timestamp": pd.Timestamp.now().isoformat(),
                "network_file": self.network_csv_path,
                "core_nodes_count": len(self.core_nodes),
                "network_edges_count": len(self.network_edges),
                "train_samples": len(self.train_df),
                "test_samples": len(self.test_df)
            },
            "network_info": {
                "core_nodes": sorted(list(self.core_nodes)),
                "edges": self.network_edges
            },
            "evaluation_strategies": {
                f"top_{self.topk}": {
                    "strategy": f"Top-{self.topk}",
                    "parameters": {"k": self.topk},
                    "metrics": topk_metrics
                },
                f"threshold_{self.prob_threshold}": {
                    "strategy": f"Threshold-{self.prob_threshold}",
                    "parameters": {"prob_threshold": self.prob_threshold},
                    "metrics": threshold_metrics
                }
            }
        }
        
        # ä¿å­˜åˆ°JSON
        results_path = os.path.join(self.results_dir, "ultra_core_metrics.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"   âœ… ç»“æœä¿å­˜åˆ°: {results_path}")
        
    def print_summary(self, topk_metrics: Dict, threshold_metrics: Dict):
        """æ‰“å°è¯„ä¼°æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“Š æç®€æ ¸å¿ƒç½‘ç»œè¯„ä¼°ç»“æœ")
        print("="*60)
        
        print(f"ç½‘ç»œç»“æ„:")
        print(f"  æ ¸å¿ƒèŠ‚ç‚¹: {len(self.core_nodes)}ä¸ª")
        print(f"  ç½‘ç»œè¾¹: {len(self.network_edges)}æ¡")
        
        print(f"\nç­–ç•¥å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<12} {'Top-3':<10} {'é˜ˆå€¼0.3':<10}")
        print("-" * 40)
        print(f"{'Precision':<12} {topk_metrics['precision']:<10.4f} {threshold_metrics['precision']:<10.4f}")
        print(f"{'Recall':<12} {topk_metrics['recall']:<10.4f} {threshold_metrics['recall']:<10.4f}")
        print(f"{'F1-Score':<12} {topk_metrics['f1']:<10.4f} {threshold_metrics['f1']:<10.4f}")
        print(f"{'PR-AUC':<12} {topk_metrics['pr_auc']:<10.4f} {threshold_metrics['pr_auc']:<10.4f}")
        print(f"{'Hits@3':<12} {topk_metrics['hits_at_k']:<10.4f} {'-':<10}")
        
        # æ¨èç­–ç•¥
        if topk_metrics['f1'] > threshold_metrics['f1']:
            best_strategy = f"Top-{self.topk}"
            best_f1 = topk_metrics['f1']
        else:
            best_strategy = f"é˜ˆå€¼{self.prob_threshold}"
            best_f1 = threshold_metrics['f1']
        
        print(f"\nğŸ¯ æ¨èç­–ç•¥: {best_strategy} (F1: {best_f1:.4f})")
        print("="*60)
        
    def run_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        print("ğŸš€ å¼€å§‹æç®€æ ¸å¿ƒç½‘ç»œè¯„ä¼°...")
        print("="*60)
        
        try:
            # 1. æ•°æ®åŠ è½½
            self.load_and_split_data()
            
            # 2. åŠ è½½ç½‘ç»œ
            if not self.load_ultra_core_network():
                return None
            
            # 3. æ„å»ºè´å¶æ–¯ç½‘ç»œ
            self.build_bayesian_network()
            
            # 4. è®¡ç®—é¢„æµ‹
            self.compute_predictions_cache()
            
            # 5. è¯„ä¼°ç­–ç•¥
            topk_metrics = self.evaluate_topk_strategy()
            threshold_metrics = self.evaluate_threshold_strategy()
            
            # 6. ä¿å­˜ç»“æœ
            self.save_results(topk_metrics, threshold_metrics)
            
            # 7. æ‰“å°æ€»ç»“
            self.print_summary(topk_metrics, threshold_metrics)
            
            print(f"\nâœ… æç®€æ ¸å¿ƒç½‘ç»œè¯„ä¼°å®Œæˆï¼")
            
            return {
                "topk_metrics": topk_metrics,
                "threshold_metrics": threshold_metrics
            }
            
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    evaluator = UltraCoreEvaluator()
    results = evaluator.run_evaluation()
    
    if results:
        print(f"\nğŸ¯ æç®€ç½‘ç»œç‰¹ç‚¹:")
        print(f"   - ç²¾é€‰é«˜é¢‘æ ¸å¿ƒé“è·¯")
        print(f"   - åŸºäºäº’ä¿¡æ¯çš„æœ€ä¼˜è¿æ¥")
        print(f"   - æ ‘çŠ¶ç»“æ„ç¡®ä¿è®¡ç®—æ•ˆç‡")
        print(f"   - é€‚åˆå®æ—¶é¢„è­¦åº”ç”¨")

if __name__ == "__main__":
    main()