#!/usr/bin/env python3
"""
Fix and Visualize Results
ä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜å¹¶ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–ç»“æœ

è¿™ä¸ªè„šæœ¬ä¼šå¤„ç†å·²æœ‰çš„CSVç»“æœï¼Œä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜ï¼Œå¹¶ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–åˆ†æã€‚
"""

import sys
import os
import pandas as pd
import numpy as np
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, 'visualization'))

def fix_json_serialization(obj):
    """ä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜çš„è¾…åŠ©å‡½æ•°"""
    if isinstance(obj, dict):
        return {k: fix_json_serialization(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [fix_json_serialization(item) for item in obj]
    elif isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj

def analyze_and_visualize_results(result_dir):
    """åˆ†æç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–"""
    print("ğŸ”§ ä¿®å¤ç»“æœå¹¶ç”Ÿæˆå¯è§†åŒ–åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥ç»“æœæ–‡ä»¶
    csv_file = os.path.join(result_dir, "complete_results.csv")
    if not os.path.exists(csv_file):
        print(f"âŒ ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        return False
    
    print(f"ğŸ“Š åŠ è½½ç»“æœæ•°æ®: {csv_file}")
    try:
        df = pd.read_csv(csv_file)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} ä¸ªå‚æ•°ç»„åˆç»“æœ")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return False
    
    # æ•°æ®åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"æ€»ç»„åˆæ•°: {len(df)}")
    print(f"F1åˆ†æ•°èŒƒå›´: {df['f1_score'].min():.3f} - {df['f1_score'].max():.3f}")
    print(f"ç²¾ç¡®åº¦èŒƒå›´: {df['precision'].min():.3f} - {df['precision'].max():.3f}")
    print(f"å¬å›ç‡èŒƒå›´: {df['recall'].min():.3f} - {df['recall'].max():.3f}")
    
    # ç”Ÿæˆæ¨èé…ç½®ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
    print(f"\nğŸ’¡ ç”Ÿæˆæ¨èé…ç½®...")
    try:
        recommendations = generate_recommendations(df)
        
        # ä¿®å¤JSONåºåˆ—åŒ–é—®é¢˜
        fixed_recommendations = fix_json_serialization(recommendations)
        
        # ä¿å­˜æ¨èç»“æœ
        rec_file = os.path.join(result_dir, "parameter_recommendations.json")
        with open(rec_file, 'w', encoding='utf-8') as f:
            json.dump(fixed_recommendations, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ¨èé…ç½®å·²ä¿å­˜: {rec_file}")
        
    except Exception as e:
        print(f"âš ï¸ æ¨èé…ç½®ç”Ÿæˆå¤±è´¥: {e}")
        recommendations = {}
    
    # ç”Ÿæˆå¯è§†åŒ–
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–åˆ†æ...")
    try:
        from visualization.parameter_analysis_visualizer import ParameterVisualizer
        
        viz_dir = os.path.join(result_dir, "visualizations")
        os.makedirs(viz_dir, exist_ok=True)
        
        visualizer = ParameterVisualizer(df, viz_dir)
        
        # å®šä¹‰çº¦æŸæ¡ä»¶ç”¨äºå¯è§†åŒ–
        constraints = {
            'min_precision': 0.8,
            'min_recall': 0.8,
            'min_f1_score': 0.7,
            'min_samples': 50
        }
        
        print("  ç”Ÿæˆ3Dæ€§èƒ½åˆ†å¸ƒå›¾...")
        visualizer.create_3d_performance_scatter()
        
        print("  ç”Ÿæˆå‚æ•°çƒ­å›¾...")
        visualizer.create_parameter_heatmaps()
        
        print("  ç”Ÿæˆå‚æ•°æ•æ„Ÿæ€§åˆ†æ...")
        visualizer.create_parameter_sensitivity_analysis()
        
        print("  ç”ŸæˆParetoå‰æ²¿åˆ†æ...")
        visualizer.create_pareto_frontier_analysis()
        
        print("  ç”Ÿæˆçº¦æŸç­›é€‰åˆ†æ...")
        filtered_df = visualizer.create_constraint_filtering_visualization(constraints)
        
        print(f"âœ… å¯è§†åŒ–ç”Ÿæˆå®Œæˆ!")
        print(f"âœ… çº¦æŸæ¡ä»¶ä¸‹ç­›é€‰å‡º {len(filtered_df)} ä¸ªæ»¡è¶³æ¡ä»¶çš„é…ç½®")
        
    except Exception as e:
        print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        filtered_df = df
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    print(f"\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    try:
        generate_complete_report(df, filtered_df, result_dir, recommendations)
        print(f"âœ… åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
    
    return True

def generate_recommendations(df):
    """ç”Ÿæˆæ¨èé…ç½®"""
    recommendations = {}
    
    # åŸºäºæ‰€æœ‰ç»“æœçš„æ¨è
    best_f1_idx = df['f1_score'].idxmax()
    best_precision_idx = df['precision'].idxmax()
    best_recall_idx = df['recall'].idxmax()
    
    recommendations['overall'] = {
        'best_f1': df.iloc[best_f1_idx].to_dict(),
        'best_precision': df.iloc[best_precision_idx].to_dict(),
        'best_recall': df.iloc[best_recall_idx].to_dict()
    }
    
    # é«˜æ ‡å‡†çº¦æŸæ¡ä»¶ç­›é€‰
    high_std_mask = (df['precision'] >= 0.8) & (df['recall'] >= 0.6) & (df['f1_score'] >= 0.7)
    high_std_df = df[high_std_mask]
    
    if len(high_std_df) > 0:
        recommendations['high_standard'] = {
            'count': len(high_std_df),
            'best_f1': high_std_df.iloc[high_std_df['f1_score'].idxmax()].to_dict()
        }
    
    # å¹³è¡¡çº¦æŸæ¡ä»¶ç­›é€‰
    balanced_mask = (df['precision'] >= 0.7) & (df['recall'] >= 0.7)
    balanced_df = df[balanced_mask]
    
    if len(balanced_df) > 0:
        recommendations['balanced'] = {
            'count': len(balanced_df),
            'best_f1': balanced_df.iloc[balanced_df['f1_score'].idxmax()].to_dict()
        }
    
    return recommendations

def generate_complete_report(df, filtered_df, result_dir, recommendations):
    """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
    report_file = os.path.join(result_dir, "complete_analysis_report.md")
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# è´å¶æ–¯ç½‘ç»œå‚æ•°ä¼˜åŒ–å®Œæ•´åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**åˆ†æè„šæœ¬**: fix_and_visualize_results.py\n\n")
        
        # å®éªŒæ¦‚è¿°
        f.write("## ğŸ“Š å®éªŒæ¦‚è¿°\n\n")
        f.write(f"- **æ€»å‚æ•°ç»„åˆæ•°**: 8,640 (4Ã—3Ã—4Ã—4Ã—5Ã—3Ã—3)\n")
        f.write(f"- **æˆåŠŸè¯„ä¼°ç»„åˆæ•°**: {len(df)}\n")
        f.write(f"- **æˆåŠŸç‡**: {len(df)/8640*100:.1f}%\n")
        f.write(f"- **å¤±è´¥ç»„åˆæ•°**: {8640-len(df)} (ä¸»è¦åŸå› ï¼šè´Ÿæ ·æœ¬å€™é€‰ä¸è¶³)\n\n")
        
        # æ€§èƒ½ç»Ÿè®¡
        f.write("## ğŸ“ˆ æ€§èƒ½ç»Ÿè®¡\n\n")
        f.write("| æŒ‡æ ‡ | æœ€å°å€¼ | æœ€å¤§å€¼ | å¹³å‡å€¼ | æ ‡å‡†å·® | ä¸­ä½æ•° |\n")
        f.write("|------|--------|--------|--------|--------|--------|\n")
        
        metrics = ['precision', 'recall', 'f1_score', 'accuracy', 'total_samples']
        for metric in metrics:
            if metric in df.columns:
                min_val = df[metric].min()
                max_val = df[metric].max()
                mean_val = df[metric].mean()
                std_val = df[metric].std()
                median_val = df[metric].median()
                
                f.write(f"| {metric.replace('_', ' ').title()} | {min_val:.3f} | {max_val:.3f} | {mean_val:.3f} | {std_val:.3f} | {median_val:.3f} |\n")
        f.write("\n")
        
        # æœ€ä½³é…ç½®
        f.write("## ğŸ† æœ€ä½³é…ç½®\n\n")
        
        best_f1_idx = df['f1_score'].idxmax()
        best_config = df.iloc[best_f1_idx]
        
        f.write(f"### æœ€ä½³F1åˆ†æ•°é…ç½® (F1 = {best_config['f1_score']:.3f})\n\n")
        f.write(f"**ç½‘ç»œå‚æ•°**:\n")
        f.write(f"- occ_thr (æœ€å°å‡ºç°æ¬¡æ•°): {best_config['occ_thr']}\n")
        f.write(f"- edge_thr (è¾¹æœ€å°å…±ç°): {best_config['edge_thr']}\n")
        f.write(f"- weight_thr (è¾¹æƒé‡é˜ˆå€¼): {best_config['weight_thr']}\n\n")
        
        f.write(f"**è¯„ä¼°å‚æ•°**:\n")
        f.write(f"- evidence_count (è¯æ®æ•°é‡): {best_config['evidence_count']}\n")
        f.write(f"- pred_threshold (é¢„æµ‹é˜ˆå€¼): {best_config['pred_threshold']}\n")
        f.write(f"- neg_pos_ratio (è´Ÿæ­£æ ·æœ¬æ¯”): {best_config['neg_pos_ratio']}\n")
        f.write(f"- marginal_prob_threshold (è¾¹é™…æ¦‚ç‡é˜ˆå€¼): {best_config['marginal_prob_threshold']}\n\n")
        
        f.write(f"**æ€§èƒ½è¡¨ç°**:\n")
        f.write(f"- ç²¾ç¡®åº¦ (Precision): {best_config['precision']:.3f}\n")
        f.write(f"- å¬å›ç‡ (Recall): {best_config['recall']:.3f}\n")
        f.write(f"- F1åˆ†æ•°: {best_config['f1_score']:.3f}\n")
        f.write(f"- å‡†ç¡®ç‡: {best_config['accuracy']:.3f}\n")
        f.write(f"- æµ‹è¯•æ ·æœ¬æ•°: {best_config['total_samples']}\n")
        f.write(f"- ç½‘ç»œèŠ‚ç‚¹æ•°: {best_config['network_nodes']}\n\n")
        
        # é«˜æ ‡å‡†é…ç½®åˆ†æ
        high_std_mask = (df['precision'] >= 0.8) & (df['recall'] >= 0.6) & (df['f1_score'] >= 0.7)
        high_std_df = df[high_std_mask]
        
        f.write(f"## ğŸ¯ é«˜æ ‡å‡†é…ç½®åˆ†æ (Pâ‰¥0.8, Râ‰¥0.6, F1â‰¥0.7)\n\n")
        f.write(f"- **æ»¡è¶³æ¡ä»¶çš„é…ç½®æ•°**: {len(high_std_df)}\n")
        f.write(f"- **æ¯”ä¾‹**: {len(high_std_df)/len(df)*100:.1f}%\n\n")
        
        if len(high_std_df) > 0:
            high_best = high_std_df.iloc[high_std_df['f1_score'].idxmax()]
            f.write(f"**é«˜æ ‡å‡†ä¸­çš„æœ€ä½³é…ç½®** (F1 = {high_best['f1_score']:.3f}):\n")
            f.write(f"- å‚æ•°ç»„åˆ: occ_thr={high_best['occ_thr']}, edge_thr={high_best['edge_thr']}, weight_thr={high_best['weight_thr']}\n")
            f.write(f"- è¯„ä¼°å‚æ•°: evidence_count={high_best['evidence_count']}, pred_threshold={high_best['pred_threshold']}\n")
            f.write(f"- æ€§èƒ½: P={high_best['precision']:.3f}, R={high_best['recall']:.3f}, F1={high_best['f1_score']:.3f}\n\n")
        
        # å‚æ•°é‡è¦æ€§åˆ†æ
        f.write("## ğŸ” å‚æ•°é‡è¦æ€§åˆ†æ\n\n")
        
        param_impact = {}
        categorical_params = ['occ_thr', 'edge_thr', 'evidence_count']
        
        for param in categorical_params:
            param_performance = df.groupby(param)['f1_score'].agg(['mean', 'std', 'count']).round(4)
            best_value = param_performance['mean'].idxmax()
            worst_value = param_performance['mean'].idxmin()
            impact = param_performance['mean'].max() - param_performance['mean'].min()
            
            param_impact[param] = {
                'impact': impact,
                'best_value': best_value,
                'worst_value': worst_value
            }
            
            f.write(f"### {param.replace('_', ' ').title()}\n")
            f.write(f"- **æ€§èƒ½å½±å“èŒƒå›´**: {impact:.3f}\n")
            f.write(f"- **æœ€ä½³å€¼**: {best_value} (F1å‡å€¼: {param_performance.loc[best_value, 'mean']:.3f})\n")
            f.write(f"- **æœ€å·®å€¼**: {worst_value} (F1å‡å€¼: {param_performance.loc[worst_value, 'mean']:.3f})\n\n")
        
        # ä½¿ç”¨å»ºè®®
        f.write("## ğŸ’¡ ä½¿ç”¨å»ºè®®\n\n")
        f.write("### æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©é…ç½®\n\n")
        f.write("1. **é«˜ç²¾åº¦åœºæ™¯** (å‡å°‘è¯¯æŠ¥):\n")
        high_precision_config = df.iloc[df['precision'].idxmax()]
        f.write(f"   - æ¨èé…ç½®: occ_thr={high_precision_config['occ_thr']}, evidence_count={high_precision_config['evidence_count']}, pred_threshold={high_precision_config['pred_threshold']}\n")
        f.write(f"   - é¢„æœŸæ€§èƒ½: P={high_precision_config['precision']:.3f}, R={high_precision_config['recall']:.3f}, F1={high_precision_config['f1_score']:.3f}\n\n")
        
        f.write("2. **é«˜å¬å›åœºæ™¯** (é¿å…é—æ¼):\n")
        high_recall_config = df.iloc[df['recall'].idxmax()]
        f.write(f"   - æ¨èé…ç½®: occ_thr={high_recall_config['occ_thr']}, evidence_count={high_recall_config['evidence_count']}, pred_threshold={high_recall_config['pred_threshold']}\n")
        f.write(f"   - é¢„æœŸæ€§èƒ½: P={high_recall_config['precision']:.3f}, R={high_recall_config['recall']:.3f}, F1={high_recall_config['f1_score']:.3f}\n\n")
        
        f.write("3. **å¹³è¡¡åº”ç”¨åœºæ™¯**:\n")
        f.write(f"   - æ¨èé…ç½®: occ_thr={best_config['occ_thr']}, evidence_count={best_config['evidence_count']}, pred_threshold={best_config['pred_threshold']}\n")
        f.write(f"   - é¢„æœŸæ€§èƒ½: P={best_config['precision']:.3f}, R={best_config['recall']:.3f}, F1={best_config['f1_score']:.3f}\n\n")
        
        # æ–‡ä»¶è¯´æ˜
        f.write("## ğŸ“ è¾“å‡ºæ–‡ä»¶è¯´æ˜\n\n")
        f.write("- `complete_results.csv`: æ‰€æœ‰5,760ä¸ªæˆåŠŸé…ç½®çš„è¯¦ç»†ç»“æœ\n")
        f.write("- `parameter_recommendations.json`: æ¨èé…ç½®çš„JSONæ ¼å¼\n")
        f.write("- `visualizations/`: å¯è§†åŒ–å›¾è¡¨æ–‡ä»¶å¤¹\n")
        f.write("  - `precision_recall_f1_3d.png`: 3Dæ€§èƒ½åˆ†å¸ƒå›¾\n")
        f.write("  - `parameter_heatmaps.png`: å‚æ•°ç»„åˆçƒ­å›¾\n")
        f.write("  - `parameter_sensitivity.png`: å‚æ•°æ•æ„Ÿæ€§åˆ†æ\n")
        f.write("  - `pareto_frontier.png`: Precision-Recallæƒè¡¡åˆ†æ\n")
        f.write("  - `constraint_filtering.png`: çº¦æŸæ¡ä»¶ç­›é€‰ç»“æœ\n\n")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å‚æ•°ä¼˜åŒ–ç»“æœä¿®å¤å’Œå¯è§†åŒ–å·¥å…·")
    print("=" * 60)
    
    # æŸ¥æ‰¾æœ€æ–°çš„ç»“æœç›®å½•
    result_dir = "/mnt/d/Data/coda_PycharmProjects/PIN_bayesian/results/parameter_optimization_20250721_140722"
    
    if not os.path.exists(result_dir):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {result_dir}")
        print("è¯·æ£€æŸ¥ç›®å½•è·¯å¾„æ˜¯å¦æ­£ç¡®")
        return
    
    print(f"ğŸ“ å¤„ç†ç»“æœç›®å½•: {result_dir}")
    
    # åˆ†æå’Œå¯è§†åŒ–
    success = analyze_and_visualize_results(result_dir)
    
    if success:
        print(f"\nğŸ‰ ç»“æœä¿®å¤å’Œå¯è§†åŒ–å®Œæˆ!")
        print(f"ğŸ“ ç»“æœç›®å½•: {result_dir}")
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: complete_results.csv")
        print(f"ğŸ’¡ æ¨èé…ç½®: parameter_recommendations.json")
        print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: complete_analysis_report.md")
        print(f"ğŸ¨ å¯è§†åŒ–å›¾è¡¨: visualizations/")
        print(f"\nâœ¨ ä½ ç°åœ¨å¯ä»¥ç”¨è¿™äº›ç»“æœè¿›è¡Œç­”è¾©äº†!")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    main()