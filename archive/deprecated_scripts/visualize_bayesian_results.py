#!/usr/bin/env python3
"""
è´å¶æ–¯ç½‘ç»œæ´ªæ°´é¢„æµ‹ç»“æœå¯è§†åŒ–
- äº¤å‰éªŒè¯æ€§èƒ½åˆ†æ
- æœ€ä½³å®éªŒè¯¦ç»†å¯è§†åŒ–
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class BayesianResultsVisualizer:
    def __init__(self, csv_file, json_file):
        self.csv_file = csv_file
        self.json_file = json_file
        self.df = None
        self.best_experiment = None
        self.load_data()
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        
        # åŠ è½½CSVäº¤å‰éªŒè¯ç»“æœ
        self.df = pd.read_csv(self.csv_file)
        print(f"âœ… åŠ è½½CSV: {len(self.df)} æ¡å®éªŒè®°å½•")
        
        # åŠ è½½æœ€ä½³å®éªŒJSON
        with open(self.json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.best_experiment = data['best_experiment']
        print(f"âœ… åŠ è½½æœ€ä½³å®éªŒ: {self.best_experiment['test_date']}")
        
    def create_performance_by_threshold_plot(self):
        """æ€§èƒ½æŒ‡æ ‡éšé˜ˆå€¼å˜åŒ–å›¾"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ğŸ¯ è´å¶æ–¯ç½‘ç»œæ€§èƒ½æŒ‡æ ‡éšé¢„æµ‹é˜ˆå€¼å˜åŒ–', fontsize=16, fontweight='bold')
        
        # æŒ‰é˜ˆå€¼åˆ†ç»„è®¡ç®—ç»Ÿè®¡
        threshold_stats = self.df.groupby('pred_threshold').agg({
            'precision': ['mean', 'std'],
            'recall': ['mean', 'std'], 
            'f1_score': ['mean', 'std'],
            'accuracy': ['mean', 'std']
        }).round(4)
        
        thresholds = threshold_stats.index
        
        # 1. Precision
        ax1.errorbar(thresholds, threshold_stats['precision']['mean'], 
                    yerr=threshold_stats['precision']['std'], 
                    marker='o', linewidth=2, markersize=8, capsize=5)
        ax1.set_title('Precision vs é˜ˆå€¼', fontweight='bold')
        ax1.set_xlabel('é¢„æµ‹é˜ˆå€¼')
        ax1.set_ylabel('Precision')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 1.1)
        
        # 2. Recall  
        ax2.errorbar(thresholds, threshold_stats['recall']['mean'],
                    yerr=threshold_stats['recall']['std'],
                    marker='s', linewidth=2, markersize=8, capsize=5, color='orange')
        ax2.set_title('Recall vs é˜ˆå€¼', fontweight='bold')
        ax2.set_xlabel('é¢„æµ‹é˜ˆå€¼') 
        ax2.set_ylabel('Recall')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1.1)
        
        # 3. F1 Score
        ax3.errorbar(thresholds, threshold_stats['f1_score']['mean'],
                    yerr=threshold_stats['f1_score']['std'], 
                    marker='^', linewidth=2, markersize=8, capsize=5, color='green')
        ax3.set_title('F1 Score vs é˜ˆå€¼', fontweight='bold')
        ax3.set_xlabel('é¢„æµ‹é˜ˆå€¼')
        ax3.set_ylabel('F1 Score') 
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1.1)
        
        # 4. Accuracy
        ax4.errorbar(thresholds, threshold_stats['accuracy']['mean'],
                    yerr=threshold_stats['accuracy']['std'],
                    marker='d', linewidth=2, markersize=8, capsize=5, color='red')
        ax4.set_title('Accuracy vs é˜ˆå€¼', fontweight='bold')
        ax4.set_xlabel('é¢„æµ‹é˜ˆå€¼')
        ax4.set_ylabel('Accuracy')
        ax4.grid(True, alpha=0.3) 
        ax4.set_ylim(0, 1.1)
        
        plt.tight_layout()
        plt.savefig('performance_by_threshold.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ’¾ ä¿å­˜: performance_by_threshold.png")
        
    def create_performance_by_date_plot(self):
        """ä¸åŒæ—¥æœŸæ´ªæ°´äº‹ä»¶çš„æ€§èƒ½å¯¹æ¯”"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ğŸ“… ä¸åŒæ´ªæ°´äº‹ä»¶æ—¥æœŸçš„æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        date_stats = self.df.groupby('test_date').agg({
            'precision': ['mean', 'std'],
            'recall': ['mean', 'std'],
            'f1_score': ['mean', 'std'],
            'accuracy': ['mean', 'std'],
            'test_roads_total': 'first'
        }).round(4)
        
        dates = date_stats.index
        road_counts = date_stats['test_roads_total']['first']
        
        # åˆ›å»ºå¸¦é“è·¯æ•°é‡æ ‡æ³¨çš„å›¾è¡¨
        x_pos = np.arange(len(dates))
        
        # 1. Precision
        bars1 = ax1.bar(x_pos, date_stats['precision']['mean'], 
                       yerr=date_stats['precision']['std'],
                       alpha=0.7, capsize=5)
        ax1.set_title('å„æ—¥æœŸ Precision å¯¹æ¯”', fontweight='bold')
        ax1.set_xlabel('æµ‹è¯•æ—¥æœŸ')
        ax1.set_ylabel('Precision')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(dates, rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ é“è·¯æ•°é‡æ ‡æ³¨
        for i, (bar, count) in enumerate(zip(bars1, road_counts)):
            ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,
                    f'{count}æ¡é“è·¯', ha='center', va='bottom', fontsize=9)
        
        # 2. Recall
        bars2 = ax2.bar(x_pos, date_stats['recall']['mean'],
                       yerr=date_stats['recall']['std'], 
                       alpha=0.7, capsize=5, color='orange')
        ax2.set_title('å„æ—¥æœŸ Recall å¯¹æ¯”', fontweight='bold')
        ax2.set_xlabel('æµ‹è¯•æ—¥æœŸ')
        ax2.set_ylabel('Recall')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(dates, rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # 3. F1 Score
        bars3 = ax3.bar(x_pos, date_stats['f1_score']['mean'],
                       yerr=date_stats['f1_score']['std'],
                       alpha=0.7, capsize=5, color='green')
        ax3.set_title('å„æ—¥æœŸ F1 Score å¯¹æ¯”', fontweight='bold')
        ax3.set_xlabel('æµ‹è¯•æ—¥æœŸ')
        ax3.set_ylabel('F1 Score')
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(dates, rotation=45)
        ax3.grid(True, alpha=0.3)
        
        # 4. æ··æ·†çŸ©é˜µæŒ‡æ ‡
        confusion_stats = self.df.groupby('test_date').agg({
            'tp': 'mean', 'fp': 'mean', 'tn': 'mean', 'fn': 'mean'
        }).round(1)
        
        width = 0.2
        x_pos = np.arange(len(dates))
        ax4.bar(x_pos - 1.5*width, confusion_stats['tp'], width, label='TP', alpha=0.7)
        ax4.bar(x_pos - 0.5*width, confusion_stats['fp'], width, label='FP', alpha=0.7) 
        ax4.bar(x_pos + 0.5*width, confusion_stats['tn'], width, label='TN', alpha=0.7)
        ax4.bar(x_pos + 1.5*width, confusion_stats['fn'], width, label='FN', alpha=0.7)
        
        ax4.set_title('å„æ—¥æœŸæ··æ·†çŸ©é˜µå¹³å‡å€¼', fontweight='bold')
        ax4.set_xlabel('æµ‹è¯•æ—¥æœŸ')
        ax4.set_ylabel('å¹³å‡æ•°é‡')
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(dates, rotation=45)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('performance_by_date.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ’¾ ä¿å­˜: performance_by_date.png")
        
    def create_precision_recall_scatter(self):
        """Precision-Recallæ•£ç‚¹å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('ğŸ¯ Precision-Recall å…³ç³»åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æŒ‰é˜ˆå€¼ç€è‰²çš„æ•£ç‚¹å›¾
        scatter = ax1.scatter(self.df['recall'], self.df['precision'], 
                             c=self.df['pred_threshold'], s=60, alpha=0.7, 
                             cmap='viridis')
        ax1.set_xlabel('Recall')
        ax1.set_ylabel('Precision')
        ax1.set_title('Precision vs Recall (æŒ‰é˜ˆå€¼ç€è‰²)', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.set_xlim(0, 1)
        ax1.set_ylim(0, 1.05)
        
        # æ·»åŠ colorbar
        cbar = plt.colorbar(scatter, ax=ax1)
        cbar.set_label('é¢„æµ‹é˜ˆå€¼')
        
        # 2. æŒ‰æ—¥æœŸç€è‰²çš„æ•£ç‚¹å›¾
        dates = self.df['test_date'].unique()
        colors = plt.cm.Set1(np.linspace(0, 1, len(dates)))
        
        for date, color in zip(dates, colors):
            mask = self.df['test_date'] == date
            ax2.scatter(self.df[mask]['recall'], self.df[mask]['precision'], 
                       label=date, s=60, alpha=0.7, color=color)
        
        ax2.set_xlabel('Recall')
        ax2.set_ylabel('Precision') 
        ax2.set_title('Precision vs Recall (æŒ‰æ—¥æœŸç€è‰²)', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax2.set_xlim(0, 1)
        ax2.set_ylim(0, 1.05)
        
        plt.tight_layout()
        plt.savefig('precision_recall_scatter.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ’¾ ä¿å­˜: precision_recall_scatter.png")
        
    def create_sample_distribution_plot(self):
        """æ ·æœ¬åˆ†å¸ƒåˆ†æå›¾"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ğŸ“Š æ ·æœ¬åˆ†å¸ƒä¸ç½‘ç»œè§„æ¨¡åˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æ­£è´Ÿæ ·æœ¬åˆ†å¸ƒ
        ax1.scatter(self.df['positive_predict_count'], self.df['negative_predict_count'], 
                   c=self.df['f1_score'], s=80, alpha=0.7, cmap='RdYlGn')
        ax1.set_xlabel('æ­£æ ·æœ¬æ•°é‡')
        ax1.set_ylabel('è´Ÿæ ·æœ¬æ•°é‡')
        ax1.set_title('æ­£è´Ÿæ ·æœ¬åˆ†å¸ƒ (F1åˆ†æ•°ç€è‰²)', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # 2. ç½‘ç»œè§„æ¨¡vsæ€§èƒ½
        ax2.scatter(self.df['network_nodes'], self.df['f1_score'], 
                   c=self.df['pred_threshold'], s=80, alpha=0.7, cmap='plasma')
        ax2.set_xlabel('ç½‘ç»œèŠ‚ç‚¹æ•°')
        ax2.set_ylabel('F1 Score')
        ax2.set_title('ç½‘ç»œè§„æ¨¡ vs F1æ€§èƒ½', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # 3. æˆåŠŸé¢„æµ‹æ¯”ä¾‹
        success_rate = self.df['successful_predictions'] / self.df['total_predict_count']
        ax3.hist(success_rate, bins=20, alpha=0.7, edgecolor='black')
        ax3.axvline(success_rate.mean(), color='red', linestyle='--', linewidth=2,
                   label=f'å¹³å‡æˆåŠŸç‡: {success_rate.mean():.3f}')
        ax3.set_xlabel('æˆåŠŸé¢„æµ‹æ¯”ä¾‹')
        ax3.set_ylabel('å®éªŒæ•°é‡')
        ax3.set_title('è´å¶æ–¯æ¨ç†æˆåŠŸç‡åˆ†å¸ƒ', fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. è¯æ®é“è·¯æ•°é‡å½±å“
        evidence_stats = self.df.groupby('evidence_roads_count').agg({
            'precision': 'mean',
            'recall': 'mean', 
            'f1_score': 'mean'
        })
        
        x = evidence_stats.index
        ax4.plot(x, evidence_stats['precision'], 'o-', label='Precision', linewidth=2)
        ax4.plot(x, evidence_stats['recall'], 's-', label='Recall', linewidth=2)  
        ax4.plot(x, evidence_stats['f1_score'], '^-', label='F1 Score', linewidth=2)
        ax4.set_xlabel('è¯æ®é“è·¯æ•°é‡')
        ax4.set_ylabel('æ€§èƒ½æŒ‡æ ‡')
        ax4.set_title('è¯æ®é“è·¯æ•°é‡å¯¹æ€§èƒ½çš„å½±å“', fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('sample_distribution_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ’¾ ä¿å­˜: sample_distribution_analysis.png")
        
    def create_best_experiment_visualization(self):
        """æœ€ä½³å®éªŒè¯¦ç»†å¯è§†åŒ–"""
        exp = self.best_experiment
        predictions = exp['detailed_predictions']
        
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        fig.suptitle(f'ğŸ† æœ€ä½³å®éªŒè¯¦ç»†åˆ†æ: {exp["test_date"]} (é˜ˆå€¼={exp["pred_threshold"]})', 
                     fontsize=18, fontweight='bold')
        
        # 1. æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        ax1 = fig.add_subplot(gs[0, 0], projection='polar')
        metrics = ['Precision', 'Recall', 'F1 Score', 'Accuracy']
        values = [exp['precision'], exp['recall'], exp['f1_score'], exp['accuracy']]
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        values += values[:1]  # é—­åˆå¤šè¾¹å½¢
        angles += angles[:1]
        
        ax1.plot(angles, values, 'o-', linewidth=2, color='red')
        ax1.fill(angles, values, alpha=0.25, color='red')
        ax1.set_xticks(angles[:-1])
        ax1.set_xticklabels(metrics)
        ax1.set_ylim(0, 1)
        ax1.set_title('æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾', fontweight='bold', pad=20)
        ax1.grid(True)
        
        # 2. æ··æ·†çŸ©é˜µ
        ax2 = fig.add_subplot(gs[0, 1])
        confusion_matrix = np.array([[exp['tp'], exp['fp']], 
                                   [exp['fn'], exp['tn']]])
        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=ax2,
                   xticklabels=['é¢„æµ‹æ´ªæ°´', 'é¢„æµ‹æ— æ´ªæ°´'],
                   yticklabels=['å®é™…æ´ªæ°´', 'å®é™…æ— æ´ªæ°´'])
        ax2.set_title('æ··æ·†çŸ©é˜µ', fontweight='bold')
        
        # 3. æ ·æœ¬åˆ†å¸ƒé¥¼å›¾
        ax3 = fig.add_subplot(gs[0, 2])
        sizes = [exp['tp'], exp['fp'], exp['tn'], exp['fn']]
        labels = [f'TP ({exp["tp"]})', f'FP ({exp["fp"]})', 
                 f'TN ({exp["tn"]})', f'FN ({exp["fn"]})']
        colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12']
        ax3.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        ax3.set_title('é¢„æµ‹ç»“æœåˆ†å¸ƒ', fontweight='bold')
        
        # 4. é“è·¯é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        ax4 = fig.add_subplot(gs[1, :])
        
        # å‡†å¤‡æ•°æ®
        road_names = [p['road_name'] for p in predictions]
        probabilities = [p['predicted_probability'] for p in predictions]
        true_labels = [p['true_label'] for p in predictions]
        predicted_labels = [p['predicted_label'] for p in predictions]
        
        # æŒ‰æ¦‚ç‡æ’åº
        sorted_indices = np.argsort(probabilities)[::-1]  # é™åº
        
        x_pos = np.arange(len(road_names))
        colors = []
        for i in sorted_indices:
            if true_labels[i] == 1 and predicted_labels[i] == 1:  # TP
                colors.append('#2ecc71')  # ç»¿è‰²
            elif true_labels[i] == 0 and predicted_labels[i] == 0:  # TN  
                colors.append('#3498db')  # è“è‰²
            elif true_labels[i] == 1 and predicted_labels[i] == 0:  # FN
                colors.append('#f39c12')  # æ©™è‰²
            else:  # FP
                colors.append('#e74c3c')  # çº¢è‰²
        
        bars = ax4.bar(x_pos, [probabilities[i] for i in sorted_indices], color=colors, alpha=0.8)
        ax4.axhline(y=exp['pred_threshold'], color='red', linestyle='--', linewidth=2, 
                   label=f'é˜ˆå€¼ = {exp["pred_threshold"]}')
        
        ax4.set_xlabel('é“è·¯ (æŒ‰é¢„æµ‹æ¦‚ç‡æ’åº)')
        ax4.set_ylabel('æ´ªæ°´æ¦‚ç‡')
        ax4.set_title('æ‰€æœ‰é“è·¯çš„æ´ªæ°´é¢„æµ‹æ¦‚ç‡', fontweight='bold')
        ax4.set_xticks(x_pos[::2])  # åªæ˜¾ç¤ºéƒ¨åˆ†æ ‡ç­¾é¿å…é‡å 
        ax4.set_xticklabels([road_names[sorted_indices[i]] for i in range(0, len(road_names), 2)], 
                           rotation=90, fontsize=8)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        # 5. è¯æ®é“è·¯ä¿¡æ¯
        ax5 = fig.add_subplot(gs[2, 0])
        evidence_roads = exp['evidence_roads']
        ax5.text(0.1, 0.9, 'ğŸ”‘ è¯æ®é“è·¯:', fontsize=12, fontweight='bold', transform=ax5.transAxes)
        
        evidence_text = ''
        for i, road in enumerate(evidence_roads):
            evidence_text += f'{i+1}. {road}\n'
        
        ax5.text(0.1, 0.8, evidence_text, fontsize=10, transform=ax5.transAxes, 
                verticalalignment='top')
        ax5.set_xlim(0, 1)
        ax5.set_ylim(0, 1) 
        ax5.axis('off')
        
        # 6. ç»Ÿè®¡æ‘˜è¦
        ax6 = fig.add_subplot(gs[2, 1:])
        
        # åˆ†æé«˜æ¦‚ç‡é¢„æµ‹
        high_prob_correct = sum(1 for p in predictions 
                               if p['predicted_probability'] > 0.7 and 
                               p['true_label'] == p['predicted_label'])
        high_prob_total = sum(1 for p in predictions if p['predicted_probability'] > 0.7)
        
        summary_text = f"""
ğŸ“Š å®éªŒç»Ÿè®¡æ‘˜è¦:
â€¢ æ€»é¢„æµ‹é“è·¯: {len(predictions)}æ¡
â€¢ æˆåŠŸé¢„æµ‹: {exp['successful_predictions']}æ¡ (100%)
â€¢ é«˜ç½®ä¿¡åº¦é¢„æµ‹(>0.7): {high_prob_total}æ¡ï¼Œå…¶ä¸­æ­£ç¡®{high_prob_correct}æ¡
â€¢ ç½‘ç»œè§„æ¨¡: {exp['network_nodes']}ä¸ªèŠ‚ç‚¹, {exp['network_edges']}æ¡è¾¹
â€¢ è¯æ®æ¯”ä¾‹: {exp['evidence_roads_count']}/{exp['test_roads_in_network']} = {exp['evidence_roads_count']/exp['test_roads_in_network']:.1%}

ğŸ¯ å…³é”®æˆåŠŸå› ç´ :
â€¢ å®Œç¾ç²¾ç¡®ç‡ (100%) - æ— è¯¯æŠ¥
â€¢ åˆç†å¬å›ç‡ ({exp['recall']:.1%}) 
â€¢ è¯æ®é“è·¯é€‰æ‹©æœ‰æ•ˆ
â€¢ è´å¶æ–¯æ¨ç†ç¨³å®šè¿è¡Œ
        """
        
        ax6.text(0.05, 0.95, summary_text, fontsize=11, transform=ax6.transAxes,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))
        ax6.axis('off')
        
        plt.savefig('best_experiment_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("ğŸ’¾ ä¿å­˜: best_experiment_analysis.png")
        
    def create_all_visualizations(self):
        """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ¨ å¼€å§‹åˆ›å»ºå¯è§†åŒ–å›¾è¡¨...")
        
        print("\n1ï¸âƒ£ åˆ›å»ºæ€§èƒ½éšé˜ˆå€¼å˜åŒ–å›¾...")
        self.create_performance_by_threshold_plot()
        
        print("\n2ï¸âƒ£ åˆ›å»ºä¸åŒæ—¥æœŸæ€§èƒ½å¯¹æ¯”å›¾...")
        self.create_performance_by_date_plot()
        
        print("\n3ï¸âƒ£ åˆ›å»ºPrecision-Recallæ•£ç‚¹å›¾...")
        self.create_precision_recall_scatter()
        
        print("\n4ï¸âƒ£ åˆ›å»ºæ ·æœ¬åˆ†å¸ƒåˆ†æå›¾...")
        self.create_sample_distribution_plot()
        
        print("\n5ï¸âƒ£ åˆ›å»ºæœ€ä½³å®éªŒè¯¦ç»†å¯è§†åŒ–...")
        self.create_best_experiment_visualization()
        
        print(f"\nğŸ‰ æ‰€æœ‰å¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   â€¢ performance_by_threshold.png")
        print("   â€¢ performance_by_date.png") 
        print("   â€¢ precision_recall_scatter.png")
        print("   â€¢ sample_distribution_analysis.png")
        print("   â€¢ best_experiment_analysis.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŠ Charlestonæ´ªæ°´é¢„æµ‹ - è´å¶æ–¯ç½‘ç»œç»“æœå¯è§†åŒ–")
    print("="*60)
    
    # æ–‡ä»¶è·¯å¾„
    csv_file = "corrected_bayesian_flood_validation_full_network_summary_20250820_112441.csv"
    json_file = "best_2017_09_11_threshold_04_experiment.json"
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = BayesianResultsVisualizer(csv_file, json_file)
    
    # ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–
    visualizer.create_all_visualizations()

if __name__ == "__main__":
    main()